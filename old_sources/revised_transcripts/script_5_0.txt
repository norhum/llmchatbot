Okay, let's start cleaning up this lecture transcript.

"Hi everyone. Today, we are continuing our implementation of Make More Now. So far, we've reached multi-layer perceptrons, and our neural net looked like this, which we've been implementing over the last few lectures. I'm sure everyone is very excited to move on to recurrent neural networks and all of their variants, how they work, and their cool-looking diagrams. It's very exciting and interesting, and we will get a better result, but unfortunately, I think we have to remain here for one more lecture. The reason is that we've already trained this multi-layer perceptron, and we are getting pretty good loss. I think we have a decent understanding of the architecture and how it works. However, the line of code that I take issue with is here: `loss.backward()`.  We are using PyTorch autograd to calculate all of our gradients, and I would like to remove the use of `loss.backward()`. I would like us to write our backward pass manually on the level of tensors. I think this is a very useful exercise for the following reasons. I actually have an entire blog post on this topic, but I'd like to call backpropagation a leaky abstraction. What I mean by that is, backpropagation doesn't just make your neural networks work magically. It's not the case that you can just stack up arbitrary Lego blocks of differentiable functions, cross your fingers, back propagate, and everything is great. Things don't just work automatically. It is a leaky abstraction in the sense that you can shoot yourself in the foot if you do not understand its internals. It will magically not work or not work optimally, and you will need to understand how it works under the hood if you are hoping to debug it and address it in your neural net. So, this blog post from a while ago goes into some of those examples. For example, we've already covered some of them already. The flat tails of these functions, and how you do not want to saturate them too much because your gradients will die, the case of dead neurons, which I've already covered as well, and the case of exploding or vanishing gradients in the case of recurrent neural networks, which we are about to cover. You will often come across some examples in the wild. This is a snippet that I found in a random codebase on the internet, where they actually have a very subtle but pretty major bug in their implementation. The bug points at the fact that the author of this code does not actually understand backpropagation. They're trying to do here is they're trying to clip the loss at a certain maximum value. But actually, what they're doing is trying to collect the gradients to have a maximum value instead of trying to clip the loss at a maximum value. Indirectly, they are basically causing some of the outliers to be ignored. Because when you clip a loss of an outlier, you are setting its gradient to zero. So, have a look through this and read through it, but there's basically a bunch of subtle issues that you're going to avoid if you actually know what you're doing. That's why I don't think it is okay for us to ignore how it works just because PyTorch or other frameworks offer autograd. We've actually already covered autograd, and we wrote micrograd, but micrograd was an autograd engine only on the level of individual scalars. The atoms were single individual numbers, and I don't think it's enough. I'd like us to think about backpropagation on the level of tensors as well. So, in summary, I think it's a good exercise. I think it is very, very valuable. You're going to become better at debugging neural networks and making sure that you understand what you're doing. It is going to make everything fully explicit, so you're not going to be nervous about what is hidden away from you. In general, we're going to emerge stronger. So let's get into it. A bit of a fun historical note is that today writing your backward pass by hand and manually is not recommended, and no one does it except for the purposes of exercise. But about 10 years ago in deep learning, this was fairly standard, and in fact, pervasive. At the time, everyone used to write their own backward pass by hand manually, including myself. It's just what you would do. So we used to write backward passes by hand, and now everyone just calls `loss.backward()`. We've lost something. I want to give you a few examples of this. Here's a 2006 paper from Jeff Hinton and Ruslan Salakhutdinov in *Science* that was influential at the time. This was training some architectures called Restricted Boltzmann Machines, and basically, it's an autoencoder trained here. This is from roughly 2010, I had a library for training Restricted Boltzmann Machines. At the time, it was written in MATLAB. Python was not used for deep learning pervasively. It was all MATLAB. MATLAB was this scientific computing package that everyone would use. So, we would write MATLAB, which is barely a programming language. It had a very convenient tensor class and was a computing environment. You would run it on a CPU of course, but you would have very nice plots to go with it and a built-in debugger. It was pretty nice. Now, the code in this package in 2010 that I wrote for fitting Restricted Boltzmann Machines, to a large extent, is recognizable. But I wanted to show you how you would. I'm creating the data in the XY batches, I'm initializing the neural net, so it's got weights and biases, just like we're used to. Then, this is the training loop where we actually do the forward pass. Here, at this time, they didn't even necessarily use backpropagation to train neural networks. This, in particular, implements contrastive divergence, which estimates a gradient. Then here, we take that gradient and use it for a parameter update along the lines that we're used to. Yeah, here, but you can see that basically, people are meddling with these gradients directly and inline themselves. It wasn't that common to use an autograd engine. Here's one more example from a paper of mine from 2014 called "Fragmented Embeddings". Here, what I was doing was aligning images and text. It's kind of like CLIP if you're familiar with it, but instead of working on the level of entire images and entire sentences, it was working on the level of individual objects and little pieces of sentences. I was embedding them and then calculating very much like a CLIP-like loss. I dug up the code from 2014 of how I implemented this, and it was already in NumPy and Python. Here, I'm plotting the cost function, and it was standard to implement not just the cost, but also the backward pass manually. Here, I'm calculating the image embeddings, sentence embeddings, the loss function. I calculate the cost; this is the loss function. Then, once I have the loss function, I do the backward pass right here. So, I backward through the loss function and through the neural net. I append regularization. Everything was done by hand manually. You would just write out the backward pass, and then you would use a gradient checker to make sure that your numerical estimate of the gradient agrees with the one you calculated during backpropagation. This was very standard for a long time, but today, of course, it is standard to use an autograd engine. But it was definitely useful, and I think people sort of understood how these neural networks work on a very intuitive level. I think it's a good exercise again, and this is where we want to be. Okay, so just as a reminder from our previous lecture, this is the Jupyter notebook that we implemented at the time. We're going to keep everything the same. We're still going to have a two-layer multi-layer perceptron with a batch normalization layer. The forward pass will be basically identical to this lecture. But here, we're going to get rid of `loss.backward()` and instead, we're going to write the backward pass manually. Here's the starter code for this lecture. We are becoming a backprop ninja in this notebook. The first few cells here are identical to what we are used to. We are doing some imports, loading the dataset, and processing the dataset. None of this changed. Now, here I'm introducing a utility function that we're going to use later to compare the gradients. In particular, we are going to have the gradients that we estimate manually ourselves, and we're going to have gradients that PyTorch calculates. We're going to be checking for correctness, assuming, of course, that PyTorch is correct. Then here, we have the initialization that we are quite used to. We have our embedding table for the characters, the first layer, the second layer, and the batch normalization in between. Here's where we create all the parameters. Now you will note that I changed the initialization a little bit to be small numbers. Normally, you would set the biases to be all zero. Here I am setting them to be small random numbers, and I'm doing this because if your variables are initialized to exactly zero, sometimes what can happen is that can mask an incorrect implementation of a gradient. Because when everything is zero, it simplifies and gives you a much simpler expression of the gradient than you would otherwise get. So, by making it small numbers, I'm trying to unmask those potential errors in these calculations. You also notice that I'm using `b1` in the first layer. I'm using a bias despite batch normalization right afterwards. This would typically not be what you do because we talked about the fact that you don't need the bias, but I'm doing this here just for fun because we're going to have a gradient with respect to it, and we can check that we are still calculating it correctly, even though this bias is spurious. Here, I'm calculating a single batch, and then here, I'm doing a forward pass. Now you'll notice that the forward pass is significantly expanded from what we are used to. Here, the forward pass was just here. Now, the reason that the forward pass is longer is for two reasons. Number one, here, we just had an `F.cross_entropy`, but here I am bringing back an explicit implementation of the loss function. Number two, I've broken up the implementation into manageable chunks. So, we have a lot more intermediate tensors along the way in the forward pass. That's because we are about to go backwards and calculate the gradients in this backpropagation from the bottom to the top. So we're going to go upwards. Just like we have, for example, the `logprobs` tensor in a forward pass, in the backward pass, we're going to have a `d_logprobs` which is going to store the derivative of the loss with respect to the `logprobs` tensor. So we're going to be prepending "d" to every one of these tensors and calculating it along the way of this backpropagation. As an example, we have `b_in_raw` here. We're going to be calculating a `db_in_raw`. Here, I'm telling PyTorch that we want to retain the grad of all these intermediate values because here in exercise one, we're going to calculate the backward pass. So we're going to calculate all these `d` values, d variables, and use the `CMP` function I've introduced above to check our correctness with respect to what PyTorch is telling us. This is going to be exercise one, where we sort of backpropagate through this entire graph. Now, just to give you a very quick preview of what's going to happen in exercise two and below, here we have fully broken up the loss and backpropagated through it manually in all the little atomic pieces that make it up. But here, we're going to collapse the loss into a single cross-entropy call. Instead, we're going to analytically derive, using math and paper and pencil, the gradient of the loss with respect to the logits. Instead of backpropagating through all of its little chunks one at a time, we're just going to analytically derive what that gradient is, and we're going to implement that, which is much more efficient, as we'll see in a bit. Then, we're going to do the exact same thing for batch normalization. Instead of breaking up batchnorm into all the tiny components, we're going to use pen and paper, mathematics, and calculus to derive the gradient through the batch normalization layer. So we're going to calculate the backward pass through batchnorm in a much more efficient expression, instead of backpropagating through all of its little pieces independently. That's going to be exercise three. Then, in exercise four, we're going to put it all together. This is the full code of training this two-layer MLP. We're going to basically insert our manual backprop, and we're going to take out `loss.backward()`. You will see that you can get all the same results using fully your own code. The only thing we're using from PyTorch is `torch.tensor` to make the calculations efficient. Otherwise, you will understand fully what it means to forward and backward a neural net and train it. I think that'll be awesome. So let's get to it. Okay, so I read all the cells of this notebook all the way up to here, and I'm going to erase this, and I'm going to start implementing the backward pass, starting with `d_logprobs`. So we want to understand what should go here to calculate the gradient of the loss with respect to all the elements of the `logprobs` tensor. Now, I'm going to give away the answer here, but I wanted to put a quick note that I think would be most pedagogically useful for you. Go into the description of this video and find the link to this Jupyter notebook. You can find it both on GitHub, but you can also find a Google Colab link. So, you don't have to install anything. You'll just go to a website on Google Colab, and you can try to implement these derivatives or gradients yourself. If you are not able to come up with the answer, then come to my video and see me do it. So work in tandem, try it yourself first, and then see me give away the answer. I think that'll be most valuable to you, and that's how I recommend you go through this lecture. So we are starting here with `d_logprobs`. Now, `d_logprobs` will hold the derivative of the loss with respect to all the elements of `logprobs`. What is inside `logprobs`? The shape of this is 32 by 27. It's not going to surprise you that `d_logprobs` should also be an array of size 32 by 27, because we want the derivative of the loss with respect to all of its elements. The sizes of those are always going to be equal. Now, how does `logprobs` influence the loss? Okay, loss is `-logprobs[range(N), YB].mean()`. Just as a reminder, `YB` is just basically an array of all the correct indices. What we're doing here is we're taking the `logprobs` array of size 32 by 27, and in every single row, we are plucking out the index 8 and then 14 and 15, and so on. So we're going down the rows; that's the iterator `range(N)`. We are always plucking out the index of the column specified by this tensor `YB`. So, in the zeroth row, we are taking the eighth column; in the first row, we are taking the 14th column, etc. `logprobs[range(N), YB]` plugs out all those log probabilities of the correct next character in a sequence. The shape of this or the size of it is, of course, 32, because our batch size is 32. These elements get plugged out, and then their mean, and the negative of that, becomes loss. I always like to work with simpler examples to understand the numerical form of the derivative. What's going on here is, once we've plucked out these examples, we're taking the mean, and then the negative. So the loss basically, I can write it this way, is the negative of say a + b + c, and the mean of those three numbers would be, say, negative, we'd divide by three. That would be how we achieve the mean of three numbers, a, b, c, although we actually have 32 numbers here. What is basically the `loss` by say, like `da`? Well, if we simplify this expression mathematically, this is negative one over three of a plus negative one over three of b plus negative one over three of c. What is `d_loss` by `da`? It's just negative one over three. You can see that if we don't just have a, b, and c, but we have 32 numbers, then `d_loss` by d every one of those numbers is going to be one over N. More generally, because n is the size of the batch, 32 in this case. So `d_loss` by `d_logprobs` is negative one over n in all these places. What about the other elements inside `logprobs`, because `logprobs` is a large array? You see that `logprobs` shape is 32 by 27, but only 32 of them participate in the loss calculation. What is the derivative of all the other most of the elements that do not get plucked out here? Their loss, intuitively, is zero. Their gradient, intuitively, is zero. That's because they did not participate in the loss. Most of these numbers inside this tensor do not feed into the loss. If we were to change these numbers, then the loss doesn't change, which is the equivalent way of saying that the derivative of the loss with respect to them is zero. They don't impact it. Here's a way to implement this derivative. We start out with `torch.zeros` of shape 32 by 27, or, let's just say, instead of doing this, because we don't want to hardcode numbers, let's do `torch.zeros_like(logprobs)`. Basically, this is going to create an array of zeros exactly in the shape of `logprobs`. Then, we need to set the derivative of negative one over n inside exactly these locations. Here's what we can do. `logprobs[range(N), YB]` will be set to negative one over n. Just like we derived here. Let me erase all this reasoning. This is the candidate derivative for `d_logprobs`. Let's uncomment the first line and check that this is correct. Okay, so CMP ran, and let's go back to CMP. You see that what it's doing is, it's calculating if the calculated value by us, which is `dt`, is exactly equal to `t.grad`, as calculated by PyTorch. This is making sure that all the elements are exactly equal, and then converting this to a single Boolean value, because we don't want the Boolean tensor. We just want a Boolean value. Here we are making sure that okay, if they're not exactly equal, maybe they are approximately equal because of some floating point issues, but they're very, very close. Here we are using `torch.allclose`, which has a little bit of a wiggle available, because sometimes you can get very, very close, but if you use a slightly different calculation because of floating point arithmetic, you can get a slightly different result. This is checking if you get an approximately close result. Then here, we are checking the maximum, basically, the value that has the highest difference, and what is the difference in the absolute value difference between those two. Here, we see that we actually have exact equality. Therefore, of course, we also have an approximate equality, and the maximum difference is exactly zero. Basically, our `d_logprobs` is exactly equal to what PyTorch calculated to be `logprobs.grad` in its backpropagation. So far, we're working pretty well. Okay, let's now continue our backpropagation. We have that `logprobs` depends on `probs` through a log. All the elements of `probs` are being element-wise applied `log` to. Now, if we want `d_probs`, then remember your micrograd training. We have a log node, it takes in `probs` and creates `logprobs`, and the `d_probs` will be the local derivative of that individual operation `log`, times the derivative of loss with respect to its output, which in this case is `d_logprobs`. What is the local derivative of this operation? We are taking log element-wise. We can come here, and we can see that d by dx of log of x is just simply one over x. Therefore, in this case, x is `probs`. We have d by dx is one over x, which is one over `probs`. This is the local derivative. Then times, because of the chain rule, `d_logprobs`. Let me uncomment this and let me run the cell in place, and we see that the derivative of probs, as we calculated here, is exactly correct. Notice here how this works. The `probs` is going to be inverted and then element-wise multiplied here. If your `probs` is very, very close to one, that means your network is currently predicting the character correctly. This will become one over one, and `d_logprobs` just gets passed through. But if your probabilities are incorrectly assigned, so if the correct character here is getting a very low probability, then 1.0 divided by it will boost this and then multiply by the `d_logprobs`. Basically, what this line is doing intuitively is, it's taking the examples that have a very low probability currently assigned, and it's boosting their gradient. You can look at it that way. Next up is `count_sum_inv`. We want the derivative of this. Let me just pause here and introduce what's happening here in general, because I know it's a little bit confusing. We have the logits that come out of the neural net. Here what I'm doing is, I'm finding the maximum in each row, and I'm subtracting it for the purposes of numerical stability. We talked about how if you do not do this, you run into numerical issues if some of the logits take on too large values because we end up exponentiating them. This is done just for safety numerically. Then here is the exponentiation of all the logits to create our counts. Then we want to take the sum of these counts and normalize, so that all of the probs sum to one. Here, instead of using one over `count_sum`, I use raised to the power of negative one. Mathematically they are identical. I just found that there's something wrong with the PyTorch implementation of the backward pass of division, and it gives like a `NaN` result, but that doesn't happen for `** -1`. That's why I'm using this formula instead. But basically, all that's happening here is, we got the logits, we're going to exponentiate all of them and want to normalize the counts to create our probabilities. It's just that it's happening across multiple lines. Here, we want to first take the derivative. We want to backpropagate into `count_sum_inv` and then into `counts` as well. What should be `d_count_sum_inv`? We actually have to be careful here because we have to scrutinize and be careful with the shapes. `counts.shape` and then `count_sum_inv.shape` are different. In particular, `counts` is 32 by 27, but `count_sum_inv` is 32 by 1. In this multiplication here, we also have an implicit broadcasting that PyTorch will do because it needs to take this column tensor of 32 numbers and replicate it horizontally 27 times to align these two tensors so it can do an element-wise multiply. Really what this looks like is the following, using a toy example again. What we really have here is `probs` is `counts` times `count_sum_inv`. It's `c = a * b`, but a is 3 by 3, and b is just 3 by 1, a column tensor. PyTorch internally replicated these elements of b, and it did that across all the columns. For example, b1, which is the first element of b, would be replicated here across all the columns in this multiplication. Now we're trying to backpropagate through this operation to `count_sum_inv`. When we're calculating this derivative, it's important to realize that this looks like a single operation, but it actually is two operations applied sequentially. The first operation that PyTorch did is, it took this column tensor and replicated it across all the columns, basically 27 times. That's the first operation. It's a replication. Then the second operation is the multiplication. Let's first backpropagate through the multiplication. If these two arrays are of the same size, and we just have a and b of both of them 3 by 3, then how do we backpropagate through a multiplication? If we just have scalars, and not tensors, then if you have `c = a * b`, then what is the derivative of c with respect to b? It's just a. That's the local derivative. Here in our case, undoing the multiplication and backpropagating through just the multiplication itself, which is element-wise, is going to be the local derivative, which in this case is simply `counts`, because counts is the a. This is the local derivative, and then times, because of the chain rule, `d_probs`. This here is the derivative, or the gradient, but with respect to replicated b. But we don't have a replicated b, we just have a single b column. How do we now backpropagate through the replication? Intuitively, this b1 is the same variable, and it's just reused multiple times. You can look at it as being equivalent to a case we've encountered in micrograd. Here, I'm just pulling out a random graph we used in micrograd. We had an example where a single node has its output feeding into two branches of basically the graph, until the last function, and we're talking about how the correct thing to do in the backward pass is, we need to sum all the gradients that arrive at any one node. Across these different branches, the gradients would sum. If a node is used multiple times, the gradients for all of its uses sum during backpropagation. Here, b1 is used multiple times in all these columns. Therefore, the right thing to do here is to sum horizontally across all the rows. I'm going to sum in dimension one, but we want to retain this dimension, so that the `count_sum_inv` and its gradient are going to be exactly the same shape. We want to make sure that we keep it as true, so we don't lose this dimension. This will make the `count_sum_inv` be exactly shape 32 by 1. So revealing this comparison as well, and running this, we see that we get an exact match. This derivative is exactly correct. Let me erase this. Now, let's also backpropagate into `counts`, which is the other variable here to create probs. From `probs` to `count_sum_inv` we just did that. Let's go into `counts` as well. So, `d_counts` will be, the chances are, a. So, `dc` by `da` is just b. Therefore, it's `count_sum_inv`, and then times, the chain rule, `d_probs`. Now `count_sum_inv` is 32 by 1. `d_probs` is 32 by 27. Those will broadcast fine and will give us `d_counts`. There's no additional summation required here. There will be a broadcasting that happens in this multiply here, because `count_sum_inv` needs to be replicated again to correctly multiply `d_probs`. That's going to give the correct result, as far as a single operation is concerned. So we backpropagated from `probs` to `counts`. But we can't actually check the derivative `counts`. I have it much later on. The reason for that is, because `count_sum_inv` depends on `counts`. So there's a second branch here that we have to finish, because `count_sum_inv` backpropagates into `count_sum`, and `count_sum` will then backpropagate into `counts`. So `counts` is a node that is being used twice. It's used right here in two `probs`, and it goes through this other branch through `count_sum_inv`. Even though we've calculated the first contribution of it, we still have to calculate the second contribution of it later. Okay, so we're continuing with this branch. We have the derivative for `count_sum_inv`. Now we want the derivative of `count_sum`. So, `d_count_sum` equals what is the local derivative of this operation? This is basically an element-wise one over `count_sum`. `count_sum` raised to the power of negative one is the same as one over `count_sum`. If we go to Wolfram Alpha, we see that `x` to the negative one, d by dx of it, is basically negative `x` to the negative two. One negative one over squared is the same as negative `x` to the negative two. `d_count_sum` here will be, the local derivative is going to be negative `count_sum` to the negative two. That's the local derivative. Times chain rule, which is `d_count_sum_inv`. That's `d_count_sum`. Let's uncomment this and check that I am correct. Okay, so we have perfect equality, and there's no sketchiness going on here with any shapes, because these are of the same shape. Okay, next up, we want to backpropagate through this line. We have that `count_sum` is `counts.sum` along the rows. I wrote out some help here. We have to keep in mind that `counts` of course is 32 by 27 and `count_sum` is 32 by 1. In this backpropagation, we need to take this column of derivatives and transform it into an array of derivatives, a two-dimensional array. What is this operation doing? We're taking in some kind of an input like say, a 3x3 matrix, `a`, and we are summing up the rows into a column tensor `b1, b2, b3`, that is basically this. Now we have the derivatives of the loss with respect to b, all the elements of b, and now we want the derivative of loss with respect to all these little a's. How do the b's depend on the a's is basically what we're after? What is the local derivative of this operation? We can see here that b1 only depends on these elements here. The derivative of b1 with respect to all of these elements down here is zero. But for these elements here, like a11, a12, etc., the local derivative is one. `db1` by `da11`, for example, is one. So it's one, one, and one. When we have the derivative of loss with respect to b1, the local derivative of b1 with respect to these inputs is zeros here, but it's one on these guys. In the ch"
