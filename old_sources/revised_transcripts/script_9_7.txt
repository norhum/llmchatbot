Okay, here is the cleaned-up transcript:

Some of the things in this repo will hopefully be fixed. I will release this build n GPT repo. Right now it looks a little ugly and preliminary, so hopefully, by the time you get here it's nicer. Down here, I'm going to show results and talk about some of the things that happened after the video. I expect that we will have fixed a small issue, but for now, this shows that our training is not completely wrong. It shows that we're able to surpass the accuracy with only 10x the token budget. Possibly, it could also be that the dataset has improved. The original GPT2 dataset was web text, and it is possible that not a lot of care and attention went into that data set. This was very early in LLMs. Now there is a lot more scrutiny on good practices around deduplication, filtering, quality filtering, and so on. It's possible that the data we're training on is just of higher quality per token, and that could be giving us a boost as well. There are a number of caveats to think about, but for now, we're pretty happy with this. Now the next thing I was interested in, as you see it’s morning now, so there was an overnight run, and I wanted to see how far I could push the result. To do an overnight run, I basically did instead of one epoch, which took roughly two hours, I just did a times four, so that would take eight hours while I was sleeping. We did four epochs, or roughly 40 billion tokens of training, and I was trying to see how far we could get. This was the only change and I reran the script. When I read the log file at 40 billion tokens, this is what the curve looked like. To narrate this, number one, we are seeing this issue here with the periodicity through the different epochs, and something really weird with the fine web edu data set. That is to be determined. Otherwise, we are seeing that the HSWAG accuracy actually went up by a lot and we almost made it to the GPT 324m accuracy up here, but not quite. It’s too bad that I didn’t sleep slightly longer. I think if this was a five epoch run, we may have gotten there. One thing to point out is that if you're doing multi-epoch runs, we're not actually being very careful in our data loader. This data loader goes through the data in exactly the same format and in exactly the same order. This is kind of suboptimal, and you would want to look into extensions where you actually permute the data randomly, you permute the documents around in every single shard on every single new epoch, and possibly even permute the shards. That would go a long way into decreasing the periodicity, and it's also better for the optimization, so that you're not seeing things in the identical format. You're introducing some randomness in how the documents follow each other. You have to remember that in every single row these documents follow each other and then there's the end of text token and then the next document. The documents are currently glued together in the exact same identical manner, but we actually want to break up the documents and shuffle them around, because the order of the documents shouldn't matter and they shouldn't— basically, we want to break up that dependence because it's a kind of a spurious correlation. So our data loader is not currently doing that, and that's one improvement you could think of making. The other thing to point out is we're almost matching GPT3 accuracy with only 40 billion tokens. GPT3 trained on 300 billion tokens, so again we're seeing about a 10x improvement here with respect to learning efficiency. I don’t actually know exactly what to attribute this to, other than some of the things that I already mentioned previously for the previous run. The other thing I wanted to briefly mention is the max LR here. I saw some people already play with this a little bit in a previous related repository. It turns out that you can actually almost like 3x it. It's possible that the maximum learning rate can be a lot higher, and for some reason, the GPT3 hyper-parameters that we are inheriting are actually extremely conservative. You can actually get away with a higher learning rate, and it would train faster. A lot of these hyper-parameters are quite tunable. Feel free to play with them, and they're probably not set precisely correctly. It’s possible that you can get away with doing this. If you wanted to exactly be faithful to GPT3, you would also want to make the following difference. You'd want to come here, and the sequence length of GPT3 is 2048 instead of 1024. You would come here and change this to 2048 for T. Then, if you want the exact same number of tokens, half a million per iteration or per step, you want to then decrease this to 32, so they still multiply to half a million. That would give your model a sequence length equal to that of GPT3. In that case, the models would be roughly identical as far as I am aware, because again GPT2 and GPT3 are very similar models. We can also look at some of the samples here from the model that was trained overnight. This is the optimization, and you see that here we stepped all the way to 76,290, and these are the HSWAG scores we achieved was 33.24. These are some of the samples from the model. You can see that if you read through this, and pause the video briefly, you can see that they are a lot more coherent. They're actually addressing the fact that it's a language model almost. For example, “Hello, I'm a language model and I try to be as accurate as possible. I'm a language model not a programming language, I know how to communicate, I use Python.” I don't know if you pause this and look at it, and then compare it to the model that was only trained for 10 billion, you will see that these are a lot more coherent, and you can play with this yourself. One more thing I added to the code by the way, is this chunk of code here. Basically, right after we evaluate the validation loss, if we are the master process, in addition to logging the validation loss every 5,000 steps, we're also going to save the checkpoint, which is really just the state dictionary of the model. Checkpointing is nice just because you can save the model and later you can use it in some way. If you wanted to resume the optimization, then in addition to saving the model, we have to also save the optimizer state dict. Remember that the optimizer has a few additional buffers because of Adam, so it's got the m and v and you need to also resume the optimizer properly. You have to be careful with your RNG seeds, random number generators, and so on. If you wanted to exactly be able to resume optimization, you have to think through the state of the training process, but if you just want to save the model, this is how you would do it. One nice reason why you might want to do this is because you may want to evaluate the model a lot more carefully. Here, we are only kind of like winging the HSWAG eval. You may want to use something nicer, like for example, the LUTHER evaluation harness. This is a way to also evaluate language models. It's possible that you may want to use different infrastructure to more thoroughly evaluate the models on different evaluations and compare it to the original GPT2 model on many other tasks, like for example, that involve math, code, or different languages. This is a nice functionality to have as well. The other thing I wanted to mention is that everything we've built here, this is only the pre-training step. The GPT here, it dreams documents, it just predicts the next token. You can't talk to it like you can talk to chat GPT. If you wanted to talk to the model, we have to fine-tune it into the chat format. It's not actually that complicated. If you're looking at supervised fine-tuning, or SFT, really what that means is we're just swapping out a data set into a data set that is a lot more conversational, and there's a user assistant, user assistant kind of structure. We just fine-tune on it, and then we basically fill in the user tokens, and we sample the assistant tokens. It's not a lot more deeper than that, but basically we swap out the data set and continue training. For now, we're going to stop at pre-training. One more thing that I wanted to briefly show you is that, of course, what we've built up today was building towards nanoGPT, which is this repository from earlier. There's actually another nanoGPT implementation, and it's hiding in a more recent project that I've been working on called llm.c. llm.c is a pure Cuda implementation of GPT2 or GPT3 training, and it just directly uses Cuda and is written as Cuda. The nanoGPT here acts as reference code in pytorch to the C implementation. We're trying to exactly match up the two, but we're hoping that the C Cuda is faster. Currently, that seems to be the case because it is a direct optimized implementation. train_gpt2.py in llm.c is basically the nanoGPT, and when you scroll through this file, you'll find a lot of things that very much look like things that we've built up in this lecture. When you look at train_gpt2_cuda, this is the C Cuda implementation. There’s a lot of MPI, NCCL, GPU Cuda cc++, and you have to be familiar with that. When this is built up, we can actually run the two side by side, and they're going to produce the exact same results, but llm.c actually runs faster. Let’s see that. On the left, I have pytorch, a nanoGPT looking thing. On the right, I have the llm.c call. Here, I'm going to launch the two. Both of these are going to be running on a single GPU. I'm putting the llm.c on GPU 1, and this one will grab GPU 0 by default. You can see here that llm.c compiled and then allocated space, and it's stepping. Meanwhile, pytorch is still compiling because torch compile is a bit slower here than the llm.c nvcc Cuda compile. This program has already started running, and we're still waiting here for torch compile. Of course, this is a very specific implementation to GPT2 and 3. Pytorch is a very general neural network framework, so they're not exactly comparable. If you're only interested in training GPT2 and 3, llm.c is very fast. It takes less space, it's faster to start, and it's faster per step. PyTorch started stepping here, and as you can see, we're running at about 223,000 tokens per second here, and about 185,000 tokens per second here. Quite a bit slower, but I don't have full confidence that I exactly squeezed out all the juice from the pytorch implementation. The important thing here is, notice that if I align up the steps, you will see that the losses and norms that are printed between these two are identical. On the left, we have the PyTorch, and on the right, this C implementation, and they're the same, except this one runs faster. That's kind of what I wanted to show you, also briefly, llm.c, and this is a parallel implementation, and it's also something that you may want to play with or look at. It's kind of interesting. At this point, I should probably start wrapping up the video because I think it's getting way longer than I anticipated. We did cover a lot of ground and we built everything from scratch. As a brief summary, we were looking at the GPT2 and GPT3 papers. We were looking at how you set up these training runs and all the considerations involved. We wrote everything from scratch, and then we saw that over the duration of either a two-hour training run, or an overnight run, we can actually match the 124 million parameter checkpoints of GPT2 and GPT3 to a very large extent. In principle, the code that we wrote would be able to train even bigger models if you have the patience or the computing resources. You could potentially think about training some of the bigger checkpoints as well. There are a few remaining issues to address: What's happening with the loss here, which I suspect has to do with the fine web edu data sampling? Why can't we turn on torch compile? It currently breaks generation and HSWAG. What’s up with that? In the data loader, we should probably be permuting our data when we reach boundaries. There are a few more issues like that, and I expect to be documenting some of those over time in the build n GPT repository here, which I'm going to be releasing with this video. If you have any questions or would like to talk about anything that we covered, please go to the discussions tab, so we can talk here, or please go to issues or pull requests, depending on what you'd like to contribute. Also, have a look at the Zero to Hero Discord, and I'm going to be hanging out here on N GPT. Otherwise, for now, I'm pretty happy about where we got, and I hope you enjoyed the video and I will see you later.
