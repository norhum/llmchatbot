Initializing neural networks, particularly deep multilayer perceptrons with nonlinearities, requires careful attention to activation behavior. Activations should remain within reasonable values, avoiding expansion to infinity or shrinking to zero. A key question is how to initialize weights to achieve this. Kingma and Ba's paper, "Delving Deep into Rectifiers," studied this problem in detail, focusing on convolutional neural networks and ReLU nonlinearities. While their analysis centered on ReLU instead of tanh, the principles are similar. The ReLU nonlinearity acts as a squashing function, clamping negative numbers to zero while passing positive numbers through. This effectively discards half of the distribution, necessitating compensation with a gain.  They found that weights should be initialized with a zero-mean Gaussian distribution, where the standard deviation is the square root of 2 over the fan-in. The fan-in is the number of inputs to a neuron. This factor of two arises because ReLU discards half the distribution. The paper also investigates backpropagation, ensuring gradients are well-behaved. While the forward pass is initialized, the backward pass is also initialized up to a constant factor relating to the size of hidden neurons in early and late layers. Empirically, this factor doesn't significantly affect performance. This initialization method is implemented in PyTorch's `torch.nn.init` module using `kaiming_normal`. This is a common approach for initializing neural networks, accepting keyword arguments. The 'mode' argument specifies whether to normalize activations or gradients, which is typically set to the default 'fan-in'. A second argument specifies the nonlinearity, which dictates the gain. For linear activations, the gain is one, resulting in a formula where standard deviation is the square root of one over fan-in. For ReLU, the gain is the square root of two. For tanh, which is a contractive function, a gain of 5 over 3 is used. Tanh squashes the output distribution, requiring boosted weights to renormalize everything to a standard unit deviation. This ensures that distributions remain within an appropriate range. Seven years ago, precise initialization of activations and gradients was crucial for training deep networks. However, modern innovations have made this less critical. These include residual connections, normalization layers such as batch, layer, and group normalization, and more sophisticated optimizers like Adam. While precise calibration is less necessary now, initializing by normalizing weights by the square root of the fan-in, similar to what was originally done, remains a good practice. For a more precise implementation using `torch.nn.init.kaiming_normal`, the standard deviation is calculated as the gain over the square root of the fan-in. In the case of a tanh nonlinearity, the gain would be 5 over 3. The standard deviation is calculated as the gain divided by the square root of the fan-in. The weights are then multiplied by this standard deviation to achieve proper initialization. After training, the validation loss is roughly the same as with a less-principled method, but this method is more scalable and provides a useful guideline.

Batch normalization, introduced in 2015, has greatly improved the reliability of training deep neural networks. Batch normalization standardizes hidden states to approximately follow a Gaussian distribution with zero mean and unit standard deviation. The idea is to normalize hidden states, making them unit Gaussian, because these values tend to be too small, rendering the tanh function ineffective, or too large, saturating the tanh function. The hidden state's mean is calculated across the zero dimension, preserving the dimension using `keepdim=True`. The standard deviation is calculated similarly, resulting in dimensions of 1x200 for 200 neurons. The hidden states are then normalized by subtracting the mean and dividing by the standard deviation, thereby making the values unit Gaussian. However, forcing the hidden states to always be unit Gaussian is undesirable, as the network may need a different distribution. The paper thus introduces a scale and shift component. Normalized inputs are scaled by a gain and offset by a bias. A batch normalization gain is initialized to ones, while a batch normalization bias is initialized to zeros, both with a dimension of 1xN_hidden. The normalized hidden states are multiplied by the batch normalization gain and then added to the bias. This ensures that, at initialization, each neuron's firing values will be unit Gaussian. These gain and bias values are trained via backpropagation to allow the network the flexibility to adjust the distribution of activations. The gain and bias parameters must be added to the model's trainable parameters. Batch normalization is also applied during testing time using the same normalization, scaling and shifting procedures.  When applying batch normalization, the validation loss can remain approximately the same, since the weights were already scaled to achieve a roughly Gaussian distribution before. However, batch normalization is essential for deeper networks, as controlling the scales of weights to get roughly Gaussian activations becomes more challenging. Batch normalization is often placed after linear or convolutional layers to control activation scales. This stabilizes training without requiring precise mathematical analysis of distributions. This stability comes at a cost, however. Batch normalization mathematically couples the examples within a batch because mean and standard deviation are calculated across all examples in a given batch. Thus, an activation for one example depends on the values in the other examples. This results in a jittering of the hidden states and logits, which is dependent on the composition of the batch. Although undesirable, this jitter acts as a regularizer, effectively padding out input examples, introducing entropy, and reducing overfitting. Batch normalization has remained in use due to its regularizing effects and its effectiveness at controlling activations, despite the coupling of examples within a batch.

Alternative techniques like layer, instance, and group normalization do not couple the examples in a batch, but batch normalization remains popular despite this drawback. When deploying a trained neural network with batch normalization, the problem arises on how to use it for single examples since the neural net expects a batch. The solution is to calibrate the batch normalization statistics by computing the mean and standard deviation across the training set once, after training, and using these fixed values during inference. The batch normalization mean and standard deviation are computed over the entire training set, and used during inference, giving an identical loss. An alternative approach, however, is to estimate the mean and standard deviation in a running manner during training. This avoids the need for a separate calibration step after training. A running mean of both the batch normalization mean and the standard deviation are updated during training. These running mean and standard deviation are not updated via backpropagation but by a simple smoothing update. The running mean is updated by a weighted average of the current running mean and the newly calculated mean. The running standard deviation is updated similarly. During training, the running mean and standard deviation values track the typical values observed during training. After training, these running values are used for inference. After the optimization process is complete, both values are closely comparable to those calculated through explicit calibration. The running mean and standard deviations are used instead of the batch mean and standard deviation during evaluation. Batch normalization is complete with two notes. First, the small epsilon term added to the denominator is used to prevent division by zero if variance becomes zero. Second, adding bias before batch normalization is redundant because the batch normalization layer will subtract the mean, effectively nullifying the bias. Therefore, when batch normalization is used the bias term of a preceding layer can be omitted.
