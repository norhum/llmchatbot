Okay, let's begin. In this lecture, we are continuing our implementation of "make more". In the last lecture, we implemented a multi-layer perceptron (MLP) for character-level language modeling, following the approach described in Benj's 2003 paper. We took in a few past characters and used an MLP to predict the next character in a sequence. Now, we want to move on to more complex and larger neural networks, like recurrent neural networks and their variations such as GRUs and LSTMs. However, before we do that, we need to stick with MLPs for a bit longer. I want us to have a good intuitive understanding of the activations in the neural net during training, especially the gradients that flow backward and how they behave. This is crucial for understanding the history of the development of these architectures. Recurrent neural networks, while very expressive and capable of implementing any algorithm, are not easily optimized with the first-order gradient-based techniques we commonly use. The key to understanding why they are not easily optimizable lies in understanding the activations and gradients and their behavior during training. Many variants of recurrent neural networks have attempted to improve this situation. That is the path we will take.

The starting code for this lecture is largely the same as before, but I have cleaned it up a bit. We are importing all the necessary PyTorch, math, and matplotlib utilities. We are reading in the words as before; these are eight example words from a total of 32,000. Here is a vocabulary of all lowercase letters and the special dot token. We are reading the dataset, processing it, and creating three splits: train, development, and test. In this MLP, which is identical to the previous one, I have removed the magic numbers. Instead, we have the dimensionality of the character embedding space and the number of hidden units in the hidden layer pulled out as variables. This avoids having to change these magic numbers all the time. We have the same neural net with 11,000 parameters, optimized over 200,000 steps with a batch size of 32. I have refactored the code a bit here, creating some extra variables and comments, but there are no functional changes. When we optimize, we see that our loss looks something like this. The train and validation loss are around 2.16. I've also refactored the code for evaluating arbitrary splits. You can pass in a string of which split you want to evaluate, and it will retrieve the correct split. Then, it performs the forward pass of the network, evaluates the loss, and prints it. This makes it nicer to work with.

You will notice that I am using a `torch.no_grad()` decorator. This decorator tells PyTorch that the function does not require any gradients. Therefore, it will not perform the bookkeeping needed to track gradients in anticipation of a backward pass. It's as if all tensors created here have `requires_grad` set to `False`, making everything more efficient. You can also use a context manager with `torch.no_grad()`. We also have the sampling function, which performs a forward pass, gets the distribution, adjusts the context window, and repeats until it gets the special end token. We are starting to get much nicer-looking words from the model, though they are still not fully name-like. This is much better than what we had with the bi-gram model. This is our starting point.

The first thing I would like to examine is the initialization. I can tell that our network is improperly configured at initialization. There are multiple things wrong, but let’s start with the first one. Look here: at the zeroth iteration, the very first iteration, we record a loss of 27, which rapidly decreases to roughly one or two. I can tell that the initialization is messed up because this value is too high. When training neural nets, you usually have a rough idea of what loss to expect at initialization. This depends on the loss function and the problem setup. In this case, I do not expect 27; I expect a much lower number. We can calculate it together. At initialization, there are 27 characters that could come next for any one training example. We have no reason to believe that any characters are much more likely than others. Therefore, we expect that the probability distribution that initially comes out is a uniform distribution, assigning equal probability to all 27 characters. The probability for any character should be roughly 1/27. The loss is the negative log probability. If we wrap this in a tensor, take the log of it, and then negate it, the expected loss would be 3.29, much lower than 27. At initialization, the neural net is creating probability distributions that are all messed up. Some characters are very confidently predicted while others are not. The network is confidently wrong, leading to a very high loss.

Here’s a smaller, four-dimensional example of the issue. Let's say we have four characters. The logits coming out of the neural net are very close to zero. When we take the softmax of all zeros, we get probabilities that are a diffused distribution. It sums to one and is exactly uniform. In this case, if the label is two, it doesn’t matter if the label is two, three, one, or zero. Because it’s a uniform distribution, we record the same loss, 1.38. This is the loss we would expect for a four-dimensional example. As we start to manipulate these logits, we will change the loss. If we were to get lucky and the correct logit was a very high number, like five, we would record a low loss because, by chance, we're assigning the correct probability to the correct label. More likely, some other dimension will have a high logit. We will then start to record a much higher loss. The logits could take on extreme values, and we can record very high loss. For example, if the logits are randomly drawn from a normal distribution, the loss that comes out is okay because the logits are near zero. But if these logits are multiplied by ten, the more extreme values make it very unlikely that we guess the correct bucket. We become confidently wrong and record high loss. If logits are even more extreme, we might get insane losses, even infinity, at initialization. This is not good, and we want the logits to be roughly zero when the network is initialized. The logits do not have to be exactly zero; they just need to be equal. For example, if all logits are one, because of the normalization inside the softmax, this will come out okay. But for symmetry, we don’t want any arbitrary positive or negative number. We want them to be all zeros and record the expected loss at initialization.

Let’s concretely see where things go wrong in our example. Let me reinitialize the neural net, and let me break after the very first iteration to see only the initial loss, which is 27. That’s way too high. The logits here, if we print the first row, take on quite extreme values. That is what creates the false confidence in incorrect answers and makes the loss very high. These logits should be much closer to zero. Let’s think about how to make the logits coming out of this neural net closer to zero. The logits are calculated as hidden states multiplied by W2 plus B2. First, we are initializing B2 as random values. Since we want roughly zero, we don't want to add a bias of random numbers. I will add `* 0` here to make B2 zero at initialization. Second, the logits are H * W2. If we want logits to be very small, we should make W2 smaller. If we scale down all the elements of W2 by 0.1, and do the very first iteration again, the loss gets much closer to what we expect. The goal is about 3.29. This is now 4.2. We can make it even smaller: 3.32. We are getting closer. You might wonder, can we set it to zero? Of course, we get exactly what we are looking for at initialization. But I am nervous, and I’ll show you why you don't want to set the weights of a neural net to exactly zero. You want them to be small numbers instead. For this specific case, it might be fine, but I’ll show you where things go wrong quickly if you do that. Let’s use 0.01. The loss is close enough, with a bit of entropy. It's not exactly zero, and this is used for symmetry breaking. The logits are now much closer to zero. If we remove the break statement and run the optimization with this new initialization, we see that we started well and then came down a bit. The plot of the loss does not have this hockey stick appearance. What was happening in the hockey stick was that the first few iterations of training were just squashing down the logits. We removed this easy part of the loss function where the weights were just being shrunk down. Therefore, we don't see these easy gains at the start. We are getting some of the harder gains from training the actual neural net. There's no hockey stick appearance, and good things are happening. First, the loss at initialization is what we expect. Second, the loss doesn't look like a hockey stick. This is true for any neural net that you might train, and it is something to watch out for. Third, the loss is improved. I believe the previous losses were 2.2 and 2.16, so we got slightly improved results. The reason is that we are spending more cycles, more time, optimizing the neural net instead of squashing down the weights. This is something to be aware of.

Let’s look at the second problem. Let me reinitialize our neural net and reintroduce the break statement so that we have a reasonable initial loss. Everything looks good on the level of the loss, but there is still a deeper problem looking inside the neural net and its initialization. The logits are now okay. The problem is with the values of H, the activations of the hidden states. If we visualize this tensor H, the problem is that many of the elements are one or negative one. Recall that the tanh function squashes numbers into a range between negative one and one. It does so smoothly. Let's look at the histogram of H to get a better idea of the distribution of the values inside this tensor. We see that H has 32 examples and 200 activations in each example. We can view it as `-1` to stretch it out into a large vector. Then, we can call `tolist()` to convert it into a Python list of floats. We can pass this into `plt.hist()` for a histogram, with 50 bins and a semicolon to suppress the output. The histogram shows that most values are close to negative one and one. This means that the tanh is very active. Let's look at why that is. We can look at the pre-activations that feed into tanh. The pre-activations have a broad distribution, with numbers between -15 and 15. Everything is being squashed by the tanh function, resulting in the range of -1 and 1. Lots of the numbers here take on very extreme values.

If you are new to neural networks, you might not see this as an issue. However, if you are well-versed in the dark arts of backpropagation, having an intuitive sense of how gradients flow through a neural net, you are sweating while looking at the distribution of tanh(h) activations. Keep in mind that during backpropagation, we are doing backward passes, starting at the loss, and flowing through the network backwards. In particular, we're going to backpropagate through this `torch.tanh()`. This layer is made up of 200 neurons for each example, and it implements an element-wise tanh. Let's look at what happens in the tanh backward pass. We can go back to our micrograd code from the first lecture and see how we implemented tanh. The input was X, and we calculate T, which is the tanh of X. T is between -1 and 1, and it is the output of tanh. In the backward pass, how do we backpropagate through tanh? We take the output gradient and multiply it by the local gradient, which takes the form of `1 - t^2`. If the outputs of your tanh are very close to -1 or 1, then plugging in one here will result in a zero, multiplying the output gradient. This kills the gradient, effectively stopping backpropagation through the tanh unit. Similarly, when T is -1, this again becomes zero. The output gradient stops. Intuitively, this makes sense because it is a tanh neuron. If its output is very close to one, we are in the tail of the tanh. Changing the input will not impact the output of the tanh because it's in a flat region. There’s no impact on the loss. The weights and biases, along with the tanh neuron, do not impact the loss because the output of the tanh unit is in a flat region. The gradient is basically zero. When T equals zero, we get one times the output gradient. When tanh takes on a value of zero, the output gradient passes through. If T is equal to zero, then the tanh unit is inactive and the gradient just passes through. The more you are in the flat tails, the more the gradient is squashed. The gradient flowing through tanh can only ever decrease, and the amount it decreases is proportional to the square. The concern here is that if all outputs H are in the flat regions of -1 and 1, then the gradients flowing through the network will be destroyed at this layer.

There is a redeeming quality here. We can get a sense of the problem. I have some code here to show you this. We want to look at H and take the absolute value. We want to see how often it is in a flat region, greater than 0.99. We get a Boolean tensor. A white pixel is true, and a black pixel is false. We see that a lot of it is white. This tells us that all the tanh neurons were very active. They are in a flat tail. In all these cases, the backward gradient is destroyed. We would be in trouble if for any of these 200 neurons, the entire column was white. We would have a dead neuron. This is a tanh neuron where the initialization of weights and biases is such that no single example activates this tanh in the active part of the tanh. If all examples land in the tail, this neuron will never learn. It is a dead neuron. We are looking for columns of white. I don’t see a single neuron that is all white. For every tanh neuron, we have some examples that activate them in the active part of the tanh. Some gradients will flow through, this neuron will learn, and it will change. Sometimes you get cases with dead neurons. The way this manifests is for a tanh neuron. No matter what inputs you plug in, the tanh neuron will always fire completely at one or completely at negative one. It will not learn because all the gradients will be zeroed out. This is true not just for tanh but also for many other nonlinearities used in neural networks. Sigmoid will have the exact same issue because it is a squashing neuron. The same will apply to sigmoid. The same will also apply to ReLU. ReLU has a flat region below zero. If you have a ReLU neuron, and the pre-activation is positive, it will pass through. If the pre-activation is negative, it will shut it off. Because the region is flat, backpropagation will set the gradient exactly to zero, instead of to a very small number depending on how positive or negative T is. You can get a dead ReLU neuron. A dead ReLU neuron is when a neuron with a ReLU nonlinearity never activates. It is always in the flat region. The weights and bias will never learn. They will never get a gradient because the neuron never activated. This can happen at initialization because weights and biases make some neurons dead by chance. It can also happen during optimization if you have a high learning rate. Sometimes neurons get too much gradient and get knocked off the data manifold. Then, no example ever activates the neuron. The neuron is dead. If your learning rate is very high, for example, and you have a neural net with ReLU neurons, you might get some loss. Then you can go through the training set and find neurons that never activate. Those are dead neurons. They will never turn on. During training, ReLU neurons are changing and moving. However, because of a high gradient somewhere, they get knocked off, and from then on, they are dead. Other nonlinearities, like Leaky ReLU, will not suffer from this issue as much because they don't have flat tails. You will almost always get gradients. ELU is also frequently used. It might suffer from this issue because it has flat parts. This is something to be aware of and be concerned about. In this case, we have too many activations that take on extreme values. Because there is no column of white, I think we will be okay. The network optimizes and gives us a pretty decent loss, but it is not optimal. We don’t want this, especially during initialization. This pre-activation that's flowing into the tanh is too extreme. It is too large, creating a distribution that is too saturated on both sides of the tanh. This means that there is less training for the neurons because they update less frequently.

How do we fix this? The pre-activation, which comes from the concatenation of the characters, are uniform Gaussians. This is multiplied by W1 plus B1. The pre-activation is too far from zero, and that’s causing the issue. We want the pre-activation to be closer to zero, similar to what we had with logits. It's okay to set the biases to a small number. We can multiply by 0.01 to get a little bit of entropy. I sometimes like to do that so there is some variation in the original initialization of these tanh neurons. I find that in practice, it can help optimization. We can also squash the weights. Let's multiply everything by 0.1. Let's rerun the first batch and look at the histogram. Now, we have a much better histogram. The pre-activations are between -1.5 and 1.5. We expect much less white. There is no white. There are no neurons saturated above 99 in either direction. This is a pretty decent place to be. Maybe we can go up a little bit, to 0.2. Something like this is a nice distribution. This is what our initialization should be. Let me erase these and run the full optimization starting with this initialization without the break. We see that the optimization finished. This is the result that we get. The previous losses were a validation loss of 2.17, which became 2.13, which became 2.10. The reason this is happening is that our initialization is better. We're spending more time doing productive training instead of non-productive training where the gradients are set to zero and we have to learn very simple things like the overconfidence of the softmax at the beginning. We are spending cycles squashing down the weight matrix. This is illustrating initialization and its impact on performance. By being aware of the internals of these neural nets, we can understand activations and gradients. We're working with a very small network. This is just a one-layer MLP. The optimization problem is quite easy and forgiving. Even though our initialization was terrible, the network still learned eventually. It just got a worse result. This is not the case in general. When working with much deeper networks that have, say, 50 layers, things can get more complicated. These problems stack up. You can get into a place where the network is not training at all if your initialization is bad enough. The deeper your network is, the more complex it is, and the less forgiving it is to these errors. This is something to definitely be aware of, scrutinize, plot, and be careful with.

Okay, this worked for us, but we have magic numbers like 0.2. Where do I get this? How am I supposed to set these with a large neural net with lots of layers? Obviously, no one does this by hand. There are principled ways of setting these scales that I would like to introduce now. Here is some code that I have prepared to motivate the discussion. We have random input X drawn from a Gaussian. There are 1,000 examples that are 10-dimensional. There's a weight layer, also initialized using a Gaussian, just like we did before. These neurons in the hidden layer look at 10 inputs. There are 200 neurons in this hidden layer. We have the multiplication X * W to get the pre-activations of these neurons. We are looking at what happens. Suppose these are uniform Gaussians and these weights are uniform Gaussians. If I do X * W, and we forget about the bias and the nonlinearity for now, what is the mean and the standard deviation of these Gaussians? The input is a normal Gaussian with mean zero and standard deviation one. The standard deviation is a measure of the spread. Once we multiply here and look at the histogram of Y, the mean stays at zero because this is a symmetric operation. The standard deviation has expanded to three. The input standard deviation was one, but now it has grown to three. This Gaussian is expanding. We don't want that. We want most of the neural net to have relatively similar activations, a unit Gaussian roughly throughout the neural net. How do we scale the W’s to preserve this distribution, to remain a Gaussian? Intuitively, if we multiply the elements of W by a larger number, like five, the Gaussian grows in standard deviation. Now we're at 15. The numbers in the output Y take on more extreme values. If we scale down by 0.2, conversely, the Gaussian is getting smaller and smaller. The standard deviation is 0.6. What do we multiply by here to exactly preserve the standard deviation to be one? The correct answer, when you work out the variance of this multiplication, is that you are supposed to divide by the square root of the fan-in. The fan-in is the number of input elements, 10 in this case. We are supposed to divide by the square root of 10. This is one way to do the square root, raising it to a power of 0.5. When you divide by the square root of 10, we see that the output Gaussian has a standard deviation of exactly one. Unsurprisingly, a number of papers have looked into how to initialize neural networks to obtain this unit variance.
