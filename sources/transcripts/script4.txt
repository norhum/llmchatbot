Okay, let's start cleaning up this lecture transcript.

"Hi everyone. Today, we are continuing our implementation of Make More Now. So far, we've reached multi-layer perceptrons, and our neural net looked like this, which we've been implementing over the last few lectures. I'm sure everyone is very excited to move on to recurrent neural networks and all of their variants, how they work, and their cool-looking diagrams. It's very exciting and interesting, and we will get a better result, but unfortunately, I think we have to remain here for one more lecture. The reason is that we've already trained this multi-layer perceptron, and we are getting pretty good loss. I think we have a decent understanding of the architecture and how it works. However, the line of code that I take issue with is here: `loss.backward()`.  We are using PyTorch autograd to calculate all of our gradients, and I would like to remove the use of `loss.backward()`. I would like us to write our backward pass manually on the level of tensors. I think this is a very useful exercise for the following reasons. I actually have an entire blog post on this topic, but I'd like to call backpropagation a leaky abstraction. What I mean by that is, backpropagation doesn't just make your neural networks work magically. It's not the case that you can just stack up arbitrary Lego blocks of differentiable functions, cross your fingers, back propagate, and everything is great. Things don't just work automatically. It is a leaky abstraction in the sense that you can shoot yourself in the foot if you do not understand its internals. It will magically not work or not work optimally, and you will need to understand how it works under the hood if you are hoping to debug it and address it in your neural net. So, this blog post from a while ago goes into some of those examples. For example, we've already covered some of them already. The flat tails of these functions, and how you do not want to saturate them too much because your gradients will die, the case of dead neurons, which I've already covered as well, and the case of exploding or vanishing gradients in the case of recurrent neural networks, which we are about to cover. You will often come across some examples in the wild. This is a snippet that I found in a random codebase on the internet, where they actually have a very subtle but pretty major bug in their implementation. The bug points at the fact that the author of this code does not actually understand backpropagation. They're trying to do here is they're trying to clip the loss at a certain maximum value. But actually, what they're doing is trying to collect the gradients to have a maximum value instead of trying to clip the loss at a maximum value. Indirectly, they are basically causing some of the outliers to be ignored. Because when you clip a loss of an outlier, you are setting its gradient to zero. So, have a look through this and read through it, but there's basically a bunch of subtle issues that you're going to avoid if you actually know what you're doing. That's why I don't think it is okay for us to ignore how it works just because PyTorch or other frameworks offer autograd. We've actually already covered autograd, and we wrote micrograd, but micrograd was an autograd engine only on the level of individual scalars. The atoms were single individual numbers, and I don't think it's enough. I'd like us to think about backpropagation on the level of tensors as well. So, in summary, I think it's a good exercise. I think it is very, very valuable. You're going to become better at debugging neural networks and making sure that you understand what you're doing. It is going to make everything fully explicit, so you're not going to be nervous about what is hidden away from you. In general, we're going to emerge stronger. So let's get into it. A bit of a fun historical note is that today writing your backward pass by hand and manually is not recommended, and no one does it except for the purposes of exercise. But about 10 years ago in deep learning, this was fairly standard, and in fact, pervasive. At the time, everyone used to write their own backward pass by hand manually, including myself. It's just what you would do. So we used to write backward passes by hand, and now everyone just calls `loss.backward()`. We've lost something. I want to give you a few examples of this. Here's a 2006 paper from Jeff Hinton and Ruslan Salakhutdinov in *Science* that was influential at the time. This was training some architectures called Restricted Boltzmann Machines, and basically, it's an autoencoder trained here. This is from roughly 2010, I had a library for training Restricted Boltzmann Machines. At the time, it was written in MATLAB. Python was not used for deep learning pervasively. It was all MATLAB. MATLAB was this scientific computing package that everyone would use. So, we would write MATLAB, which is barely a programming language. It had a very convenient tensor class and was a computing environment. You would run it on a CPU of course, but you would have very nice plots to go with it and a built-in debugger. It was pretty nice. Now, the code in this package in 2010 that I wrote for fitting Restricted Boltzmann Machines, to a large extent, is recognizable. But I wanted to show you how you would. I'm creating the data in the XY batches, I'm initializing the neural net, so it's got weights and biases, just like we're used to. Then, this is the training loop where we actually do the forward pass. Here, at this time, they didn't even necessarily use backpropagation to train neural networks. This, in particular, implements contrastive divergence, which estimates a gradient. Then here, we take that gradient and use it for a parameter update along the lines that we're used to. Yeah, here, but you can see that basically, people are meddling with these gradients directly and inline themselves. It wasn't that common to use an autograd engine. Here's one more example from a paper of mine from 2014 called "Fragmented Embeddings". Here, what I was doing was aligning images and text. It's kind of like CLIP if you're familiar with it, but instead of working on the level of entire images and entire sentences, it was working on the level of individual objects and little pieces of sentences. I was embedding them and then calculating very much like a CLIP-like loss. I dug up the code from 2014 of how I implemented this, and it was already in NumPy and Python. Here, I'm plotting the cost function, and it was standard to implement not just the cost, but also the backward pass manually. Here, I'm calculating the image embeddings, sentence embeddings, the loss function. I calculate the cost; this is the loss function. Then, once I have the loss function, I do the backward pass right here. So, I backward through the loss function and through the neural net. I append regularization. Everything was done by hand manually. You would just write out the backward pass, and then you would use a gradient checker to make sure that your numerical estimate of the gradient agrees with the one you calculated during backpropagation. This was very standard for a long time, but today, of course, it is standard to use an autograd engine. But it was definitely useful, and I think people sort of understood how these neural networks work on a very intuitive level. I think it's a good exercise again, and this is where we want to be. Okay, so just as a reminder from our previous lecture, this is the Jupyter notebook that we implemented at the time. We're going to keep everything the same. We're still going to have a two-layer multi-layer perceptron with a batch normalization layer. The forward pass will be basically identical to this lecture. But here, we're going to get rid of `loss.backward()` and instead, we're going to write the backward pass manually. Here's the starter code for this lecture. We are becoming a backprop ninja in this notebook. The first few cells here are identical to what we are used to. We are doing some imports, loading the dataset, and processing the dataset. None of this changed. Now, here I'm introducing a utility function that we're going to use later to compare the gradients. In particular, we are going to have the gradients that we estimate manually ourselves, and we're going to have gradients that PyTorch calculates. We're going to be checking for correctness, assuming, of course, that PyTorch is correct. Then here, we have the initialization that we are quite used to. We have our embedding table for the characters, the first layer, the second layer, and the batch normalization in between. Here's where we create all the parameters. Now you will note that I changed the initialization a little bit to be small numbers. Normally, you would set the biases to be all zero. Here I am setting them to be small random numbers, and I'm doing this because if your variables are initialized to exactly zero, sometimes what can happen is that can mask an incorrect implementation of a gradient. Because when everything is zero, it simplifies and gives you a much simpler expression of the gradient than you would otherwise get. So, by making it small numbers, I'm trying to unmask those potential errors in these calculations. You also notice that I'm using `b1` in the first layer. I'm using a bias despite batch normalization right afterwards. This would typically not be what you do because we talked about the fact that you don't need the bias, but I'm doing this here just for fun because we're going to have a gradient with respect to it, and we can check that we are still calculating it correctly, even though this bias is spurious. Here, I'm calculating a single batch, and then here, I'm doing a forward pass. Now you'll notice that the forward pass is significantly expanded from what we are used to. Here, the forward pass was just here. Now, the reason that the forward pass is longer is for two reasons. Number one, here, we just had an `F.cross_entropy`, but here I am bringing back an explicit implementation of the loss function. Number two, I've broken up the implementation into manageable chunks. So, we have a lot more intermediate tensors along the way in the forward pass. That's because we are about to go backwards and calculate the gradients in this backpropagation from the bottom to the top. So we're going to go upwards. Just like we have, for example, the `logprobs` tensor in a forward pass, in the backward pass, we're going to have a `d_logprobs` which is going to store the derivative of the loss with respect to the `logprobs` tensor. So we're going to be prepending "d" to every one of these tensors and calculating it along the way of this backpropagation. As an example, we have `b_in_raw` here. We're going to be calculating a `db_in_raw`. Here, I'm telling PyTorch that we want to retain the grad of all these intermediate values because here in exercise one, we're going to calculate the backward pass. So we're going to calculate all these `d` values, d variables, and use the `CMP` function I've introduced above to check our correctness with respect to what PyTorch is telling us. This is going to be exercise one, where we sort of backpropagate through this entire graph. Now, just to give you a very quick preview of what's going to happen in exercise two and below, here we have fully broken up the loss and backpropagated through it manually in all the little atomic pieces that make it up. But here, we're going to collapse the loss into a single cross-entropy call. Instead, we're going to analytically derive, using math and paper and pencil, the gradient of the loss with respect to the logits. Instead of backpropagating through all of its little chunks one at a time, we're just going to analytically derive what that gradient is, and we're going to implement that, which is much more efficient, as we'll see in a bit. Then, we're going to do the exact same thing for batch normalization. Instead of breaking up batchnorm into all the tiny components, we're going to use pen and paper, mathematics, and calculus to derive the gradient through the batch normalization layer. So we're going to calculate the backward pass through batchnorm in a much more efficient expression, instead of backpropagating through all of its little pieces independently. That's going to be exercise three. Then, in exercise four, we're going to put it all together. This is the full code of training this two-layer MLP. We're going to basically insert our manual backprop, and we're going to take out `loss.backward()`. You will see that you can get all the same results using fully your own code. The only thing we're using from PyTorch is `torch.tensor` to make the calculations efficient. Otherwise, you will understand fully what it means to forward and backward a neural net and train it. I think that'll be awesome. So let's get to it. Okay, so I read all the cells of this notebook all the way up to here, and I'm going to erase this, and I'm going to start implementing the backward pass, starting with `d_logprobs`. So we want to understand what should go here to calculate the gradient of the loss with respect to all the elements of the `logprobs` tensor. Now, I'm going to give away the answer here, but I wanted to put a quick note that I think would be most pedagogically useful for you. Go into the description of this video and find the link to this Jupyter notebook. You can find it both on GitHub, but you can also find a Google Colab link. So, you don't have to install anything. You'll just go to a website on Google Colab, and you can try to implement these derivatives or gradients yourself. If you are not able to come up with the answer, then come to my video and see me do it. So work in tandem, try it yourself first, and then see me give away the answer. I think that'll be most valuable to you, and that's how I recommend you go through this lecture. So we are starting here with `d_logprobs`. Now, `d_logprobs` will hold the derivative of the loss with respect to all the elements of `logprobs`. What is inside `logprobs`? The shape of this is 32 by 27. It's not going to surprise you that `d_logprobs` should also be an array of size 32 by 27, because we want the derivative of the loss with respect to all of its elements. The sizes of those are always going to be equal. Now, how does `logprobs` influence the loss? Okay, loss is `-logprobs[range(N), YB].mean()`. Just as a reminder, `YB` is just basically an array of all the correct indices. What we're doing here is we're taking the `logprobs` array of size 32 by 27, and in every single row, we are plucking out the index 8 and then 14 and 15, and so on. So we're going down the rows; that's the iterator `range(N)`. We are always plucking out the index of the column specified by this tensor `YB`. So, in the zeroth row, we are taking the eighth column; in the first row, we are taking the 14th column, etc. `logprobs[range(N), YB]` plugs out all those log probabilities of the correct next character in a sequence. The shape of this or the size of it is, of course, 32, because our batch size is 32. These elements get plugged out, and then their mean, and the negative of that, becomes loss. I always like to work with simpler examples to understand the numerical form of the derivative. What's going on here is, once we've plucked out these examples, we're taking the mean, and then the negative. So the loss basically, I can write it this way, is the negative of say a + b + c, and the mean of those three numbers would be, say, negative, we'd divide by three. That would be how we achieve the mean of three numbers, a, b, c, although we actually have 32 numbers here. What is basically the `loss` by say, like `da`? Well, if we simplify this expression mathematically, this is negative one over three of a plus negative one over three of b plus negative one over three of c. What is `d_loss` by `da`? It's just negative one over three. You can see that if we don't just have a, b, and c, but we have 32 numbers, then `d_loss` by d every one of those numbers is going to be one over N. More generally, because n is the size of the batch, 32 in this case. So `d_loss` by `d_logprobs` is negative one over n in all these places. What about the other elements inside `logprobs`, because `logprobs` is a large array? You see that `logprobs` shape is 32 by 27, but only 32 of them participate in the loss calculation. What is the derivative of all the other most of the elements that do not get plucked out here? Their loss, intuitively, is zero. Their gradient, intuitively, is zero. That's because they did not participate in the loss. Most of these numbers inside this tensor do not feed into the loss. If we were to change these numbers, then the loss doesn't change, which is the equivalent way of saying that the derivative of the loss with respect to them is zero. They don't impact it. Here's a way to implement this derivative. We start out with `torch.zeros` of shape 32 by 27, or, let's just say, instead of doing this, because we don't want to hardcode numbers, let's do `torch.zeros_like(logprobs)`. Basically, this is going to create an array of zeros exactly in the shape of `logprobs`. Then, we need to set the derivative of negative one over n inside exactly these locations. Here's what we can do. `logprobs[range(N), YB]` will be set to negative one over n. Just like we derived here. Let me erase all this reasoning. This is the candidate derivative for `d_logprobs`. Let's uncomment the first line and check that this is correct. Okay, so CMP ran, and let's go back to CMP. You see that what it's doing is, it's calculating if the calculated value by us, which is `dt`, is exactly equal to `t.grad`, as calculated by PyTorch. This is making sure that all the elements are exactly equal, and then converting this to a single Boolean value, because we don't want the Boolean tensor. We just want a Boolean value. Here we are making sure that okay, if they're not exactly equal, maybe they are approximately equal because of some floating point issues, but they're very, very close. Here we are using `torch.allclose`, which has a little bit of a wiggle available, because sometimes you can get very, very close, but if you use a slightly different calculation because of floating point arithmetic, you can get a slightly different result. This is checking if you get an approximately close result. Then here, we are checking the maximum, basically, the value that has the highest difference, and what is the difference in the absolute value difference between those two. Here, we see that we actually have exact equality. Therefore, of course, we also have an approximate equality, and the maximum difference is exactly zero. Basically, our `d_logprobs` is exactly equal to what PyTorch calculated to be `logprobs.grad` in its backpropagation. So far, we're working pretty well. Okay, let's now continue our backpropagation. We have that `logprobs` depends on `probs` through a log. All the elements of `probs` are being element-wise applied `log` to. Now, if we want `d_probs`, then remember your micrograd training. We have a log node, it takes in `probs` and creates `logprobs`, and the `d_probs` will be the local derivative of that individual operation `log`, times the derivative of loss with respect to its output, which in this case is `d_logprobs`. What is the local derivative of this operation? We are taking log element-wise. We can come here, and we can see that d by dx of log of x is just simply one over x. Therefore, in this case, x is `probs`. We have d by dx is one over x, which is one over `probs`. This is the local derivative. Then times, because of the chain rule, `d_logprobs`. Let me uncomment this and let me run the cell in place, and we see that the derivative of probs, as we calculated here, is exactly correct. Notice here how this works. The `probs` is going to be inverted and then element-wise multiplied here. If your `probs` is very, very close to one, that means your network is currently predicting the character correctly. This will become one over one, and `d_logprobs` just gets passed through. But if your probabilities are incorrectly assigned, so if the correct character here is getting a very low probability, then 1.0 divided by it will boost this and then multiply by the `d_logprobs`. Basically, what this line is doing intuitively is, it's taking the examples that have a very low probability currently assigned, and it's boosting their gradient. You can look at it that way. Next up is `count_sum_inv`. We want the derivative of this. Let me just pause here and introduce what's happening here in general, because I know it's a little bit confusing. We have the logits that come out of the neural net. Here what I'm doing is, I'm finding the maximum in each row, and I'm subtracting it for the purposes of numerical stability. We talked about how if you do not do this, you run into numerical issues if some of the logits take on too large values because we end up exponentiating them. This is done just for safety numerically. Then here is the exponentiation of all the logits to create our counts. Then we want to take the sum of these counts and normalize, so that all of the probs sum to one. Here, instead of using one over `count_sum`, I use raised to the power of negative one. Mathematically they are identical. I just found that there's something wrong with the PyTorch implementation of the backward pass of division, and it gives like a `NaN` result, but that doesn't happen for `** -1`. That's why I'm using this formula instead. But basically, all that's happening here is, we got the logits, we're going to exponentiate all of them and want to normalize the counts to create our probabilities. It's just that it's happening across multiple lines. Here, we want to first take the derivative. We want to backpropagate into `count_sum_inv` and then into `counts` as well. What should be `d_count_sum_inv`? We actually have to be careful here because we have to scrutinize and be careful with the shapes. `counts.shape` and then `count_sum_inv.shape` are different. In particular, `counts` is 32 by 27, but `count_sum_inv` is 32 by 1. In this multiplication here, we also have an implicit broadcasting that PyTorch will do because it needs to take this column tensor of 32 numbers and replicate it horizontally 27 times to align these two tensors so it can do an element-wise multiply. Really what this looks like is the following, using a toy example again. What we really have here is `probs` is `counts` times `count_sum_inv`. It's `c = a * b`, but a is 3 by 3, and b is just 3 by 1, a column tensor. PyTorch internally replicated these elements of b, and it did that across all the columns. For example, b1, which is the first element of b, would be replicated here across all the columns in this multiplication. Now we're trying to backpropagate through this operation to `count_sum_inv`. When we're calculating this derivative, it's important to realize that this looks like a single operation, but it actually is two operations applied sequentially. The first operation that PyTorch did is, it took this column tensor and replicated it across all the columns, basically 27 times. That's the first operation. It's a replication. Then the second operation is the multiplication. Let's first backpropagate through the multiplication. If these two arrays are of the same size, and we just have a and b of both of them 3 by 3, then how do we backpropagate through a multiplication? If we just have scalars, and not tensors, then if you have `c = a * b`, then what is the derivative of c with respect to b? It's just a. That's the local derivative. Here in our case, undoing the multiplication and backpropagating through just the multiplication itself, which is element-wise, is going to be the local derivative, which in this case is simply `counts`, because counts is the a. This is the local derivative, and then times, because of the chain rule, `d_probs`. This here is the derivative, or the gradient, but with respect to replicated b. But we don't have a replicated b, we just have a single b column. How do we now backpropagate through the replication? Intuitively, this b1 is the same variable, and it's just reused multiple times. You can look at it as being equivalent to a case we've encountered in micrograd. Here, I'm just pulling out a random graph we used in micrograd. We had an example where a single node has its output feeding into two branches of basically the graph, until the last function, and we're talking about how the correct thing to do in the backward pass is, we need to sum all the gradients that arrive at any one node. Across these different branches, the gradients would sum. If a node is used multiple times, the gradients for all of its uses sum during backpropagation. Here, b1 is used multiple times in all these columns. Therefore, the right thing to do here is to sum horizontally across all the rows. I'm going to sum in dimension one, but we want to retain this dimension, so that the `count_sum_inv` and its gradient are going to be exactly the same shape. We want to make sure that we keep it as true, so we don't lose this dimension. This will make the `count_sum_inv` be exactly shape 32 by 1. So revealing this comparison as well, and running this, we see that we get an exact match. This derivative is exactly correct. Let me erase this. Now, let's also backpropagate into `counts`, which is the other variable here to create probs. From `probs` to `count_sum_inv` we just did that. Let's go into `counts` as well. So, `d_counts` will be, the chances are, a. So, `dc` by `da` is just b. Therefore, it's `count_sum_inv`, and then times, the chain rule, `d_probs`. Now `count_sum_inv` is 32 by 1. `d_probs` is 32 by 27. Those will broadcast fine and will give us `d_counts`. There's no additional summation required here. There will be a broadcasting that happens in this multiply here, because `count_sum_inv` needs to be replicated again to correctly multiply `d_probs`. That's going to give the correct result, as far as a single operation is concerned. So we backpropagated from `probs` to `counts`. But we can't actually check the derivative `counts`. I have it much later on. The reason for that is, because `count_sum_inv` depends on `counts`. So there's a second branch here that we have to finish, because `count_sum_inv` backpropagates into `count_sum`, and `count_sum` will then backpropagate into `counts`. So `counts` is a node that is being used twice. It's used right here in two `probs`, and it goes through this other branch through `count_sum_inv`. Even though we've calculated the first contribution of it, we still have to calculate the second contribution of it later. Okay, so we're continuing with this branch. We have the derivative for `count_sum_inv`. Now we want the derivative of `count_sum`. So, `d_count_sum` equals what is the local derivative of this operation? This is basically an element-wise one over `count_sum`. `count_sum` raised to the power of negative one is the same as one over `count_sum`. If we go to Wolfram Alpha, we see that `x` to the negative one, d by dx of it, is basically negative `x` to the negative two. One negative one over squared is the same as negative `x` to the negative two. `d_count_sum` here will be, the local derivative is going to be negative `count_sum` to the negative two. That's the local derivative. Times chain rule, which is `d_count_sum_inv`. That's `d_count_sum`. Let's uncomment this and check that I am correct. Okay, so we have perfect equality, and there's no sketchiness going on here with any shapes, because these are of the same shape. Okay, next up, we want to backpropagate through this line. We have that `count_sum` is `counts.sum` along the rows. I wrote out some help here. We have to keep in mind that `counts` of course is 32 by 27 and `count_sum` is 32 by 1. In this backpropagation, we need to take this column of derivatives and transform it into an array of derivatives, a two-dimensional array. What is this operation doing? We're taking in some kind of an input like say, a 3x3 matrix, `a`, and we are summing up the rows into a column tensor `b1, b2, b3`, that is basically this. Now we have the derivatives of the loss with respect to b, all the elements of b, and now we want the derivative of loss with respect to all these little a's. How do the b's depend on the a's is basically what we're after? What is the local derivative of this operation? We can see here that b1 only depends on these elements here. The derivative of b1 with respect to all of these elements down here is zero. But for these elements here, like a11, a12, etc., the local derivative is one. `db1` by `da11`, for example, is one. So it's one, one, and one. When we have the derivative of loss with respect to b1, the local derivative of b1 with respect to these inputs is zeros here, but it's one on these guys. In the ch"

The chain rule states that the local derivative multiplied by the derivative of B1. Since the local derivative is one on these three elements, multiplying by the derivative of B1 results in the derivative of B1. This can be viewed as a router where the gradient from above is routed equally to all elements participating in the addition. Therefore, the derivative of B1 flows equally to the derivatives of a11, a12, and a13. If we have the derivatives of all the elements of B, represented as a column tensor, which is the D counts sum calculated earlier, we see that these flow to all the elements of a horizontally. We want to take the D counts sum, which is of size 30 by 1, and replicate it 27 times horizontally to create a 30 by 27 array. There are multiple ways to implement this, including replicating the tensor. A cleaner approach involves using a two-dimensional array of ones with the shape of counts, 30 by 27, multiplied by the D counts sum. This uses broadcasting to implement the replication. However, D counts was already calculated for the first branch, and we are now finishing the second branch. So, these gradients need to be added together using plus equals. Let's comment out the comparison and verify that we have the correct result. PyTorch agrees with this gradient. Now, consider 'counts' as an element-wise exponential of 'norm logits'. To calculate 'D norm logits', since this is an element-wise operation, we use the local derivative of e to the x, which is just e to the x. We already calculated this, and it is stored in 'counts'. So, the local derivative times the 'D counts' is equivalent to 'counts' times 'D counts'. Let's erase the previous calculation and verify the new one. The verification looks good, confirming the 'norm logits' calculation. We are now at the line that computes 'norm logits' and want to back propagate through it to calculate the derivatives of logits and logit Maxes. The shapes are not the same, leading to implicit broadcasting. 'Norm logits' and 'logits' have shapes of 32 by 27, but 'logit Maxes' is 32 by 1. In the subtraction, every element of 'C' (result) comes from the corresponding element of 'A' minus the associated element of 'B'. The derivatives of each element of 'C' with respect to the inputs are one for 'A' and negative one for 'B'. Thus, the derivatives on 'C' flow to the corresponding 'A's and 'B's. The 'B's are broadcast, requiring an additional sum, similar to previous operations. The derivatives for 'B's undergo a minus sign because the local derivative is negative one. We implement this by setting 'delogits' to a copy of 'D norm logits' using 'clone'. The 'loaded Maxes' will be the negative of 'D norm logits', due to the negative sign. 'Logit Maxes' is a column and is replicated. So, we do a sum along axis one with 'keepdims=True' to avoid dimension destruction. This makes 'logic Maxes' the same shape. This 'delogits' is not final, because we have a second gradient signal coming into 'logits'. For now, the 'logic Maxes' is the final derivative. Let's verify that the 'logit Maxes' derivative matches PyTorch's. PyTorch agrees with the 'logit Maxes' gradient. This was the derivative through this line. Now, let's examine 'logic Maxes' and their gradients. This is primarily for the numerical stability of softmax. If you add or subtract any value equally to all elements of the logits row, it won’t change softmax value. This ensures that values do not overflow and that the highest value in each logits row is zero, which is numerically safe. Therefore, changing 'logit Maxes' shouldn't change the probs, the loss or its gradient. These values should be zero. Due to floating-point issues, the values are very small, but not exactly zero, like 1e-9 or 1e-10. This reflects that 'logit Maxes' doesn't impact the loss. It may seem unusual to backpropagate through this branch since cross-entropy implementations typically skip this branch for numerical stability. However, even with a breakdown into full atoms, the computation for numerical stability yields the correct outcome with extremely small gradients, demonstrating that the values of these do not matter with respect to the final loss. Now, we proceed with back propagation through this line. We have the 'logit Maxes' derivative, and we now back propagate into logits through the second branch. In PyTorch, the max operation returns both the maximum values and the indices at which they occurred. In the backward pass, knowing where the maximum values came from is useful for back propagation. The derivative flowing through this should be one for the appropriate entry that was plucked out, times the derivative of the 'logit Maxes'. We take the derivative of 'logit Maxes' and scatter it to the correct positions in the 'logits' where the maximum values came from. One way to do this is with a one-hot vector. The 'max' over the first dimension’s indices are converted into one-hot vectors with a dimension of 27. This one-hot vector is then multiplied by the 'logit Maxes', which is a 32 by 1 column. Broadcasting replicates this column, and an element-wise multiplication routes the 'logit Maxes' to the correct position in 'logits'. We are using plus equals because we already calculated the 'logits' derivative, and this is a second branch. Let's verify that the new 'logits' derivative is correct. We have the correct answer. Next, we continue with logits, an outcome of a matrix multiplication and a bias offset. Logits is 32 by 27, H is 32 by 64, and W projects the 64-dimensional vector to 27 dimensions. There is a 27-dimensional bias. The plus operation here broadcasts. The 27-dimensional bias is padded to the left with a dimension of one, making it a row vector replicated vertically 32 times, resulting in a 32 by 27 matrix for element-wise multiplication. We need to back propagate from 'logits' to the hidden states, the weight matrix, and the bias. We can derive the derivative using first principles by working through a small example. We have A multiplied by B plus C resulting in D and the loss derivative with respect to D. We need to determine the derivative of the loss with respect to A, B, and C. Consider a simple two-dimensional example of a matrix multiplication of a two-by-two matrix with another two-by-two matrix, plus a two element bias vector. The bias vector is replicated vertically. By breaking down the matrix multiplication and examining how each element of D is calculated from its components, we can derive the formulas for the gradients. d11 is the result of a dot product between the first row of a and the first column of b. It is obvious that the derivatives for each element of A can be found by differentiating the simple formulas. For example, we want to find the derivative of the loss with respect to a11. We see that a11 appears twice in our expression and influences d11 and d12. Therefore, the derivative of the loss with respect to a11 is the derivative of the loss with respect to d11 times the local derivative of d11 with respect to a11, plus the derivative of the loss with respect to d12 times the local derivative of d12 with respect to a11. The local derivative in the first case is b11 and in the second case, it is b12. We add the contributions of both. We can perform the same analysis for all other elements of A.  The derivative of the loss with respect to matrix A, arranged as the same shape as A, can be expressed as a matrix multiplication of the derivative of the loss with respect to matrix D, times the transpose of B.  If we perform similar analysis for the derivative with respect to matrix B, we find that the derivative of the loss with respect to B is also a matrix multiplication. It is the transpose of matrix A, multiplied by the derivative of the loss with respect to D. For derivatives of vector C with respect to individual elements, the result is the sum of the derivatives of the loss with respect to D across the columns. In summary, the backward path of a matrix multiplication is also a matrix multiplication.  In the scalar case we have D equals A times B plus C, and in the matrix case we have a matrix multiply. The derivative of D with respect to A is the derivative of the loss with respect to D, matrix-multiplied by the transpose of B. The derivative of D with respect to B is the transpose of A multiplied by the derivative of the loss with respect to D. And, the derivative of D with respect to C is the sum of the loss derivative with respect to D across the columns. I often don’t remember those formulas, but I can back propagate using dimensional analysis. For example, if I want to find DH, I know its shape must be the same as the shape of H, which is 32 by 64. I also know DH must result from a matrix multiplication of 'logits' with W2. 'Logits' is 32 by 27, and W2 is 64 by 27. There is only one way for the shapes to work out correctly: the matrix multiplication of delogits and the transpose of W2. Similarly, if we want DW2, which has a shape of 64 by 27, it must come from some matrix multiplication of 'delogits' and H. The only way to get 64 by 27 is the transpose of H multiplied by delogits. Finally, DB2 is the sum, where the only way for the shapes to work out is to sum along the zero axis of delogits. With this method, we do not have to memorize the back propagation formulas. The hacky method of figuring out the matrix multiplications is successful. Now, let’s move the implementation of the backward pass for a linear layer here. We can now uncomment and verify that all three gradients are correct. H, W2, and B2 are correct, so we have back propagated through a linear layer. We have the derivative for H and need to back propagate through the tanH function into H preact. We need to calculate DH preact by back propagating through tanH. We know from micrograd that the derivative of tanH is simple. In particular, if A is equal to tanH of Z, the derivative of A with respect to Z is equal to one minus A squared, where A is the output of the tanH. So the local derivative is 1 minus the output of tanH squared, which is 1 minus H squared in our case. That is the local derivative times the chain rule derivative of H. Let’s come here and see if this gives us the right answer. The result is correct. Now, we have DH preact and need to back propagate to the gain, the B, the raw, and the B and bias of the batch norm. This is an element-wise multiply and must be handled differently than matrix multiplies. The broadcasting behavior of this line of code is important. The B and gain and the B and bias are 1 by 64, while H preact and B and raw are 32 by 64. Let's start with DB and gain. Whenever we have A times B equals C, the local derivative is B. Therefore, the local derivative here is B and raw, and chain rule derivative DH preact. Because B and gain has the shape 1 by 64, but the result is 32 by 64, we sum the rows to maintain the 1 by 64 shape. Therefore, we sum across the examples, which is dimension zero. We also need to keep the dimensions to maintain the tensor shapes. Next is DB and raw, which is B and gain multiplied by DH preact. Here, the shapes work out correctly using broadcasting. Finally, the bias term has a similar behavior. It's the sum of DH preact along the zeroth dimension. We need to sum across the examples, keeping dimensions true, giving us the 1 by 64 result. This is our candidate implementation. Let’s uncomment the check and see if the gradients are correct. All three derivatives are correct. We have completed the back propagation through the batch norm parameters. B and raw is the output of the standardization. Batch norm is broken into manageable pieces for back propagation. We calculate B and mean, the sum of inputs across the examples. B and diff is X minus mu. B and diff squared is x minus mu squared. B and VAR is the variance, the sum of squares. I normalize by 1 over m-1 instead of n to use Bessel’s correction. BNvar inv is one over the square root of variance plus epsilon. Finally, B and raw is equal to B and diff times BNvar inv. We already have back propagated through the line creating H preact, so we have B and raw, and need to back propagate into B and diff and B and VAR M. Now we need to back propagate through the line where B and raw equals B and diff times BN var m. The shapes are as follows: B and VAR m is of shape 1 by 64, which requires broadcasting. It is an element-wise multiplication. To get DB and diff, we multiply B and varm with DB and raw. Conversely, to get DB and VAR inv, we multiply B and diff with DB and raw. Broadcasting needs to be considered. B and VAR m times DB and raw works, but DB and VAR inv is a 32 by 64 result and B and VAR inv is 1 by 64. We need to sum across the examples using keepdim true to keep the shape. Here is the candidate implementation. Let's uncomment these lines. The BMR m derivative is correct, but DB and diff is incorrect, which is expected, because we are not done with it. DB and diff branches into multiple branches and we have only done one of them so far. We still need to come back to the DB and diff. It is good to see the comparison is detecting incorrect gradients. Now we want to back propagate through the line B and VAR inv equals B and VAR plus epsilon to the power of negative point five. The power rule says that the derivative is equal to bringing the exponent down, which is negative 0.5, times x to the power of minus 1.5, in this case the expression to the negative 1.5 power. We need to apply chain rule in our head since the B and VAR with respect to what's inside the brackets in the expression. Because it is an element-wise operation, this derivative is simple.

The local derivative is just one, so there's nothing to do there. This is then multiplied by the global derivative to create the chain rule. This is just times the `dbm_bar`. This is our candidate. Let me bring this down and uncomment the check, and we see that we have the correct result. Before we propagate through the next line, I want to briefly talk about the note here where I'm using the Bessel's correction, dividing by `n - 1` instead of dividing by `n` when I normalize the sum of squares. You'll notice that this is a departure from the paper, which uses one over `n` instead of one over `n - 1`. Their `m` is `RN`, and it turns out that there are two ways of estimating the variance of an array. One is the biased estimate, which is one over `n`, and the other one is the unbiased estimate, which is one over `n - 1`. Confusingly, in the paper, this is not very clearly described, and it's a detail that matters. They are using the biased version during training time, but later, when they are talking about inference, they mention that when they do inference, they are using the unbiased estimate, which is the `n - 1` version, to calibrate the running mean and the running variance. They actually introduce a train-test mismatch where, in training, they use the biased version, and in test time, they use the unbiased version. I find this extremely confusing. You can read more about the Bessel's correction and why dividing by `n - 1` gives you a better estimate of the variance in a case where you have population size or samples for the population that are very small, which is indeed the case for us because we are dealing with many patches, and these mini batches are a small sample of a larger population, which is the entire training set. It turns out that if you estimate it using one over `n`, that almost always underestimates the variance, and it is a biased estimator. It is advised that you use the unbiased version and divide by `n - 1`. You can go through the article I've linked, which describes the full reasoning. I'll link it in the video description. When you calculate the `torch.var` variance, you'll notice that they take the unbiased flag for whether or not you want to divide by `n` or `n - 1`. Confusingly, they do not mention what the default is for unbiased, but I believe that unbiased by default is true. I'm not sure why the docs don't cite that. In `BatchNorm1d`, the documentation is again kind of wrong and confusing. It says that the standard deviation is calculated via the biased estimator, but this is not exactly right, and people have pointed out that it is not right in a number of issues since then. The rabbit hole is deeper, and they follow the paper exactly and use the biased version for training. But when they're estimating the running standard deviation, they're using the unbiased version, so again, there's the train-test mismatch. Long story short, I'm not a fan of train-test discrepancies. I consider the fact that we use the bias version during training and the unbiased version during test time to be a bug. I don't think there's a good reason for that; they don't really go into the detail of the reasoning behind it in this paper. That's why I prefer to use the Bessel's correction in my own work. Unfortunately, `BatchNorm` does not take a keyword argument that tells you whether or not you want to use the unbiased version or the biased version in both train and test. Therefore, anyone using batch normalization has a bit of a bug in the code, in my view. This turns out to be much less of a problem if your mini-batch sizes are larger, but still, I find this kind of unpardonable. Maybe someone can explain why this is okay, but for now, I prefer to use the unbiased version consistently during both training and test time, and that's why I'm using one over `n - 1` here. Okay, let's now backpropagate through this line. The first thing that I always like to do is scrutinize the shapes first. Looking at the shapes of what's involved, I see that `b` and `var` shape is 1 by 64, so it's a row vector, and `bndiff_2` shape is 32 by 64. Clearly, here, we're doing a sum over the zeroth axis to squash the first dimension of the shapes using a sum. That right away hints that there will be some kind of a replication or broadcasting in the backward pass. Maybe you're noticing the pattern, but basically, anytime you have a sum in the forward pass, that turns into a replication or broadcasting in the backward pass along the same dimension. Conversely, when we have a replication or a broadcasting in the forward pass, that indicates variable reuse, and so, in the backward pass, that turns into a sum over the exact same dimension. Hopefully, you're noticing the duality that those two are kind of like the opposite of each other in the forward and backward pass. Once we understand the shapes, the next thing I like to do is look at a toy example in my head to understand roughly how the variable dependencies go in the mathematical formula. Here we have a two-dimensional array of `bndiff_2`, which we are scaling by a constant, and then we are summing vertically over the columns. If we have a 2x2 matrix `a` and then we sum over the columns and scale, we would get a row vector `b1`, `b2`. `b1` depends on `a` in this way, where it is a sum scaled of `a`, and `b2` in this way, where it's the second column summed and scaled. Looking at this, basically what we want to do now is we have the derivatives on `b1` and `b2`, and we want to backpropagate them into the `a`s. It's clear that, differentiating in your head, the local derivative here is one over `n - 1` times one for each one of these `a`s, and the derivative of `b1` has to flow through the columns of `a` scaled by one over `n - 1`. That's roughly what's happening here. Intuitively, the derivative flow tells us that `dbndiff_2` will be the local derivative of this operation. There are many ways to do this, but I like to do something like `torch.ones_like` of `bndiff_2`. I'll create a large two-dimensional array of ones, and then I will scale it, so 1.0 divided by `n - 1`. This is an array of one over `n - 1`, which is sort of like the local derivative. Now, for the chain rule, I will simply just multiply it by `dbm_bar`. Notice what's going to happen here: this is 32 by 64, and this is just 1 by 64. I'm letting the broadcasting do the replication because, internally in PyTorch, `dbm_bar`, which is a 1 by 64 row vector, will get copied vertically until the two are of the same shape, and then there will be an element-wise multiply. Broadcasting is basically doing the replication, and I will end up with the derivatives of `dbndiff_2` here. This is the candidate solution. Let's bring it down here. Let's uncomment this line where we check it, and let's hope for the best. Indeed, we see that this is the correct formula. Next up, let's differentiate here. We have that `bndiff` is element-wise squared to create `bndiff_2`. This is a relatively simple derivative because it's a simple element-wise operation, similar to the scalar case. We have that `dbndiff` should be, if this is x squared, then the derivative is 2x, so it's simply 2 times `bndiff`. That's the local derivative, and then times the chain rule. The shapes of these are the same, so times this. That's the backward pass for this variable. Let me bring that down here. Now we have to be careful because we already calculated `dbm_diff`. This is just the end of the other branch coming back to `bndiff` because `bndiff` was already backpropagated from `bmraw`. We have now completed the second branch, and that's why I have to do `+=`. If you recall, we had an incorrect derivative for `bndiff` before, and I'm hoping that once we append this last missing piece, we have the exact correctness. Let's run, and `dbndiff` now actually shows the exact correct derivative. That's comforting. Okay, let's now backpropagate through this line. The first thing we do, of course, is we check the shapes. I wrote them out here. The shape of `bndiff` is 32 by 64. `hpbn` is the same shape, but `bn_mean` is a row vector, 1 by 64. This minus here will actually do broadcasting, and we have to be careful with that. As a hint to us, because of the duality, a broadcasting in the forward pass means a variable reuse, and therefore, there will be a sum in the backward pass. Let's write out the backward pass. Backpropagating into the `hpbn`, because these are the same shape, the local derivative for each one of the elements here is just one for the corresponding element in here. The gradient simply copies; it's just a variable assignment. I'm just going to clone this tensor just for safety to create an exact copy of `dbndiff`. To backpropagate into `bn_mean`, the local derivative is negative `torch.ones_like` of the shape of `bndiff`, times the derivative here, `dbndiff`. This is the backpropagation for the replicated `bn_mean`. I still have to backpropagate through the replication in the broadcasting, and I do that by doing a sum. I'm going to take this whole thing, and I'm going to do a sum over the zeroth dimension, which was the replication. If you scrutinize this, you'll notice that this is the same shape as that. What I'm doing doesn't actually make that much sense because it's just an array of ones multiplying `dbndiff`. In fact, I can just do this, and that is equivalent. This is the candidate backward pass. Let me copy it here, and then let me comment out this one. Enter, and it's wrong. Actually, sorry, this is supposed to be wrong because we are backpropagating from `bndiff` into `hpbn`, and but we're not done. `bn_mean` depends on `hpbn`, and there will be a second portion of that derivative coming from the second branch. We're not done yet, and we expect it to be incorrect. There you go. Let's now backpropagate from `bn_mean` into `hpbn`. Here, again, we have to be careful because there's a broadcasting along, or there's a sum along, the zeroth dimension, so this will turn into broadcasting in the backward pass. I'm going to go a little faster on this line because it is very similar to the line that we had before. `hpbn` will be scaled by 1 over `n`, and then this gradient here on `dbn_mean` is going to be scaled by 1 over `n`, and then it's going to flow across all the columns and deposit itself into the `hpbn`. What we want is this thing scaled by 1 over `n`. I'll put the constant up front here. We scale down the gradient, and now we need to replicate it across all the rows. I like to do that by `torch.ones_like` of `hpbn`, and I will let the broadcasting do the work of replication like that. This is `dhpbn`, and hopefully, we can `+=` that. This here is broadcasting, and then this is the scaling. This should be correct. Okay, so that completes the backpropagation of the batchnorm layer. We are now here. Let's backpropagate through the linear layer. Now, because everything is getting a little vertically crazy, I copy-pasted the line here, and let's just backpropagate through this one line. First, of course, we inspect the shapes. We see that `dhpbn` is 32 by 64. `mcat` is 32 by 30. `w1` is 30 by 64, and `b1` is just 64. As I mentioned, backpropagating through linear layers is fairly easy just by matching the shapes. Let's do that. We have that `dmcat` should be some matrix multiplication of `dhpbn` with `w1`, with one transpose thrown in there. To make `mcat` be 32 by 30, I need to take `dhpbn`, 32 by 64, and multiply it by `w1.transpose` to get the one I need to end up with 30 by 64. To get that, I need to take `mcat.transpose` and multiply that by `dhpbn`. Finally, to get `db1`, this is an addition, and we saw that basically, I need to sum the elements in `dhpbn` along some dimension. To make the dimensions work out, I need to sum along the zeroth axis here to eliminate this dimension. We do not keep dims, so that we want to get a single one-dimensional vector of 64. These are the claimed derivatives. Let me put that here, and let me uncomment these three lines and cross our fingers. Everything is great. Okay, so we now continue. Almost there. We have the derivative of `mcat`, and we want to backpropagate into `m`. I again copied this line over here. This is the forward pass, and these are the shapes. Remember that the shape here was 32 by 30, and the original shape of `m` was 32 by 3 by 10. This layer, in the forward pass, did the concatenation of these three 10-dimensional character vectors. Now, we just want to undo that. This is actually a relatively straightforward operation because the backward pass of the view is just a representation of the array; it's just a logical form of how you interpret the array. Let's just re-interpret it to be what it was before. In other words, the derivative of `m` is not 32 by 30, it is basically `dmcat`, but if you view it as the original shape, which is just `m.shape`, you can pass tuples into view, and so this should just be okay. We just re-represent that view, and then we uncomment this line here. Hopefully, yeah, the derivative of `m` is correct. In this case, we just have to re-represent the shape of those derivatives into the original view. Now we are at the final line, and the only thing that's left to backpropagate through is this indexing operation here, `m_scat_xb`. As I did before, I copy-pasted this line here, and let's look at the shapes of everything that's involved and remind ourselves how this worked. `m.shape` was 32 by 3 by 10. There are 32 examples, and then we have three characters, each one of them has a 10-dimensional embedding. This was achieved by taking the lookup table `C`, which has 27 possible characters, each of them 10-dimensional, and we looked up the rows that were specified inside this tensor `xb`. `xb` is 32 by 3, and it's basically giving us, for each example, the identity or the index of which character is part of that example. I'm showing the first five rows of three of this tensor `xb`. We can see that, for example, here it was the first example in this batch, and that the first character, the first character, and the fourth character come into the neural net. Then, we want to predict the next character in a sequence after the characters one, one, four. What's happening here is that there are integers inside `xb`, and each one of these integers is specifying which row of `C` we want to pluck out, right? Then, we arrange those rows that we've plucked out into a 32 by 3 by 10 tensor. We just package them into the tensor. Now, we have `dm`, so for every one of these plucked-out rows, we have their gradients now, but they're arranged inside this 32 by 3 by 10 tensor. All we have to do now is just route this gradient backward through this assignment. We need to find which row of `C` every one of these 10-dimensional embeddings comes from, and then we need to deposit them into `dc`. We just need to undo the indexing. Of course, if any of these rows of `C` was used multiple times, which almost certainly is the case, like row one was used multiple times, then we have to remember that the gradients that arrive there have to add. For each occurrence, we have to have an addition. Let's now write this out. I don't actually know if there's a much better way to do this than a for loop in Python. Maybe someone can come up with a vectorized, efficient operation, but for now, let's just use for loops. Let me create a `torch.zeros_like(C)` to initialize a 27 by 10 tensor of all zeros. Then, for `k` in range `xb.shape[0]`, maybe someone has a better way to do this, but for `j` in range `xb.shape[1]`, this is going to iterate over all the elements of `xb`, all of these integers. Let's get the index at this position, so the index is basically `xb[k, j]`. An example of that is 11 or 14, and so on. Now, in the forward pass, we took the row of `C` at `index` and we deposited it into `m` at position `k` of `j`. That's what happened. Now, we need to go backward, and we just need to route `dm` at the position `k` of `j`. We now have these derivatives for each position, and it's 10-dimensional. We just need to go into the correct row of `C`. `dc`, at index, is this, plus equals, because there could be multiple occurrences. The same row could have been used many, many times, and all of those derivatives will go backward through the indexing, and they will add. This is my candidate solution. Let's copy it here. Let's uncomment this, and cross our fingers. Hey, so that's it. We've backpropagated through this entire beast. There we go. Totally makes sense. Now, we come to exercise two. It basically turns out that in this first exercise, we were doing way too much work. We were backpropagating way too much, and it was all good practice, but it's not what you would do in practice. The reason for that is, for example, here, I separated out this loss calculation over multiple lines, and I broke it up all to its smallest atomic pieces, and we backpropagated through all of those individually. It turns out that if you just look at the mathematical expression for the loss, then you can do the differentiation on pen and paper, and a lot of terms cancel and simplify. The mathematical expression you end up with can be significantly shorter and easier to implement than backpropagating through all the little pieces of everything you've done. Previously, we had this complicated forward path going from logits to the loss, but in PyTorch, everything can just be glued together into a single call of that cross-entropy. You just pass in the logits and the labels, and you get the exact same loss, as I verify here. Our previous loss and the fast loss, coming from a chunk of operations as a single mathematical expression, are the same. But it's much faster in a forward pass. It's also much faster in a backward pass. The reason for that is if you just look at the mathematical form of this and differentiate again, you will end up with a very small and short expression. That's what we want to do here. We want, in a single operation or in a single go, or very quickly, go directly to `dlogits`. We need to implement the `dlogits` as a function of logits and `yb`s, but it will be significantly shorter than whatever we did here. To get to `dlogits`, we had to go all the way here. All of this work can be skipped with a much simpler mathematical expression that you can implement here. You can give it a shot yourself. Look at what exactly is the mathematical expression of loss and differentiate with respect to the logits. Let me show you a hint. You can try it fully yourself, but if not, I can give you some hint on how to get started mathematically. Basically, what's happening here is we have the logits, then there's a softmax that takes the logits and gives you probabilities. Then we are using the identity of the correct next character to pluck out a row of probabilities, take the negative log of it to get our negative log probability, and then we average up all the log probabilities or negative log probabilities to get our loss. Basically, what we have is for a single individual example, the loss is equal to the negative log probability, where `p` here is thought of as a vector of all the probabilities. At the `y` position, where `y` is the label, we have that `p` is the softmax function. The ith component of `p` of this probability vector is just the softmax function: raising all the logits to the power of e and normalizing so everything sums to one. If you write out `p` of `y` here, you can just write out the softmax, and what we're interested in is the derivative of the loss with respect to the `i` logit. It's `d / dli` of this expression here, where we have `l` indexed with the specific label `y`. On the bottom, we have a sum over `j` of `e` to the power of `lj`, the negative log of all that. Potentially, give it a shot on pen and paper and see if you can derive the expression for the loss divided by `dli`, and then we're going to implement it here. Okay, I'm going to give away the result here. This is some of the math I did to derive the gradients analytically. We see that I'm just applying the rules of calculus from your first or second year of bachelor's degree, if you took it. The expression actually simplifies quite a bit. You have to separate out the analysis in the case where the ith index that you're interested in inside logits is either equal to the label or not equal to the label. The expression simplifies and cancels in a slightly different way. What we end up with is something very, very simple. We either end up with `p[i]`, where `p` is again this vector of probabilities after a softmax, or `p[i] - 1`, where we simply subtract a one. In any case, we just need to calculate the softmax `p`, and then, in the correct dimension, we need to subtract one. That's the gradient that it takes analytically. Let's implement this. We have to keep in mind that this is only done for a single example, but here, we are working with batches of examples, so we have to be careful of that. The loss for a batch is the average loss over all the examples. The loss for all the individual examples summed up, and then divided by `n`, and we have to backpropagate through that, as well. `dlogits` is going to be of that softmax. PyTorch has a softmax function that you can call. We want to apply the softmax on the logits, and we want to go in the dimension that is one. We want to do the softmax along the rows of these logits. At the correct positions, we need to subtract a one. `dlogits`, iterating over all the rows and indexing into the columns provided by the correct labels inside `yb`, we need to subtract one. Finally, it's the average loss that is the loss. In the average, there's a one over `n` of all the losses added up. We also need to propagate through that division, so the gradient has to be scaled down by `n` as well because of the mean. Otherwise, this should be the result. Now, if we verify this, we see that we don't get an exact match, but the maximum difference from the logits from PyTorch and the `dlogits` here is on the order of `5e-9`. It's a tiny, tiny number. Because of floating-point weirdness, we don't get the exact bitwise result, but we basically get the correct answer approximately. I'd like to pause here briefly before we move on to the next exercise because I'd like us to get an intuitive sense of what the logits are. It has a beautiful and very simple explanation. Here, I'm taking the logits, and I'm visualizing it. We see that we have a batch of 32 examples of 27 characters. What are the logits intuitively? The logits are the probabilities in the forward pass, but here, these black squares are the positions of the correct indices where we subtracted a one. What is this doing? These are the derivatives on the logits. Let's look at just the first row. I'm plotting the probabilities of these logits, and then I'm taking just the first row, and this is the probability row. Then the logits of the first row are multiplied by `n`, just for us, so that we don't have the scaling by `n`. Everything is more interpretable. We see that it's exactly equal to the probability. The position of the correct index has a minus equals one, so minus one on that position. Notice that if you take `dlogits[0]` and sum it, it sums to zero. You should think of these gradients here at each cell as a force. We are going to be pulling down on the probabilities of the incorrect characters, and we're going to be pulling up on the probability at the correct index. That's basically what's happening in each row. The amount of push and pull is exactly equalized because the sum is zero. The amount to which we pull down in the probabilities, and the amount that we push up on the probability of the correct character, is equal. The repulsion and the attraction are equal. Think of the neural net now as a massive pulley system. We're up here on top of the logits, and we're pulling down the probabilities of incorrect characters and pulling up the probability of the correct characters. In this complicated pulley system, because everything is mathematically determined, think of it as tension translating to this complicated pulling mechanism. Eventually, we get a tug on the weights and the biases. In each update, we just tug in the direction that we like for each of these elements. The parameters are slowly giving in to the tug. That's what training in a neural net looks like on a high level. I think the forces of push and pull in these gradients are intuitive. We're pushing and pulling on the correct answer and the incorrect answers. The amount of force that we're applying is proportional to the probabilities that came out in the forward pass. For example, if our probabilities came out exactly correct, they would have had zero everywhere except for one at the correct position. Then, the logits would be all a row of zeros for that example. There would be no push and pull. The amount to which your prediction is incorrect is exactly the amount by which you're going to get a pull or a push in that dimension. If you have a very confidently mispredicted element here, then that element is going to be pulled down very heavily, and the correct answer is going to be pulled up to the same amount. The other characters are not going to be influenced too much. The amounts to which you mispredict are proportional to the strength of the pole, and that's happening independently in all the dimensions of this tensor. It's intuitive and easy to think through. That's the magic of the cross-entropy loss and what it's doing dynamically in the backward pass of the neural net. Now we get to exercise number three, which is a very fun exercise, depending on your definition of fun. We're going to do for batch normalization exactly what we did for cross-entropy loss in exercise two. We're going to consider it as a glued single mathematical expression and backpropagate through it in a very efficient manner. We're going to derive a much simpler formula for the backward path of batch normalization. We're going to do that using pen and paper. Previously, we broke up batch normalization into all the little intermediate pieces and all the atomic operations inside it. We backpropagated through it one by one. Now, we just have a single forward pass of batch norm, and it's all glued together. We see that we get the exact same result as before. For the backward pass, we'd like to implement a single formula for backpropagating through this entire operation, which is the batch normalization. In the forward pass, previously, we took `hpbn`, the hidden states of the pre-batch normalization, and created `h_preact`, which is the hidden states just before the activation. In the batch normalization paper, `hpbn` is X, and `h_preact` is Y. In the backward pass, we have `dh_preact`, and we'd like to produce `d_hpbn` in an efficient manner. That's the name of the game: calculate `dh_pbn` given `dh_preact`. For the purposes of this exercise, we're going to ignore gamma and beta and their derivatives because they take on a very simple form, similar to what we did above. Let's calculate this given that. To help you a little bit, like I did before, I started off the implementation on pen and paper. I took two sheets of paper to derive the mathematical formulas for the backward pass. To set up the problem, write out `mu`, `sigma_squared` variance, `x_i_hat`, and `y_i`, exactly as in the paper, except for the Bessel's correction.

In a backward pass, we have the derivative of the loss with respect to all elements of Y, which is a vector with multiple numbers. We also have gamma and beta, and this is like the compute graph.  We have x-hat, mu, sigma squared, and x. We are given DL by DYI and we want DL by dxi for all the Is in these vectors.  It’s important to note that while x, x-hat, and Y contain multiple nodes, mu and sigma squared are individual scalars, and you have to consider this or you'll get your math wrong. I suggest going in the following order for backpropagation: x-hat, then sigma squared, then mu, and then x.  This is similar to a topological sort; in micrograd we would go from right to left, and we’re doing the same with symbols on paper. To get DL by dxi-hat, we take DL by DYI and multiply it by gamma, because any individual Yi is gamma times x i-hat plus beta. This gives us the derivatives for all the x-hats. Now, let's derive DL by D sigma squared. We must remember that there are many x-hats, and sigma squared is a single number. Sigma squared has a large fan out, with many arrows going to the x-hats. There's a back propagating signal from each x-hat into sigma squared, so we sum over all Is from 1 to m of DL by dxi-hat times dxi-hat by D sigma squared, which is the local gradient.  Mathematically, this simplifies to a specific expression for DL by D sigma squared. We will use this when backpropagating into mu, and then x.  Now let's backpropagate into mu.  Mu influences all the x-hats, so if our mini-batch size is 32, there are 32 numbers and 32 arrows going back to mu.  Mu going to sigma squared is a single arrow since sigma squared is a scalar.  So, in total, there are 33 arrows emanating from mu, and they all have gradients going into mu that must be summed up.  Thus, when looking at the expression for DL by D mu, I am summing over all the gradients of DL by dxi-hat times dxi-hat by D mu, plus the one arrow, which is DL by D sigma squared times D sigma squared by D mu. After simplifying, the first term is straightforward, and for the second term, if we assume mu is the average of the xI's, as it is in this case, then the gradient D sigma squared by D mu vanishes and becomes zero, canceling out the entire second term, leaving us with a straightforward expression for DL by D mu. Now for deriving DL by dxi, first count how many numbers are inside X; there are 32 numbers, each xI. From each xI, there's an arrow to mu, to sigma squared, and to x-hat. Each x i-hat is a function of xI, so there are 32 parallel arrows going between x and x-hat. Each xI has three arrows emanating from it: to mu, to sigma squared, and to the associated x-hat.  We apply the chain rule and add the contributions from these three paths. This means chaining through mu, sigma squared, and x-hat. We have DL by dxi-hat, DL by D mu, and DL by D sigma squared, but we need dxi-hat by dxi, D mu by dxi, and D sigma squared by dxi. You can derive these by differentiating the expressions. After doing this, we plug everything together, multiplying the terms and adding them up. When plugging in the DL by D sigma squared term, we use a different iterator, J, to avoid mixing up the local iterator within the expression with the iterator over the batch elements, I. After simplifying, some terms can be factored, and we end up with a fairly simple mathematical expression that uses the derivatives from earlier steps. This expression gives us DL by dxi for all the Is. This is the end of the backward pass analytically. I implemented the expression into a single line of code, and you can see the max difference is tiny, so this is the correct implementation.  Getting this formula was not trivial and involves considering that this formula is for a single neuron and a batch of 32 examples, whereas we have 64 neurons and this expression has to evaluate the backward pass for all of them in parallel. Additionally, we need to make sure the sums broadcast correctly onto everything else. After verifying that the shapes are correct, and that this expression works, you can also check against PyTorch and get the same answer, giving confidence in its correctness.  Exercise four involves putting everything together.  We re-initialize the neural net and, instead of calling loss.backward, we use our manual backpropagation. By copying and pasting the code we derived, we can drive our own gradients and optimize the neural net. I achieved a good loss, and that is expected because we replaced loss.backward with our own derived code, and the gradients are identical.  This gives full visibility on what's under the hood. The full backward pass includes backpropagating through cross entropy, the second layer, the tanh nonlinearity, batch normalization, the first layer, and the embedding. This entire process is only about 20 lines of code and gives us gradients and allows us to remove loss.backward. After filling in the manual backpropagation, this code should run for 100 iterations and then break, so you can check that the manual gradients match the PyTorch gradients, which will be very close, with differences around 1e-9, which I don't know where it’s coming from exactly. Once confident, you can remove the break and also remove loss.backward. Then you don’t need pytorch’s grad, and you will update with the manually derived gradients. You can also add “with no_grad” to the training loop to make it more efficient, then you can run the optimization, and you'll see that the loss.backward isn't running. After finishing, I calibrated the batchnorm parameters, ran the loss and sampled from the model. The model worked and produced similar output as before. We estimated the gradients manually without using PyTorch's autograd. Hopefully, you see that the backward pass of this neural net is not too complicated, where each layer is a few lines of code, with batch normalization being an exception. This provides a good sense of how these backward passes work and how to derive them yourself. In the next lecture, we'll go into recurrent neural networks, LSTMs, and other variants of RNNs, to achieve better log likelihoods.

