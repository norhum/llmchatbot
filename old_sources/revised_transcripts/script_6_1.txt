We can flatten an input tensor by viewing it in a desired way. We want to avoid using negative one to stretch things out. Instead, we will create a three-dimensional array.  We can fuse consecutive vectors. For example, if we fuse two vectors, we can specify a dimension of 20 and use a negative one. Python will then determine how many groups are needed for the batch dimension.  Let's go to the `flatten` implementation. We will create a constructor that takes the number of consecutive elements to concatenate in the output's last dimension, storing this value as `self.n`. We must be careful because PyTorch's `torch.flatten` has different keyword arguments and functionality.  Our `flatten` will be different, so we'll call it `flatten_consecutive`. It flattens `n` consecutive elements into the last dimension. The shape of the input, `X`, is B by T by C. Let's unpack those dimensions.  In our example, B was 4, T was 8, and C was 10. Instead of `x.view(B, -1)`, we want `x.view(B, T // n, c*n)`. We'll use integer division for `T // n`. If `x.shape[1]` is one, it's a spurious dimension. We don't want a three-dimensional tensor with a one in the second dimension. Instead, we'll return a two-dimensional tensor, as before. We will use `x.squeeze(1)` to remove the dimension of one. If the condition is met, we return `x.view(B, c*n)`.  We will store the output in `self.out` and return that. Note that we use `self.n` instead of `n`. Let's test this implementation.  Initially, we will set consecutive elements to 8 to recover the previous behavior, so `flatten_consecutive(8)` should match the previous behavior.  We can run the model and inspect the shapes. We iterate over the layers, printing the name and shape. The shapes should match our expectations. Now, let's restructure things hierarchically, using `flatten_consecutive(2)`. The first linear layer will have inputs of `embed*2`, or 20. The second linear layer will take `hidden*2` inputs. The last layer also takes `hidden*2` as input. This is a naive implementation that creates a larger model.  We should be able to forward the model and inspect the intermediate shapes. We see the expected shape changes after each layer. The batch norm seems to work with three-dimensional inputs.  We see that the last flattened consecutive operation squeezes out the dimension of one, resulting in four by four hundred.  The logits end up in the same shape as before. Now we have a three-layer neural net. This network corresponds to the structure discussed except for the fact that the total receptive field size is 8 in this implementation, whereas it's 16 in the example. We need to determine good channel numbers to use here. We will use 68 hidden units to match the previous parameter count, 22,000.  The question is whether we can use parameters more efficiently in the new architecture. After getting rid of the debugging cells and rerunning the optimization, we see similar performance. The validation loss is 2.029 compared to 2.027 before. Changing from flat to hierarchical, while controlling the number of parameters, does not yet improve performance. There are a few things to point out: first, we haven't thoroughly tested the architecture, this is just my first guess and there are lots of hyper parameter searches to perform.  Second, there might be a bug in the `BatchNorm1D` layer. Although it runs without errors, is it working as intended?   When we inspect the layer, the batch norm receives a 32 by 4 by 68 input. Our batch norm implementation assumes a two-dimensional input `N by D`. We are reducing the mean and variance over the zeroth dimension. Now `X` is three-dimensional. This is working due to broadcasting, but not as we want. The mean and variance will have shape 1 by 4 by 68.  These means are calculated over the 32 numbers in the first dimension. The running mean will have shape 1 by 4 by 68.  We expect 68 means and variances, but we have an array of 4 by 68. This means that batch norm is working in parallel across the 4 times 68 dimensions instead of 68 channels. We are maintaining separate statistics for the four positions independently. We want to treat this four as a batch dimension.  Instead of averaging over 32 numbers, we want to average over 32 times 4 numbers for every one of the 68 channels.  We see that in PyTorch, specifying dimension as a tuple reduces across all listed dimensions. We can reduce over multiple dimensions using a tuple, for instance reducing over dimensions 0 and 1. The output will have the same dimensions, but it is now taking the mean over both dimensions. If we look at the shape of the mean, it will be 1 by 1 by 68. The running mean and variance will be one by one by 68. We are maintaining means and variances for 68 channels instead of averaging across 32 times 4 dimensions, which is correct.  We will change `BatchNorm1D` to accept both two-dimensional and three-dimensional inputs. We want to reduce over either dimension 0 or the tuple 0,1, based on the dimensionality of `X`. If `x.ndim` is 2, the dimension to reduce is 0. If `x.ndim` is 3, the dimensions are 0 and 1. Otherwise, we raise an error.  This is different from the PyTorch batch norm, which expects inputs to be `N by C` or `N by C by L`, and assumes C is the second dimension. Our batch norm assumes that C is always the last dimension. We think our approach is more reasonable for our needs.  After redefining layers, we re-initialize the neural net and do a forward pass. The shapes are the same, but the running mean now has a shape of one by one by 68. We are maintaining 68 means and variances, treating dimensions zero and one as batch dimensions. After retraining the neural net with the bug fix, we get a slight performance increase from 2.029 to 2.022. The batch norm bug was causing a small performance issue. We are getting a tiny improvement due to having more samples for each estimate. With the more general architecture in place, we can push for better performance by increasing network size. For example, we increased embeddings to 24 and hidden units. We now have 76,000 parameters and the training is slower, but we obtain a validation performance of 1.993. We have exceeded the 2.0 threshold, but experiments are starting to take longer. Also, we are not sure about optimal hyperparameter settings. We need an experimental harness to tune the architecture.  Our performance improved from 2.1 down to 1.9, but it's just blind guess and check. We don't have an experimental harness, and we're just looking at the training loss. Typically, you want to look at both training and validation loss. We implemented the architecture from the wavenet paper but did not include gated linear layers or residual connections. The use of convolutions is for efficiency only, they do not alter our model implementation. For example, if we have a name like DeAndre, we have seven letters and eight independent examples. We can forward any one of these examples independently. We add an extra batch dimension to call our model on an individual name.  We can forward all eight examples by looping eight times.  With our current model, this is eight independent calls to the model. Convolutions allow us to slide the model efficiently over the input sequence. The loop is done in Cuda kernels, which makes it more efficient. Convolutions apply a linear filter over space of some input sequence, this for loop gets hidden into the convolution operation.  We are sliding filters over input data. This is basically the same calculation, but convolutions slide over and apply the calculation efficiently. These connections are the same, but we can reuse the values. This diagram highlights one individual calculation. We've implemented this single structure, calculating single output, convolution slide and evaluate over all the positions at once. Convolutions take linear layers as filters, slide them across the input sequence, and do the calculations for each layer efficiently. We will cover convolutions in a future video.  We have built all of these layers and containers. We are re-implementing `torch.nn` on top of `torch.tensor`. The real `torch.nn` is superior, but now we understand how these modules work and how they're nested. We have roughly unlocked the usage of `torch.nn`, and we should start using it directly. I hope you get a sense of the development process of building deep neural networks.  First, we spend time in PyTorch documentation, reading about layers, inputs, and functionality. Unfortunately, the PyTorch documentation is not very good, it can be wrong or incomplete. Second, making the shapes work is tedious. We have to perform gymnastics with multidimensional arrays. It is tricky to remember which shape formats layers take, it becomes quite messy.  I prototype in jupyter notebooks, make sure the shapes work, and copy it over to our actual code repo. I usually have jupyter notebook and VS code open. I develop in jupyter, then I paste it into VS code and kick off experiments from there. We have unlocked a lot of potential topics including converting the model to dilated causal convolutions. Another topic would be explaining residual connections and skip connections. We also need to set up an evaluation harness, since this guessing and checking approach isn't representative of typical workflows. Lastly, we should cover recurrent neural networks, LSTMs, GRUs, and Transformers. There are lots of places to go and we will cover those in the future.  If you're interested, try to beat the 1.993 validation loss, since we haven't explored the parameter space. Perhaps the embedding dimensions are wrong. Maybe the original single-layer network can perform better if it is made large enough. It would be kind of embarrassing if it was better. Maybe read the wavenet paper, implement the layers, tune optimization, or tune initialization. I'm curious if you can find a way to beat our performance. That is all for now.
