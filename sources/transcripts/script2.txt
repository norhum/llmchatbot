Today, we are continuing our implementation of makemore. In the last lecture, we implemented the bigram language model using both counts and a simple neural network with a single linear layer. We saw that we predicted the distribution for the next character in a sequence by looking at only the single previous character. We did this by taking counts and normalizing them into probabilities, so that each row sums to one. This works if you only have one character of previous context. However, the predictions from this model are not very good because it only considers one character of context, and thus the model did not produce name-like sounding results. If we take more context into account when predicting the next character, the size of the table grows exponentially with the length of the context. For example, if we take a single character, there are 27 possibilities.  If we take two characters, there are 729 possibilities. If we take three characters, there are 20,000 possibilities. There are way too many rows for the matrix, too few counts for each possibility, and the model does not work well. Therefore, today we will implement a multi-layer perceptron model to predict the next character in a sequence. This modeling approach follows the Bengio et al. 2003 paper. This is not the very first paper that proposed using multi-layer perceptrons for this task, but it is very influential. It is often cited to represent this idea and is a very nice write-up. I invite you to read it.  In the introduction, they describe the same problem that I just described. To address it, they propose the following model. Keep in mind that we are building a character-level language model, while in this paper, they have a vocabulary of 17,000 words and build a word-level language model. We will still stick with characters, but we will take the same modeling approach. They associate a 30-dimensional feature vector to each of the 17,000 words, embedding each word into a 30-dimensional space. In the beginning, these words are initialized completely randomly. We will tune these word embeddings using backpropagation. During training, these vectors will move around in the space. Words with similar meanings or synonyms might end up in a similar part of the space, and words with very different meanings would go elsewhere in the space. Their modeling approach is otherwise identical to ours. They use a multi-layer neural network to predict the next word given the previous words, and they maximize the log likelihood of the training data to train the network. Here, they provide an example to illustrate why this approach works. Suppose you are trying to predict a word in the phrase "a dog was running in a blank." If this exact phrase has never occurred in the training data, the model is out of distribution. However, this approach allows you to get around that. You may not have seen the exact phrase, but maybe you have seen similar phrases. Perhaps your network has learned that 'a' and 'the' are often interchangeable and so their embeddings are nearby. Knowledge can transfer through this embedding, and generalization can occur. Similarly, the network could know that cats and dogs are animals and co-occur in similar contexts, even if you haven't seen them together in this exact phrase. Let’s look at the diagram of the neural network. In this example, we are taking three previous words and trying to predict the fourth word. Every word is represented by its index in the vocabulary. Since there are 17,000 words, these are integers between 0 and 16,999. There is a lookup table called C, which is a matrix of 17,000 by 30. Every index is plucking out a row of this embedding matrix, converting each word to its corresponding 30-dimensional vector.  The input layer has 30 neurons for each of the three words, totaling 90 neurons. The matrix C is shared across all words. Next is the hidden layer, the size of which is a hyperparameter and can be as large or as small as you would like. For example, the size could be 100. We will evaluate multiple choices for the size of this hidden layer. All 100 neurons are fully connected to the 90 inputs. This is followed by a tanh non-linearity and then the output layer, which has 17,000 neurons because there are 17,000 possible words. All of these neurons are fully connected to the hidden layer. This layer has the most parameters and computation.  The 17,000 logits are passed to a softmax layer, where each logit is exponentiated, and then everything is normalized to sum to one, giving a probability distribution for the next word. During training, we have the label, which is the index of the next word.  This index is used to pluck out the probability of that word. We maximize the probability of that word with respect to the parameters of this neural network, including the weights and biases of the output layer, the hidden layer, and the embedding lookup table C. All of this is optimized using back propagation. I have started a new notebook.  We are importing PyTorch and matplotlib.  I am reading all the names into a list of words and showing the first eight. There are 32,000 names in total. I am building out the vocabulary of characters and mappings from characters as strings to integers and vice versa.  The first thing we need to do is to compile the data set for the neural network. I have rewritten the code for that. I will explain it briefly after I run it. First, we define the `block_size`, which is the context length, or how many characters to use to predict the next one. In this example, we are taking three characters to predict the fourth one. Then we build out the `x` and `y` arrays.  The `x` array is the input to the neural net, and the `y` array contains the labels.  I am looping over the first five words for efficiency, but later we will use the entire training set. Here, I am printing the word ‘emma’ and showing the five examples generated from it. Given a context of “..”, the first character is 'e', and the label is 'm'. When the context is “..e”, the label is 'm', and so forth. I start with a padded context of zero tokens.  I iterate over all the characters, build the `y` array with the current character and the `x` array with the current context, print everything, and crop the context to include the new character. This creates a rolling window of context.  We can change the `block_size`, for example to four, five, or ten.  Currently, it is three, which matches the paper. Finally, the data set contains 32 examples from the five words. Each input to the neural net is three integers, and the label is also an integer. Now, let's write a neural network that takes these inputs and predicts the outputs. First, let’s build the embedding lookup table `c`.  We have 27 characters and we will embed them into a lower dimensional space.  In the paper, they embed 17,000 words into a 30-dimensional space.  In our case, we have only 27 characters, so we will start with a two-dimensional space. The lookup table will have 27 rows and two columns of random numbers. Each of the 27 characters will have a two-dimensional embedding. Before we embed all the integers in `x`, let's embed a single integer, say 5, so we get a sense of how this works.  One way to do this is to index into row 5 of `c`. Another way is to use one-hot encoding.  We must make some type errors intentionally so you get a sense of how to fix them. So, we will make a one-hot vector for integer 5 with 27 classes. The input must be a torch tensor.  The shape of this tensor is 27, and the fifth dimension is one.  If we multiply this one-hot vector by `c`, we get an error because the types are mismatched.  We have to cast the one-hot vector to float.  The result is the same because all the zeros in the one-hot vector mask out everything in `c` except for the fifth row, which is plucked out. This tells you that we can think of the embedding as indexing into a lookup table or as the first layer of a neural net where we encode integers into one-hot vectors and feed those into a linear layer. We will use indexing because it is much faster. Now, embedding a single integer like 5 is easy, but how do we embed all of the 32 by 3 integers stored in `x`? PyTorch indexing is very flexible.  We can index using lists. We can index with a tensor of integers.  We can repeat rows.  We can also index with multi-dimensional tensors.  We can simply use `c` at `x`. The shape of this is 32 by 3 by 2. We are retrieving the embedding for every integer in `x`. PyTorch indexing is awesome. To embed all the integers in `x`, we simply use `c` of `x`. That's our embedding. Now let’s construct the hidden layer. We have weights, `w1`, initialized randomly. The number of inputs is 3 times 2, or 6. The number of neurons is a variable, let’s say 100. The biases, `b1`, are also initialized randomly with 100 elements. We cannot directly multiply the embeddings with `w1` and add the bias because the embeddings are stacked up in the dimensions of the input tensor. We need to concatenate these inputs, so we can perform the multiplication. How do we transform the 32 by 3 by 2 tensor into a 32 by 6 tensor? There are usually many ways to implement the same thing in PyTorch. The documentation is extensive, containing many functions. We can use the `torch.cat` function, which concatenates a sequence of tensors in a given dimension. We can concatenate the three embeddings for each input. We want to grab all the examples, the zeroth index, and then all of this. This plucks out the 32 by 2 embeddings of just the first word. We treat this as a sequence and call `torch.cat`. We have to tell it along which dimension to concatenate, which is dimension 1. The result is a tensor of shape 32 by 6. We took 32 by 3 by 2 and flattened it to 32 by 6. This is not a great approach because it is hard-coded to a `block_size` of 3. If we changed it to 5, then we would need to rewrite our code. PyTorch comes to the rescue with `unbind`, which removes a tensor dimension and returns a tuple of all the slices. When we call `torch.unbind` on `m` along dimension 1, it gives us a list of tensors. We call `torch.cat` on this list along the first dimension. The result is the same, but this version works even if we change `block_size`. There is a more efficient way using `view` that hints at how tensors work internally. Let's create an array of elements from 0 to 17, with a shape of 18. We can represent this as tensors of different sizes by calling `view`. As long as the total number of elements remains the same, we can reshape a tensor in a variety of ways. This is an extremely efficient operation in PyTorch because the underlying storage, which is a one-dimensional vector, is not being changed. The only thing that is changed are attributes that dictate how the one-dimensional vector is interpreted. So, when we call `view`, no memory is being changed or created. There is a blog post called PyTorch Internals that explains this in more detail.  We can view our embeddings of the shape 32 by 3 by 2 instead as 32 by 6. PyTorch concatenates these two dimensions. This gives us the same result. So, we can view the embedding as a 32 by 6 tensor, which will work in our multiplication.  The shape of `h` is 100-dimensional activations for each of the 32 examples.  We need to apply the `tanh` non-linearity to get the final `h`. The final h are now numbers between -1 and 1 with a shape of 32 by 100. This represents the hidden layer of activations. We have to be careful with the bias addition. The shape of `h` is 32 by 100, and the shape of `b1` is 100.  The broadcasting aligns on the right, and so it adds the same bias vector to all rows of the matrix. Now, let’s create the final layer, with `w2` and `b2`. The input is now 100. The output number of neurons will be 27 because we have 27 characters. The biases will also be 27.  The logits, the output of the neural net, are computed by multiplying `h` by `w2` and adding `b2`. The shape of the logits is 32 by 27. We want to exponentiate the logits to get the fake counts and then normalize them into a probability. We divide the counts by the sum along the first dimension, keeping the dimension. The shape of `prob` is now 32 by 27, and every row sums to one. Now we have the actual letter that comes next, in the array `y`, which was created during dataset creation.  We want to index into the rows of `prob` and pluck out the probability assigned to the correct character given by `y`. We create a range of 32, which is like an iterator over the rows of `prob`. For each row, we grab the column as given by `y`.  The probabilities for the correct characters are obtained. For some of the characters the probability is 0.2 but for others it is as low as 0.01. This means the network thinks some characters are very unlikely. However, we haven't trained the neural network yet. We take these probabilities, compute their log probability, average it, and then take the negative of it to create the negative log likelihood loss.  The loss is 17, which is the loss we would like to minimize. I have rewritten everything to make it more respectable. Here’s our dataset. Here are all the parameters. I am now using a generator to make it reproducible. I have clustered all the parameters into a single list, so it is easy to count them. In total, there are currently about 3400 parameters. This is the forward pass, and we arrive at a single number, the loss. I would like to make this even more respectable. These lines where we take the logits and calculate the loss are reinventing the wheel. This is just classification, and many people use classification, and thus, there is a function `functional.cross_entropy` in PyTorch to calculate this more efficiently. We can pass in the array of targets, `y`, and this calculates the exact same loss. In fact, we can simply put this here and erase these three lines, and we are going to get the exact same result. There are actually many good reasons to prefer `f.cross_entropy` over rolling your own implementation like this. I did this for educational reasons, but you'd never use this in practice. Number one, when you use `f.cross_entropy`, PyTorch will not actually create all these intermediate tensors because these are all new tensors in memory. All of this is fairly inefficient to run like this. Instead, PyTorch will cluster up all these operations and very often create fused kernels that very efficiently evaluate these expressions that are sort of like clustered mathematical operations. Number two, the backward pass can be made much more efficient, and not just because it's a fused kernel, but also analytically and mathematically it's often a much simpler backward pass to implement. We actually saw this with micrograd. You see here when we implemented `tanh`, the forward pass of this operation to calculate the `tanh` was actually a fairly complicated mathematical expression, but because it's a clustered mathematical expression, when we did the backward pass, we didn't individually backward through the x and the two times and the minus one in division, etc. We just said it's one minus `t` squared, and that's a much simpler mathematical expression. We were able to do this because we're able to reuse calculations, and because we are able to mathematically and analytically derive the derivative. Often that expression simplifies mathematically, so there's much less to implement. Not only can it be made more efficient because it runs in a fused kernel, but also because the expressions can take a much simpler form mathematically. So that's number one, number two. Under the hood, `f.cross_entropy` can also be significantly more numerically well behaved. Let me show you an example of how this works. Suppose we have logits of negative two, three, negative three, zero, and five, and then we are taking the exponent of it and normalizing it to sum to one. So, when logits take on these values, everything is well and good, and we get a nice probability distribution. Now consider what happens when some of these logits take on more extreme values, and that can happen during optimization of the neural network. Suppose that some of these numbers grow very negative, like say negative 100. Then actually, everything will come out fine. We still get the probabilities that are well behaved and they sum to one, and everything is great. But because of the way the `exp` works, if you have very positive logits, let's say positive 100 in here, you actually start to run into trouble, and we get `NaN` here. The reason for that is that these counts have an `exp` here, so if you pass in a very negative number to `exp`, you just get a very small number, very very near zero, and that's fine. But if you pass in a very positive number, suddenly we run out of range in our floating point number that represents these counts. Basically, we're taking `e` and we're raising it to the power of 100, and that gives us `NaN` because we run out of dynamic range on this floating point number that is count. We cannot pass very large logits through this expression. Now let me reset these numbers to something reasonable. The way PyTorch solved this is that you see how we have a well-behaved result here. It turns out that because of the normalization here, you can actually offset logits by any arbitrary constant value that you want. So, if I add one here, you actually get the exact same result, or if I add two, or if I subtract three, any offset will produce the exact same probabilities. Because negative numbers are okay, but positive numbers can actually overflow this `exp`, what PyTorch does is it internally calculates the maximum value that occurs in the logits and it subtracts it. So, in this case it would subtract five, and therefore the greatest number in logits will become zero, and all the other numbers will become some negative numbers. The result of this is always well behaved. Even if we have 100 here, previously not good, but because PyTorch will subtract 100, this will work. So, there's many good reasons to call `cross_entropy`. Number one, the forward pass can be much more efficient. The backward pass can be much more efficient, and also things can be much more numerically well behaved. Okay, so let's now set up the training of this neural net. We have the forward pass. We don't need these. We have the loss equal to `f.cross_entropy`, that's the forward pass. Then we need the backward pass. First, we want to set the gradients to be zero, so for `p` in parameters, we want to make sure that `p.grad` is none, which is the same as setting it to zero in PyTorch. Then `loss.backward()` to populate those gradients. Once we have the gradients, we can do the parameter update. So, for `p` in parameters, we want to take all the data, and we want to nudge it learning rate times `p.grad`. Then we want to repeat this a few times, and let's print the loss here as well. Now this won't suffice, and it will create an error because we also have to go for `p` in parameters, and we have to make sure that `p.requires_grad` is set to true in PyTorch. This should just work. Okay, so we started off with a loss of 17, and we're decreasing it. Let's run longer, and you see how the loss decreases a lot here. If we just run for a thousand times, we get a very very low loss, and that means that we're making very good predictions. The reason that this is so straightforward right now is because we're only overfitting 32 examples. We only have 32 examples of the first five words, and therefore it's very easy to make this neural net fit only these 32 examples because we have 3,400 parameters and only 32 examples. We're doing what's called overfitting a single batch of the data and getting a very low loss and good predictions. But that's just because we have so many parameters for so few examples, so it's easy to make this be very low. Now, we're not able to achieve exactly zero, and the reason for that is we can, for example, look at logits which are being predicted, and we can look at the max along the first dimension. In PyTorch, `max` reports both the actual values that take on the maximum number, but also the indices of pieces. You'll see that the indices are very close to the labels, but in some cases they differ. For example, in this very first example, the predicted index is 19, but the label is five. We're not able to make loss be zero, and fundamentally that's because here the very first, or the zeroth index, is the example where dot dot dot is supposed to predict `e`, but you see how dot dot dot is also supposed to predict an `o`, and dot dot is also supposed to predict an `i`, and then `s` as well. Basically, `e`, `o`, `a`, or `s` are all possible outcomes in a training set for the exact same input. So, we're not able to completely overfit, and make the loss be exactly zero. But we're getting very close. In the cases where there's a unique input for a unique output, in those cases we do what's called overfit, and we basically get the exact same and the exact correct result. Now, all we have to do is we just need to make sure that we read in the full data set and optimize the neural net. Okay, so let's swing back up where we created the data set. We see that here we only use the first five words. Let me now erase this, and let me erase the print statements, otherwise we'd be printing way too much. So, when we processed the full data set of all the words, we now had 228,000 examples instead of just 32. So, let's now scroll back down to this is much larger. Reinitialize the weights, the same number of parameters, they all require gradients. Then let's push this `print out loss.item` to be here, and let's just see how the optimization goes if we run this. Okay, so we started with a fairly high loss, and then as we're optimizing, the loss is coming down. You'll notice that it takes quite a bit of time for every single iteration. Let's actually address that because we're doing way too much work forwarding and backwarding 220,000 examples. In practice, what people usually do is they perform forward and backward pass and update on many batches of the data. What we will want to do is we want to randomly select some portion of the data set, and that's a mini batch. Then, only forward backward and update on that little mini batch, and then we iterate on those many batches. In PyTorch, we can, for example, use `torch.randint`. We can generate numbers between zero and five, and make 32 of them. I believe the size has to be a tuple in PyTorch, so we can have a tuple 32 of numbers between zero and five. Actually, we want `x.shape` of zero here. So, this creates integers that index into our data set, and there's 32 of them. If our mini batch size is 32, then we can come here and we can first do a mini batch construct. The integers that we want to optimize in this single iteration are in the `ix`. Then, we want to index into `x` with `ix` to only grab those rows. We're only getting 32 rows of `x`, and therefore embeddings will again be 32 by three by two, not two hundred thousand by three by two. Then this `ix` has to be used not just to index into `x`, but also to index into `y`. Now this should be mini batches, and this should be much much faster. Okay, so it's instant almost. This way, we can run many many examples nearly instantly, and decrease the loss much much faster. Because we're only dealing with mini batches, the quality of our gradient is lower. The direction is not as reliable. It's not the actual gradient direction, but the gradient direction is good enough even when it's estimating on only 32 examples that it is useful. It's much better to have an approximate gradient and just make more steps than it is to evaluate the exact gradient and take fewer steps. That's why in practice this works quite well. Let's now continue the optimization. Let me take out this `loss.item` from here, and place it over here at the end. Okay, so we're hovering around 2.5 or so. However, this is only the loss for that mini batch. Let's actually evaluate the loss here for all of `x` and for all of `y`, just so we have a full sense of exactly how all the model is doing right now. So right now, we're at about 2.7 on the entire training set. Let's run the optimization for a while. Okay, right 2.6, 2.57, 2.53. One issue of course is we don't know if we're stepping too slow or too fast. This point one I just guessed it. One question is how do you determine this learning rate, and how do we gain confidence that we're stepping in the right sort of speed. I'll show you one way to determine a reasonable learning rate. It works as follows. Let's reset our parameters to the initial settings. Now let's print in every step, but let's only do 10 steps or so, or maybe maybe 100 steps. We want to find a very reasonable set search range, if you will. For example, if this is very low, then we see that the loss is barely decreasing. That's like too low basically. Let's try this one. Okay, so we're decreasing the loss, but not very quickly. That's a pretty good low range. Now, let's reset it again, and now let's try to find the place at which the loss kind of explodes. Maybe at negative one. Okay, we see that we're minimizing the loss, but you see how it's kind of unstable, it goes up and down quite a bit. Negative one is probably like a fast learning rate. Let's try negative 10. Okay, so this isn't optimizing, this is not working very well. Negative 10 is way too big. Negative one was already kind of big. Therefore, negative one was somewhat reasonable. If I reset, so I'm thinking that the right learning rate is somewhere between negative 0.001 and negative one. The way we can do this here is we can use `torch.linspace`. We want to basically do something like this between zero and one, but the number of steps is one more parameter that's required. Let's do a thousand steps. This creates 1000 numbers between 0.01 and 1, but it doesn't really make sense to step between these linearly. Instead, let me create learning rate exponents, and instead of 0.001, this will be a negative three, and this will be a zero. Then the actual `lrs` that we want to search over are going to be 10 to the power of `lre`. Now what we're doing is we're stepping linearly between the exponents of these learning rates. This is 0.001, and this is 1 because 10 to the power of 0 is 1. Therefore, we are spaced exponentially in this interval. These are the candidate learning rates that we want to search over roughly. Now what we're going to do is here we are going to run the optimization for 1000 steps. Instead of using a fixed number, we are going to use learning rate indexing into here, `lrs` of `i`, and make this `i`. Let me reset this to be again starting from random, creating these learning rates between negative 0.001 and one, but exponentially spaced. Here what we're doing is we're iterating a thousand times. We're going to use the learning rate that's in the beginning very very low. In the beginning is going to be 0.001, but by the end it's going to be 1. Then we're going to step with that learning rate. Now what we want to do is we want to keep track of the learning rates that we used, and we want to look at the losses that resulted. Here, let me track stats, so `lri.append(lr)` and `loss_stats.append(loss.item())`. Again, reset everything, and then run. Basically, we started with a very low learning rate, and we went all the way up to a learning rate of negative one. Now what we can do is we can `plt.plot`, and we can plot the two. We can plot the learning rates on the x-axis, and the losses we saw on the y-axis. Often you're going to find that your plot looks something like this, where in the beginning you had very low learning rates, so basically barely anything happened. Then we got to a nice spot here, and then as we increase the learning rate enough, we basically started to be kind of unstable here. A good learning rate turns out to be somewhere around here. Because we have `lri` here, we actually may want to do not `lr`, not the learning rate, but the exponent, so that would be the `lre` at `i` is maybe what we want to log. Let me reset this and redo that calculation, but now on the x axis we have the exponent of the learning rate. So, we can see the exponent of the learning rate that is good to use would be sort of like roughly in the valley here, because here the learning rates are just way too low, and then here where we expect relatively good learning rates somewhere here, and then here things are starting to explode. Somewhere around negative one, the exponent of the learning rate is a pretty good setting, and 10 to the negative one is 0.1. So 0.1 was actually a fairly good learning rate around here, and that's what we had in the initial setting. That's roughly how you would determine it. Here now, we can take out the tracking of these, and we can just simply set `lr` to be 10 to the negative one, or basically otherwise 0.1 as it was before. Now we have some confidence that this is actually a fairly good learning rate. What we can do now is we can crank up the iterations. We can reset our optimization, and we can run for a pretty long time using this learning rate. Oops, and we don't want to print, that's way too much printing. Let me again reset and run 10,000 stops. Okay, so we're 0.2, 2.48 roughly. Let's run another 10,000 steps. 2.46. Now let's do one learning rate decay. What this means is we're going to take our learning rate, and we're going to 10x lower it. We're at the late stages of training potentially, and we may want to go a bit slower. Let's do one more actually at 0.1 just to see if we're making a dent here. Okay, we're still making a dent. By the way, the bi-gram loss that we achieved last video was 2.45, so we've already surpassed the bi-gram model. Once I get a sense that this is actually kind of starting to plateau off, people like to do, as I mentioned, this learning rate decay. Let's try to decay the loss, the learning rate I mean, and we achieve it about 2.3. Obviously, this is janky, and not exactly how you would train it in production, but this is roughly what you're going through. You first find a decent learning rate using the approach that I showed you. Then you start with that learning rate, and you train for a while. Then at the end, people like to do a learning rate decay where you decay the learning rate by say a factor of 10, and you do a few more steps, and then you get a trained network roughly speaking. We've achieved 2.3 and dramatically improved on the bi-gram language model using this simple neural net as described here using these 3,400 parameters. Now there's something we have to be careful with. I said that we have a better model because we are achieving a lower loss, 2.3, much lower than 2.45 with the bi-gram model previously. That's not exactly true, and the reason that's not true is that this is actually a fairly small model, but these models can get larger and larger if you keep adding neurons and parameters. You can imagine that we don't potentially have a thousand parameters, we could have 10,000, or 100,000, or millions of parameters. As the capacity of the neural network grows, it becomes more and more capable of overfitting your training set. What that means is that the loss on the training set, on the data that you're training on, will become very very low, as low as zero. All that the model is doing is memorizing your training set verbatim. If you take that model, and it looks like it's working really well, but you try to sample from it, you will basically only get examples exactly as they are in the training set. You won't get any new data. In addition to that, if you try to evaluate the loss on some withheld names or other words, you will actually see that the loss on those can be very high. Basically, it's not a good model. The standard in the field is to split up your data set into three splits as we call them. We have the training split, the dev split, or the validation split, and the test split. So training split, test or, sorry, dev or validation split, and test split. Typically this would be say 80% of your data set. This could be 10%, and this 10% roughly. So you have these three splits of the data. Now these 80% of your training, of the data set, the training set, is used to optimize the parameters of the model, just like we're doing here using gradient descent. These 10% of the examples, the dev or validation split, they're used for development over all the hyper parameters of your model. Hyper parameters are, for example, the size of this hidden layer, the size of the embedding. This is 100 or a two for us, but we could try different things, the strength of the regularization, which we aren't using yet so far. There's lots of different hyper parameters and settings that go into defining your neural net. You can try many different variations of them and see whichever one works best on your validation split. This is used to train the parameters, this is used to train the hyperprimers, and test split is used to evaluate basically the performance of the model at the end. We're only evaluating the loss on the test plate very very sparingly and very few times, because every single time you evaluate your test loss and you learn something from it, you are basically starting to also train on the test split. You are only allowed to test the loss on a test set very very few times, otherwise you risk overfitting to it as well as you experiment on your model. Let's also split up our training data into train, dev, and test, and then we are going to train on train and only evaluate on tests very very sparingly. Okay, so here we go. Here is where we took all the words and put them into `x` and `y` tensors. Instead, let me create a new cell here, and let me just copy paste some code here because I don't think it's that complex, but we're going to try to save a little bit of time. I'm converting this to be a function now. This function takes some list of words and builds the arrays `x` and `y` for those words only. Here I am shuffling up all the words. These are the input words that we get. We are randomly shuffling them all up. Then we're going to set `n1` to be the number of examples that there's 80% of the words, and `n2` to be 90% of the way of the words. If `len(words)` is 32,000, `n1` is, well sorry I should probably run this, `n1` is 25,000, and `n2` is 28,000. Here we see that I'm calling `build_data_set` to build the training set `x` and `y` by indexing into up to `n1`. We're going to have only 25,000 training words. Then we're going to have roughly `n2 - n1`, 3,000 validation examples or dev examples, and we're going to have `len(words)` basically minus `n2`, or 3,204 examples here for the test set. Now we have `x`'s and `y`'s for all those three splits. Oh yeah, I'm printing their size here inside the function as well, but here we don't have words, but these are already the individual examples made from those words. Let's now scroll down here. The data set now for training is more like this. Then when we reset the network, when we're training, we're only going to be training using `x_train`, `x_train`, and `y_train`. That's the only thing we're training on. Let's see where we are on the single batch. Let's now train maybe a few more steps. Training neural networks can take a while. Usually you don't do it inline. You launch a bunch of jobs and you wait for them to finish. It can take multiple days and so on. Luckily, this is a very small network. Okay, so the loss is pretty good. Oh, we accidentally used a learning rate that is way too low. Let me actually come back. We used the decay learning rate of 0.01, so this will train much faster. Here when we evaluate, let's use the dev set here, `x_dev` and `y_dev` to evaluate the loss. Let's now decay the learning rate and only do say 10,000 examples, and let's evaluate the dev loss once here. Okay, so we're getting about 2.3 on dev. The neural network when it was training did not see these dev examples. It hasn't optimized on them, and yet when we evaluate the loss on these dev, we actually get a pretty decent loss. We can also look at what the loss is on all of training set. Oops, we see that the training and the dev loss are about equal. We're not overfitting. This model is not powerful enough to just be purely memorizing the data. So far, we are what's called underfitting because the training loss and the dev or test losses are roughly equal. What that typically means is that our network is very tiny, very small, and we expect to make performance improvements by scaling up the size of this neural net. Let's do that now. Let's come over here, and let's increase the size of the neural net. The easiest way to do this is we can come here to the hidden layer which currently has 100 neurons, and let's just bump this up. Let's do 300 neurons, and then this is also 300 biases. Here we have 300 inputs into the final layer. Let's initialize our neural net. We now have ten thousand, ten thousand parameters instead of three thousand parameters. We're not using this. Here what I'd like to do is I'd like to actually uh keep track of uh tap um. Let's just do this. Let's keep stats again. Here when we're keeping track of the loss, let's just also keep track of the steps, and let's just have `i` here. Let's train on thirty thousand, or rather say, okay let's try thirty thousand, and we are at point one, and we should be able to run this and optimize the neural net. Here basically, I want to `plt.plot` the steps against the loss. These are the `x`'s and `y`'s, and this is the loss function and how it's being optimized. You see that there's quite a bit of thickness to this, and that's because we are optimizing over these mini batches, and the mini batches create a little bit of noise in this. Where are we in the dev set? We are at 2.5. We still haven't optimized this neural net very well, and that's probably because we made it bigger, it might take longer for this neural net to converge. Let's continue training. Yeah, let's just continue training. One possibility is that the batch size is so low that we just have way too much noise in the training, and we may want to increase the batch size so that we have a bit more correct gradient, and we're not thrashing too much, and we can actually like optimize more properly. Okay, this will now become meaningless because we've reinitialized these. Yeah, this looks not pleasing right now, but there probably is like a tiny improvement, but it's so hard to tell. Let's go again, 2.52. Let's try to decrease the learning rate by factor two. Okay, we're at 2.32. Let's continue training. We basically expect to see a lower loss than what we had before, because now we have a much much bigger model, and we were under fitting. We'd expect that increasing the size of the model should help the neural net. 2.32. Okay, so that's not happening too well. One other concern is that even though we've made the `tanh` layer here, or the hidden layer, much much bigger, it could be that the bottleneck of the network right now are these embeddings that are two dimensional. It can be that we're just cramming way too many characters into just two dimensions, and the neural net is not able to really use that space effectively, and that that is sort of like the bottleneck to our network's performance. Okay, 2.23. Just by decreasing the learning rate, I was able to make quite a bit of progress. Let's run this one more time, and then evaluate the training and the dev loss. One more thing after training that I'd like to do is I'd like to visualize the embedding vectors for these characters before we scale up the embedding size from two. We'd like to make this bottleneck potentially go away, but once I make this greater than two, we won't be able to visualize them. Here, okay, we're at 2.23 and 2.24. We're not improving much more. Maybe the bottleneck now is the character embedding size which is two. Here I have a bunch of code that will create a figure, and then we're going to visualize the embeddings that were trained by the neural net on these characters, because right now the embedding has just two. We can visualize all the characters with the `x` and the `y` coordinates as the two embedding locations for each of these characters. Here are the `x` coordinates and the `y` coordinates, which are the columns of `c`. For each one, I also include the text of the little character. What we see is actually kind of interesting. The network has basically learned to separate out the characters and cluster them a little bit. For example, you see how the vowels a, e, i, o, u are clustered up here. That's telling us that is that the neural net treats these is very similar, because when they feed into the neural net, the embedding for all these characters is very similar. The neural net thinks that they're very similar and kind of like interchangeable if that makes sense. The points that are really far away are for example, `q`. `q` is kind of treated as an exception, and `q` has a very special embedding vector so to speak. Similarly, `dot`, which is a special character, is all the way out here. A lot of the other letters are sort of like clustered up here. It's kind of interesting that there's a little bit of structure here after the training, and it's not definitely not random, and these embeddings make sense. We're now going to scale up the embedding size and won't be able to visualize it directly, but we expect that because we're under fitting, and we made this layer much bigger and did not sufficiently improve the loss, we're thinking that the constraint to better performance right now could be these embedding pictures. Let's make them bigger. Let's scroll up here. Now we don't have two dimensional embeddings. We are going to have say 10 dimensional embeddings for each word. Then this layer will receive three times 10, so 30 inputs will go into the hidden layer. Let's also make the hidden layer a bit smaller. Instead of 300, let's just do 200 neurons in that hidden layer. Now the total number of elements will be slightly bigger at 11,000. Here we have to be a bit careful because, okay, the learning rate we set to 0.1. Here we are hardcoded in six, and obviously if you're working in production, you don't want to be hard-coding magic numbers. Instead of six, this should now be 30. Let's run for 50,000 iterations, and let me split out the initialization here outside so that when we run this cell multiple times, it's not going to wipe out our loss. In addition to that, here let's instead of logging `loss.item`, let's actually log the, let's do log10, I believe that's a function, of the loss. I'll show you why in a second. Let's optimize this. Basically, I'd like to plot the log loss instead of the loss, because when you plot the loss, many times it can have this hockey stick appearance, and log squashes it in. It just kind of like looks nicer. The x-axis is step i, and the y-axis will be the loss i. Here, this is 30. Ideally, we wouldn't be hard-coding these. Let's look at the loss. Okay, it's again very thick, because the mini batch size is very small, but the total loss over the training se. The tests and the dev set are both at 2.38, so so far so good. Let's try to decrease the learning rate by a factor of 10 and train for another 50,000 iterations. We would hope to beat 2.32, but we are doing this haphazardly. I don't have confidence that our learning rate or learning rate decay is set very well. The optimization here is suspect, and this isn't how you would typically do it in production. In production, you would create parameters or hyper parameters out of all these settings. Then, you would run lots of experiments to see which ones are working well. Okay, so we have 2.17 now and 2.2. You see how the training and validation performance are starting to slowly depart? Maybe we're getting the sense that the neural net is good enough or that the number of parameters is large enough that we are slowly starting to overfit. Let’s run one more iteration and see where we get. You would be running lots of experiments and then scrutinizing which ones give you the best dev performance. Once you find the hyper parameters that make your dev performance good, you evaluate the test set performance a single time. That’s the number that you report in your paper. Let's rerun the plot, train, and dev. Because we're getting lower loss, it's likely the embedding size was holding us back. Okay, so we’re roughly getting 2.16 and 2.19. There are many ways to go from here. We can continue tuning the optimization, playing with the sizes of the neural net, or increasing the number of words or characters we take as input. Instead of three characters, we could take more as input, which could further improve the loss. I changed the code slightly, so we have 200,000 steps of optimization. In the first 100,000, we’re using a learning rate of 0.1, and in the next 100,000, we're using a learning rate of 0.01. This is the loss I achieved, and these are the performances on the training and validation loss. The best validation loss I’ve obtained in the last 30 minutes is 2.17. Now, I invite you to beat this number. You have quite a few knobs available to you. You can change the number of neurons in the hidden layer, the dimensionality of the embedding lookup table, the number of characters that are feeding in as input, and the details of the optimization. This includes how long are we running, what is the learning rate, how does it change over time, and how does it decay. You can also change the batch size and may achieve a much better convergence speed. I invite you to read this 19-page paper. At this point, you should be able to understand a good chunk of it, and it has quite a few ideas for improvements. All those are available to you, and you should be able to beat this number. I'm leaving that as an exercise for the reader. That's it for now, and I'll see you next time. Before we wrap up, I also wanted to show how you would sample from the model. We're going to generate 20 samples. First, we begin with all dots as the context. Until we generate the zeroth character again, we're going to embed the current context using the embedding table. Usually, the first dimension was the size of the training set, but here we are working with a single example, so this is just a dimension of one for simplicity. This embedding gets projected into the end state, and we get the logits. Now, we calculate the probabilities using `f.softmax` of logits. That exponentiates the logits and makes them sum to one. Similar to cross-entropy, it is careful that there are no overflows. Once we have the probabilities, we sample from them using `torch.multinomial` to get our next index. Then, we shift the context window to append the index and record it. We can decode all the integers to strings and print them out. These are some example samples. You can see that the model now works much better. The words here are more word-like or name-like. We have things like "ham joes". It's starting to sound more name-like. We are definitely making progress, but we can still improve this model quite a lot. There's some bonus content I wanted to mention. I want to make these notebooks more accessible, so you don't have to install Jupyter notebooks and torch. I will be sharing a link to a Google Colab, which will look like a notebook in your browser. You can go to the URL and execute all the code that you saw. This is me executing the code in this lecture, shortened a little. You can train the exact same network and then plot and sample from the model. Everything is ready for you to tinker with the numbers right there in your browser, with no installation necessary. The link to this will be in the video description.

