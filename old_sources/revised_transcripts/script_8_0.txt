Okay, let's cover the process of tokenization in large language models. I have a set face because tokenization is my least favorite part of working with large language models. Unfortunately, it is necessary to understand in some detail because it is fairly complex and there are many hidden issues to be aware of. Many of the oddities with large language models trace back to tokenization. What is tokenization? In my previous video, "Let's Build GPT from scratch," we already did tokenization. However, it was a very naive and simple version. If you go to the Google Colab for that video, you will see that we loaded our training set. Our training set was the Shakespeare dataset. Initially, the Shakespeare dataset is just a large string in Python. It is just text. The question is: how do we plug text into large language models? In this case, we created a vocabulary of 65 possible characters that we saw in the string. These were the possible characters, and we found that there were 65 of them. Then, we created a lookup table for converting from every possible character, or little string piece, into a token, which is an integer. For example, we tokenized the string "Hi there" and received this sequence of tokens. Here, we took the first 1,000 characters of our dataset and encoded them into tokens. Since this was character-level tokenization, we received 1,000 tokens in a sequence, such as token 18, 47, etc. Later, we saw that the way we plug these tokens into the language model is by using an embedding table. If we have 65 possible tokens, then this embedding table will have 65 rows. Roughly speaking, we take the integer associated with every single token and use that as a lookup into this table. We then pluck out the corresponding row. This row contains trainable parameters that we will train using backpropagation. This vector then feeds into the Transformer. That is how the Transformer perceives every single token. We had a very naive tokenization process that was a character-level tokenizer. In practice, state-of-the-art language models use much more complicated schemes for constructing these token vocabularies. We are not dealing on the character level, but on the chunk level. These character chunks are constructed using algorithms such as the byte pair encoding algorithm, which we will go into in detail in this video. I would like to briefly show you the paper that introduced byte-level encoding as a mechanism for tokenization in the context of large language models. I would say that is probably the GPT-2 paper. If you scroll down to the "Input Representation" section, this is where they cover tokenization. They discuss the types of properties that you would like the tokenization to have. They conclude that they are going to have a tokenizer with a vocabulary of 50,257 possible tokens. The context size will be 1,024 tokens. In the attention layer of the Transformer neural network, every single token is attending to the previous tokens in the sequence, up to 1,024 tokens. Tokens are this fundamental unit of large language models. Everything is in units of tokens. Tokenization is the process for translating strings or text into sequences of tokens, and vice versa. When you go to the Llama 2 paper, if you search for "token," you will find 63 hits. Again, tokens are pervasive. Here, they mention that they trained on two trillion tokens of data. We are going to build our own tokenizer. Luckily, the byte pair encoding algorithm is not super complicated. We can build it from scratch ourselves and see exactly how it works. Before we dive into code, I would like to give you a brief taste of some of the complexities that arise from tokenization. I want to make sure that we have sufficient motivation for why we are doing this and why it can be so problematic. Tokenization is at the heart of a lot of weirdness in large language models. I advise that you do not brush it off. Many issues that may appear to be issues with the neural network architecture or the large language model itself are actually issues with the tokenization. These issues fundamentally trace back to tokenization. If you have noticed any issues with large language models not being able to easily do spelling tasks, that is usually due to tokenization. Simple string processing can be difficult for the large language model to perform natively. Non-English languages can work much worse. To a large extent, this is due to tokenization. Sometimes, LLMs are bad at simple arithmetic, which can also be traced to tokenization. GPT-2 specifically would have had quite a few more issues with Python than future versions due to tokenization. There are many other issues, such as the weird warnings about trailing white spaces, which is a tokenization issue. If you had asked GPT earlier about "solid gold Magikarp" and what it is, the LLM would go totally crazy and start going off on a completely unrelated tangent. You may have been told to use YAML over JSON in structured data. All of that has to do with tokenization. Basically, tokenization is at the heart of many issues. I will come back to these at the end of the video, but for now, let me just skip over it a little bit and let's go to this web app, the Tik tokenizer bell.app. I have it loaded here. What I like about this web app is that tokenization is running live in your browser using JavaScript. You can just type here, for example, "hello world." The whole string is tokenized. On the left is the string that you put in. On the right, we are currently using the GPT-2 tokenizer. This string that I pasted here is currently tokenizing into 300 tokens. Here, they are shown explicitly in different colors for every single token. For example, the word "tokenization" became two tokens: the token 3,642 and 1,634. The token for a space is token 318. Be careful, at the bottom you can show whitespace. Keep in mind that there are spaces and new line characters here. You can hide them for clarity. The token "space at" is token 379. The token "space the" is 262. You will notice that the space is part of that token chunk. This is kind of how our English sentence breaks up, and it seems all well and good. Here I put in some arithmetic. We see that token 127 is a single token for the plus sign. Then, there is token six, space, and then 6, followed by 77. What is happening is that 127 is feeding in as a single token into the large language model. However, the number 677 will feed in as two separate tokens. The large language model has to take account of that and process it correctly in its network. The number 804 will be broken up into two tokens. This is all completely arbitrary. Sometimes, you have multiple digits as a single token. Sometimes you have individual digits as multiple tokens. It is all arbitrary and coming out of the tokenizer. Here is another example. We have the string "egg," and you see that this became two tokens. However, for some reason, when I say, "I have an egg," you see when it's "space egg," it is a single token. "Egg" by itself at the beginning of a sentence is two tokens. Here, as "space egg," it is suddenly a single token, for the exact same string. Lowercase "egg" turns out to be a single token, and in particular, notice that the color is different. This is a different token. This is case-sensitive. Of course, a capital "Egg" would also be different tokens. Again, this would be two tokens arbitrarily. For the same concept of "egg," depending on if it is at the beginning of a sentence, at the end of the sentence, lowercase, uppercase, or mixed, all of these will be very different tokens with different IDs. The language model has to learn from raw data, from all the internet text that it is trained on, that these are actually all the exact same concept. It has to group them in the parameters of the neural network and understand, just based on the data patterns, that these are all very similar, maybe not almost exactly similar, but very, very similar. After the "egg" demonstration, here is an introduction from OpenAI's ChatGPT in Korean, "manaso pangâ€¦". The reason I put this here is because you will notice that non-English languages work slightly worse in ChatGPT. Part of this is, of course, because the training dataset for ChatGPT is much larger for English and everything else. However, the same is true not just for the large language model itself, but also for the tokenizer. When we train the tokenizer, we will see that there is a training set as well. There is a lot more English than non-English. What ends up happening is that we are going to have a lot of longer tokens for English. If you have a single sentence in English and you tokenize it, you might see that it is 10 tokens or something like that. If you translate that sentence into say, Korean or Japanese, you will typically see that the number of tokens used is much larger. This is because the chunks here are much more broken up. We are using a lot more tokens for the exact same thing. This bloats up the sequence length of all the documents. You are using up more tokens. Then in the attention of the Transformer, when these tokens try to attend to each other, you are running out of context in the maximum context length of the Transformer. All non-English text is stretched out from the perspective of the Transformer. This has to do with the training data used for the tokenizer and the tokenization itself. It will create a lot bigger tokens and larger groups in English and have a lot of little boundaries for all the other non-English text. If we translated this into English, it would be significantly fewer tokens. The final example I have here is a little snippet of Python for doing FizzBuzz. What I would like you to notice is that all of these individual spaces are separate tokens. They are token 220. Token 220, 220, 220, and then "space if" is a single token. When the Transformer is going to consume or create this text, it needs to handle all of these spaces individually. They all feed in one by one into the entire Transformer in the sequence. This is extremely wasteful. Tokenizing it this way causes GPT-2 to not be very good with Python. This has nothing to do with coding or the language model itself. If you use a lot of indentation with spaces in Python, you just end up bloating out all of the text. It is separated across way too much of the sequence. We are running out of context length in the sequence. We are being too wasteful and taking up way too much token space. We can also scroll up here and change the tokenizer. Note here that the GPT-2 tokenizer creates a token count of 300 for this string. We can change it to CL 100K base, which is the GPT-4 tokenizer. We see that the token count drops to 185. For the exact same string, we now have roughly half the number of tokens. The number of tokens in the GPT-4 tokenizer is roughly double that of the number of tokens in the GPT-2 tokenizer. We went from roughly 50k to roughly 100k. You can imagine that this is a good thing because the same text is now squished into half as many tokens. This is a much denser input to the Transformer. In the Transformer, every single token has a finite number of tokens before it that it is going to pay attention to. What this is doing is we are roughly able to see twice as much text as context for what token to predict next. This is due to this change. Of course, increasing the number of tokens is not strictly better infinitely. As you increase the number of tokens, your embedding table gets much larger. At the output, we are trying to predict the next token, and the softmax there also grows. We will go into more detail later on this. There is some kind of sweet spot where you have just the right number of tokens in your vocabulary, where everything is appropriately dense and still fairly efficient. One thing I would like you to note specifically for the GPT-4 tokenizer is that the handling of whitespace for Python has improved a lot. You see that here, these four spaces are represented as one single token. For the three spaces here, it is a token for "space if". Here, seven spaces were grouped into a single token. We are being a lot more efficient in how we represent Python. This was a deliberate choice made by OpenAI when they designed the GPT-4 tokenizer. They group a lot more space into a single character. This densifies Python, so we can attend to more code when we are trying to predict the next token in the sequence. The improvement in Python coding ability from GPT-2 to GPT-4 is not just a matter of the language model and the architecture and the details of the optimization. A lot of the improvement here is also coming from the design of the tokenizer and how it groups characters into tokens. So let's now start writing some code. Remember, what we want to do is take strings and feed them into language models. For that, we need to somehow tokenize strings into some integers, using some fixed vocabulary. Then we will use those integers to look up vectors in a lookup table and feed those vectors into the Transformer as input. The reason this gets a little tricky is that we don't just want to support the simple English alphabet. We want to support different languages. This is "anango" in Korean, which means hello. We also want to support many kinds of special characters that we might find on the internet, like emojis. How do we feed this text into Transformers? What is this text anyway in Python? If you go to the documentation of a string in Python, you can see that strings are immutable sequences of Unicode code points. What are Unicode code points? We can go to the Unicode Consortium. Unicode code points are defined by the Unicode Consortium as part of the Unicode standard. This is just a definition of roughly 150,000 characters right now. It also defines what they look like and what integers represent those characters. It states that there are 150,000 characters across 161 scripts as of right now. If you scroll down here, you can see that the standard is very much alive. The latest standard is 15.1 from September 2023. This is just a way to define many types of characters, like all these characters across different scripts. We can access the Unicode code point of a single character by using the `ord` function in Python. For example, I can pass in `ord('H')` and see that the Unicode code point for the single character 'H' is 104. This can be arbitrarily complicated. We can take our emoji here and see that the code point for this one is 128,000. Or we can take "an" and see that this is 50,000. Keep in mind you can't plug in strings here because this doesn't have a single code point; it only takes a single Unicode code point character and tells you its integer. In this way, we can look up all the characters of this specific string and their code points. `ord(x) for x in this_string` gives us this encoding. We have already turned the raw code points into integers. Why can't we simply just use these integers and not have any tokenization at all? Why can't we just use this natively as is and use the code point? One reason is that the vocabulary in that case would be quite long. In this case, for Unicode, this is a vocabulary of 150,000 different code points. More worryingly, I think the Unicode standard is very much alive, and it keeps changing. It is not necessarily a stable representation that we might want to use directly. For these reasons, we need something better. To find something better, we turn to encodings. If we go to the Wikipedia page, we see that the Unicode Consortium defines three types of encodings: UTF-8, UTF-16, and UTF-32. These encodings are the way by which we can take Unicode text and translate it into binary data or byte streams. UTF-8 is by far the most common. This is the UTF-8 page. This Wikipedia page is quite long, but what is important for our purposes is that UTF-8 takes every single code point and translates it to a byte stream. This byte stream is between one to four bytes, so it is a variable-length encoding. Depending on the Unicode point, according to the schema, you will end up with between one to four bytes for each code point. On top of that, there are UTF-16 and UTF-32. UTF-32 is nice because it is fixed length instead of variable length, but it has many other downsides. The full spectrum of pros and cons of all these different three encodings is beyond the scope of this video. I just like to point out that I enjoyed this blog post. This blog post at the end also has a number of references that can be quite useful. One of them is the "UTF-8 Everywhere Manifesto." This manifesto describes the reasons why UTF-8 is significantly preferred and a lot nicer than the other encodings, and why it is used a lot more prominently on the internet. One of the major advantages, to give you a sense, is that UTF-8 is the only one of these that is backwards compatible to the much simpler ASCII encoding of text. I am not going to go into the full detail in this video. It is enough to say that we like the UTF-8 encoding. Let's try to take the string and see what we get if we encode it into UTF-8. The string class in Python has a `.encode` method, and you can give it the encoding, such as "utf-8." What we get out of this is a bytes object. It's not very nice in the way that it is printed. I personally like to take it through a list. Then, we get the raw bytes of this encoding. These are the raw bytes that represent this string according to the UTF-8 encoding. We can also look at UTF-16. We get a slightly different byte stream, and we start to see one of the disadvantages of UTF-16. You see how we have "0 0 something, 0 something, 0 something." We start to get a sense that this is a bit of a wasteful encoding. Indeed, for simple ASCII characters or English characters, we just have the structure of "0 something, 0 something," and it is not exactly nice. It's the same for UTF-32. When we expand this, we can start to get a sense of the wastefulness of this encoding. You see a lot of zeros followed by something. This is not desirable. We would like to stick with UTF-8 for our purposes. However, if we just use UTF-8 naively, these are byte streams, which would imply a vocabulary length of only 256 possible tokens. This vocabulary size is very, very small. If we were to use it naively, all of our text would be stretched out over very long sequences of bytes. The embedding table would be tiny, and the prediction at the final layer would be tiny, but our sequences are very long. Remember, we have pretty finite context length and the attention that we can support in a Transformer for computational reasons. We only have so much context length, but now we have very, very long sequences. This is inefficient. It will not allow us to attend to sufficiently long text before us for the purposes of the next token prediction task. We don't want to use the raw bytes of the UTF-8 encoding. We want to be able to support a larger vocabulary size that we can tune as a hyperparameter. We want to stick with the UTF-8 encoding of these strings. What do we do? The answer is that we turn to the byte pair encoding algorithm. This will allow us to compress these byte sequences to a variable amount. We will get to that in a bit, but I just want to briefly speak to the fact that I would love nothing more than to be able to feed raw byte sequences into language models. There is a paper about how this could potentially be done from last summer. The problem is that you actually have to go in and modify the Transformer architecture. As I mentioned, you will have a problem where the attention will start to become extremely expensive because the sequences are so long. In this paper, they propose a hierarchical structuring of the Transformer that could allow you to just feed in raw bytes. At the end, they say, "Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale." Tokenization-free would indeed be amazing. We could just feed byte streams directly into our models. Unfortunately, I do not know that this has really been proven out yet by sufficiently many groups at a sufficient scale. Something like this at one point would be amazing, and I hope someone comes up with it. For now, we have to come back and we can't feed this directly into language models. We have to compress it using the byte pair encoding algorithm. Let's see how that works. As I mentioned, the byte pair encoding algorithm is not all that complicated. The Wikipedia page is quite instructive as far as the basic idea goes. What we are doing is we have some kind of input sequence. For example, here we have only four elements in our vocabulary: a, b, c, and d. We have a sequence of them. Instead of bytes, let's say we just have a vocabulary size of four. The sequence is too long, and we'd like to compress it. We iteratively find the pair of tokens that occur the most frequently. Once we have identified that pair, we replace it with a single new token that we append to our vocabulary. For example, here, the byte pair "AA" occurs most often. We mint a new token, let's call it capital Z. We replace every single occurrence of "AA" with Z. Now, we have two Z's here. We took a sequence of 11 characters with a vocabulary size of four and converted it to a sequence of only nine tokens, but with a vocabulary of five because we have a fifth vocabulary element that we just created. That is Z, standing for the concatenation of "AA." We can again repeat this process. We again look at the sequence and identify the pair of tokens that are most frequent. Let's say that that is now "AB." We are going to replace "AB" with a new token that we will call Y. Y becomes "AB," and every single occurrence of "AB" is replaced with Y. We end up with this. Now, we only have seven characters in our sequence. We have not just four vocabulary elements or five, but now we have six. For the final round, we again look through the sequence, find that the phrase "ZY" or the pair "ZY" is most common, and replace it one more time with another character, let's say X. X is "ZY". We replace all occurrences of "ZY" and we get the following sequence. After we have gone through this process, instead of having a sequence of 11 tokens with a vocabulary length of four, we now have a sequence of five tokens. Our vocabulary length is now seven. In this way, we can iteratively compress our sequence. We mint new tokens. In the exact same way, we start with byte sequences, so we have a 256 vocabulary size. We are going to go through these and find the byte pairs that occur the most. We are going to iteratively start minting new tokens, appending them to our vocabulary, and replacing things. In this way, we are going to end up with a compressed training dataset and also an algorithm for taking any arbitrary sequence and encoding it using this vocabulary. We can also decode it back to strings. Let's now implement all of that. Here is what I did. I went to the blog post that I enjoyed and took the first paragraph and copy-pasted it into the notebook. This is one very long line. To get the tokens, as I mentioned, we take our text and encode it into UTF-8. The tokens here will be raw bytes, a single stream of bytes. So that it is easier to work with, instead of just a bytes object, I'm going to convert all those bytes to integers. I will create a list of it so it's easier to manipulate, work with, and visualize in Python. Here I'm printing all of that. This is the original paragraph, and its length is 533 code points. Here are the bytes encoded in UTF-8. We see that this has a length of 616 bytes or 616 tokens. The reason this is more is because a lot of these simple ASCII characters or simple characters just become a single byte. A lot of these more complex Unicode characters become multiple bytes, up to four. So we are expanding the size. Now, what we would like to do as a first step of the algorithm is to iterate over this and find the pair of bytes that occurs most frequently because we are then going to merge them. If you are working on a notebook on the side, then I encourage you to click on the link, find this notebook, and try to write that function yourself. Otherwise, I'm going to come here and implement first the function that finds the most common pair. Here's what I came up with. There are many different ways to implement this, but I'm calling the function `get_stats`. It expects a list of integers. I'm using a dictionary to keep track of the counts. This is a Pythonic way to iterate through consecutive elements of this list, which we covered in the previous video. Here I am incrementing the count by one for all the pairs. If I call this on all the tokens, the stats come out here. This is the dictionary. The keys are tuples of consecutive elements, and this is the count. To print it in a slightly better way, this is one way that I like to do that. You can pause if you like. We iterate over all the items. The items called on the dictionary return pairs of key and value. Instead, I create a list here of value and key. If it is a value-key list, then I can call `sort` on it. By default, Python will use the first element, which in this case will be the value, to sort by if it is given tuples, and then reverse it so it is descending. Then, print that. It looks like 101, 32 was the most commonly occurring consecutive pair, and it occurred 20 times. We can double-check that, and that makes reasonable sense. If I search for 101, 32, then you see that these are the 20 occurrences of that pair. If we would like to take a look at what exactly that pair is, we can use `chr`, which is the opposite of `ord` in Python. We give it a Unicode code point, so `chr(101)` and `chr(32)`, and we see that this is "e" and "space". There is a lot of "e space" here. A lot of words seem to end with "e". Here's "e space" as an example. There is a lot of that going on here. This is the most common pair. Now that we have identified the most common pair, we would like to iterate over this sequence. We are going to mint a new token with the ID of 256, because these tokens currently go from 0 to 255. When we create a new token, it will have an ID of 256. We are going to iterate over this entire list, and every time we see 101, 32, we are going to swap that out for 256. Let's implement that now, and feel free to do that yourself as well. First, I commented this so we don't pollute the notebook too much. This is a nice way of obtaining the highest-ranking pair in Python. We are basically calling `max` on this dictionary `stats`, and this will return the maximum key. The question is: how does it rank keys? You can provide it with a function that ranks the keys. That function is just `stats.get`. `stats.get` would basically return the value. We are ranking by the value and getting the maximum key. It is 101, 32, as we saw. To actually merge 101, 32, this is the function that I wrote. There are many different versions of it. We will take a list of IDs and the pair that we want to replace. That pair will be replaced with the new index, `idx`. Iterating through the IDs, if we find the pair, swap it out for `idx`. We create a new list, start at zero, and go through this entire list sequentially from left to right. Here, we are checking for equality at the current position with the pair.
