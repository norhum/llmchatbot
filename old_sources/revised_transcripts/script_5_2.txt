The local derivative is just one, so there's nothing to do there. This is then multiplied by the global derivative to create the chain rule. This is just times the `dbm_bar`. This is our candidate. Let me bring this down and uncomment the check, and we see that we have the correct result. Before we propagate through the next line, I want to briefly talk about the note here where I'm using the Bessel's correction, dividing by `n - 1` instead of dividing by `n` when I normalize the sum of squares. You'll notice that this is a departure from the paper, which uses one over `n` instead of one over `n - 1`. Their `m` is `RN`, and it turns out that there are two ways of estimating the variance of an array. One is the biased estimate, which is one over `n`, and the other one is the unbiased estimate, which is one over `n - 1`. Confusingly, in the paper, this is not very clearly described, and it's a detail that matters. They are using the biased version during training time, but later, when they are talking about inference, they mention that when they do inference, they are using the unbiased estimate, which is the `n - 1` version, to calibrate the running mean and the running variance. They actually introduce a train-test mismatch where, in training, they use the biased version, and in test time, they use the unbiased version. I find this extremely confusing. You can read more about the Bessel's correction and why dividing by `n - 1` gives you a better estimate of the variance in a case where you have population size or samples for the population that are very small, which is indeed the case for us because we are dealing with many patches, and these mini batches are a small sample of a larger population, which is the entire training set. It turns out that if you estimate it using one over `n`, that almost always underestimates the variance, and it is a biased estimator. It is advised that you use the unbiased version and divide by `n - 1`. You can go through the article I've linked, which describes the full reasoning. I'll link it in the video description. When you calculate the `torch.var` variance, you'll notice that they take the unbiased flag for whether or not you want to divide by `n` or `n - 1`. Confusingly, they do not mention what the default is for unbiased, but I believe that unbiased by default is true. I'm not sure why the docs don't cite that. In `BatchNorm1d`, the documentation is again kind of wrong and confusing. It says that the standard deviation is calculated via the biased estimator, but this is not exactly right, and people have pointed out that it is not right in a number of issues since then. The rabbit hole is deeper, and they follow the paper exactly and use the biased version for training. But when they're estimating the running standard deviation, they're using the unbiased version, so again, there's the train-test mismatch. Long story short, I'm not a fan of train-test discrepancies. I consider the fact that we use the bias version during training and the unbiased version during test time to be a bug. I don't think there's a good reason for that; they don't really go into the detail of the reasoning behind it in this paper. That's why I prefer to use the Bessel's correction in my own work. Unfortunately, `BatchNorm` does not take a keyword argument that tells you whether or not you want to use the unbiased version or the biased version in both train and test. Therefore, anyone using batch normalization has a bit of a bug in the code, in my view. This turns out to be much less of a problem if your mini-batch sizes are larger, but still, I find this kind of unpardonable. Maybe someone can explain why this is okay, but for now, I prefer to use the unbiased version consistently during both training and test time, and that's why I'm using one over `n - 1` here. Okay, let's now backpropagate through this line. The first thing that I always like to do is scrutinize the shapes first. Looking at the shapes of what's involved, I see that `b` and `var` shape is 1 by 64, so it's a row vector, and `bndiff_2` shape is 32 by 64. Clearly, here, we're doing a sum over the zeroth axis to squash the first dimension of the shapes using a sum. That right away hints that there will be some kind of a replication or broadcasting in the backward pass. Maybe you're noticing the pattern, but basically, anytime you have a sum in the forward pass, that turns into a replication or broadcasting in the backward pass along the same dimension. Conversely, when we have a replication or a broadcasting in the forward pass, that indicates variable reuse, and so, in the backward pass, that turns into a sum over the exact same dimension. Hopefully, you're noticing the duality that those two are kind of like the opposite of each other in the forward and backward pass. Once we understand the shapes, the next thing I like to do is look at a toy example in my head to understand roughly how the variable dependencies go in the mathematical formula. Here we have a two-dimensional array of `bndiff_2`, which we are scaling by a constant, and then we are summing vertically over the columns. If we have a 2x2 matrix `a` and then we sum over the columns and scale, we would get a row vector `b1`, `b2`. `b1` depends on `a` in this way, where it is a sum scaled of `a`, and `b2` in this way, where it's the second column summed and scaled. Looking at this, basically what we want to do now is we have the derivatives on `b1` and `b2`, and we want to backpropagate them into the `a`s. It's clear that, differentiating in your head, the local derivative here is one over `n - 1` times one for each one of these `a`s, and the derivative of `b1` has to flow through the columns of `a` scaled by one over `n - 1`. That's roughly what's happening here. Intuitively, the derivative flow tells us that `dbndiff_2` will be the local derivative of this operation. There are many ways to do this, but I like to do something like `torch.ones_like` of `bndiff_2`. I'll create a large two-dimensional array of ones, and then I will scale it, so 1.0 divided by `n - 1`. This is an array of one over `n - 1`, which is sort of like the local derivative. Now, for the chain rule, I will simply just multiply it by `dbm_bar`. Notice what's going to happen here: this is 32 by 64, and this is just 1 by 64. I'm letting the broadcasting do the replication because, internally in PyTorch, `dbm_bar`, which is a 1 by 64 row vector, will get copied vertically until the two are of the same shape, and then there will be an element-wise multiply. Broadcasting is basically doing the replication, and I will end up with the derivatives of `dbndiff_2` here. This is the candidate solution. Let's bring it down here. Let's uncomment this line where we check it, and let's hope for the best. Indeed, we see that this is the correct formula. Next up, let's differentiate here. We have that `bndiff` is element-wise squared to create `bndiff_2`. This is a relatively simple derivative because it's a simple element-wise operation, similar to the scalar case. We have that `dbndiff` should be, if this is x squared, then the derivative is 2x, so it's simply 2 times `bndiff`. That's the local derivative, and then times the chain rule. The shapes of these are the same, so times this. That's the backward pass for this variable. Let me bring that down here. Now we have to be careful because we already calculated `dbm_diff`. This is just the end of the other branch coming back to `bndiff` because `bndiff` was already backpropagated from `bmraw`. We have now completed the second branch, and that's why I have to do `+=`. If you recall, we had an incorrect derivative for `bndiff` before, and I'm hoping that once we append this last missing piece, we have the exact correctness. Let's run, and `dbndiff` now actually shows the exact correct derivative. That's comforting. Okay, let's now backpropagate through this line. The first thing we do, of course, is we check the shapes. I wrote them out here. The shape of `bndiff` is 32 by 64. `hpbn` is the same shape, but `bn_mean` is a row vector, 1 by 64. This minus here will actually do broadcasting, and we have to be careful with that. As a hint to us, because of the duality, a broadcasting in the forward pass means a variable reuse, and therefore, there will be a sum in the backward pass. Let's write out the backward pass. Backpropagating into the `hpbn`, because these are the same shape, the local derivative for each one of the elements here is just one for the corresponding element in here. The gradient simply copies; it's just a variable assignment. I'm just going to clone this tensor just for safety to create an exact copy of `dbndiff`. To backpropagate into `bn_mean`, the local derivative is negative `torch.ones_like` of the shape of `bndiff`, times the derivative here, `dbndiff`. This is the backpropagation for the replicated `bn_mean`. I still have to backpropagate through the replication in the broadcasting, and I do that by doing a sum. I'm going to take this whole thing, and I'm going to do a sum over the zeroth dimension, which was the replication. If you scrutinize this, you'll notice that this is the same shape as that. What I'm doing doesn't actually make that much sense because it's just an array of ones multiplying `dbndiff`. In fact, I can just do this, and that is equivalent. This is the candidate backward pass. Let me copy it here, and then let me comment out this one. Enter, and it's wrong. Actually, sorry, this is supposed to be wrong because we are backpropagating from `bndiff` into `hpbn`, and but we're not done. `bn_mean` depends on `hpbn`, and there will be a second portion of that derivative coming from the second branch. We're not done yet, and we expect it to be incorrect. There you go. Let's now backpropagate from `bn_mean` into `hpbn`. Here, again, we have to be careful because there's a broadcasting along, or there's a sum along, the zeroth dimension, so this will turn into broadcasting in the backward pass. I'm going to go a little faster on this line because it is very similar to the line that we had before. `hpbn` will be scaled by 1 over `n`, and then this gradient here on `dbn_mean` is going to be scaled by 1 over `n`, and then it's going to flow across all the columns and deposit itself into the `hpbn`. What we want is this thing scaled by 1 over `n`. I'll put the constant up front here. We scale down the gradient, and now we need to replicate it across all the rows. I like to do that by `torch.ones_like` of `hpbn`, and I will let the broadcasting do the work of replication like that. This is `dhpbn`, and hopefully, we can `+=` that. This here is broadcasting, and then this is the scaling. This should be correct. Okay, so that completes the backpropagation of the batchnorm layer. We are now here. Let's backpropagate through the linear layer. Now, because everything is getting a little vertically crazy, I copy-pasted the line here, and let's just backpropagate through this one line. First, of course, we inspect the shapes. We see that `dhpbn` is 32 by 64. `mcat` is 32 by 30. `w1` is 30 by 64, and `b1` is just 64. As I mentioned, backpropagating through linear layers is fairly easy just by matching the shapes. Let's do that. We have that `dmcat` should be some matrix multiplication of `dhpbn` with `w1`, with one transpose thrown in there. To make `mcat` be 32 by 30, I need to take `dhpbn`, 32 by 64, and multiply it by `w1.transpose` to get the one I need to end up with 30 by 64. To get that, I need to take `mcat.transpose` and multiply that by `dhpbn`. Finally, to get `db1`, this is an addition, and we saw that basically, I need to sum the elements in `dhpbn` along some dimension. To make the dimensions work out, I need to sum along the zeroth axis here to eliminate this dimension. We do not keep dims, so that we want to get a single one-dimensional vector of 64. These are the claimed derivatives. Let me put that here, and let me uncomment these three lines and cross our fingers. Everything is great. Okay, so we now continue. Almost there. We have the derivative of `mcat`, and we want to backpropagate into `m`. I again copied this line over here. This is the forward pass, and these are the shapes. Remember that the shape here was 32 by 30, and the original shape of `m` was 32 by 3 by 10. This layer, in the forward pass, did the concatenation of these three 10-dimensional character vectors. Now, we just want to undo that. This is actually a relatively straightforward operation because the backward pass of the view is just a representation of the array; it's just a logical form of how you interpret the array. Let's just re-interpret it to be what it was before. In other words, the derivative of `m` is not 32 by 30, it is basically `dmcat`, but if you view it as the original shape, which is just `m.shape`, you can pass tuples into view, and so this should just be okay. We just re-represent that view, and then we uncomment this line here. Hopefully, yeah, the derivative of `m` is correct. In this case, we just have to re-represent the shape of those derivatives into the original view. Now we are at the final line, and the only thing that's left to backpropagate through is this indexing operation here, `m_scat_xb`. As I did before, I copy-pasted this line here, and let's look at the shapes of everything that's involved and remind ourselves how this worked. `m.shape` was 32 by 3 by 10. There are 32 examples, and then we have three characters, each one of them has a 10-dimensional embedding. This was achieved by taking the lookup table `C`, which has 27 possible characters, each of them 10-dimensional, and we looked up the rows that were specified inside this tensor `xb`. `xb` is 32 by 3, and it's basically giving us, for each example, the identity or the index of which character is part of that example. I'm showing the first five rows of three of this tensor `xb`. We can see that, for example, here it was the first example in this batch, and that the first character, the first character, and the fourth character come into the neural net. Then, we want to predict the next character in a sequence after the characters one, one, four. What's happening here is that there are integers inside `xb`, and each one of these integers is specifying which row of `C` we want to pluck out, right? Then, we arrange those rows that we've plucked out into a 32 by 3 by 10 tensor. We just package them into the tensor. Now, we have `dm`, so for every one of these plucked-out rows, we have their gradients now, but they're arranged inside this 32 by 3 by 10 tensor. All we have to do now is just route this gradient backward through this assignment. We need to find which row of `C` every one of these 10-dimensional embeddings comes from, and then we need to deposit them into `dc`. We just need to undo the indexing. Of course, if any of these rows of `C` was used multiple times, which almost certainly is the case, like row one was used multiple times, then we have to remember that the gradients that arrive there have to add. For each occurrence, we have to have an addition. Let's now write this out. I don't actually know if there's a much better way to do this than a for loop in Python. Maybe someone can come up with a vectorized, efficient operation, but for now, let's just use for loops. Let me create a `torch.zeros_like(C)` to initialize a 27 by 10 tensor of all zeros. Then, for `k` in range `xb.shape[0]`, maybe someone has a better way to do this, but for `j` in range `xb.shape[1]`, this is going to iterate over all the elements of `xb`, all of these integers. Let's get the index at this position, so the index is basically `xb[k, j]`. An example of that is 11 or 14, and so on. Now, in the forward pass, we took the row of `C` at `index` and we deposited it into `m` at position `k` of `j`. That's what happened. Now, we need to go backward, and we just need to route `dm` at the position `k` of `j`. We now have these derivatives for each position, and it's 10-dimensional. We just need to go into the correct row of `C`. `dc`, at index, is this, plus equals, because there could be multiple occurrences. The same row could have been used many, many times, and all of those derivatives will go backward through the indexing, and they will add. This is my candidate solution. Let's copy it here. Let's uncomment this, and cross our fingers. Hey, so that's it. We've backpropagated through this entire beast. There we go. Totally makes sense. Now, we come to exercise two. It basically turns out that in this first exercise, we were doing way too much work. We were backpropagating way too much, and it was all good practice, but it's not what you would do in practice. The reason for that is, for example, here, I separated out this loss calculation over multiple lines, and I broke it up all to its smallest atomic pieces, and we backpropagated through all of those individually. It turns out that if you just look at the mathematical expression for the loss, then you can do the differentiation on pen and paper, and a lot of terms cancel and simplify. The mathematical expression you end up with can be significantly shorter and easier to implement than backpropagating through all the little pieces of everything you've done. Previously, we had this complicated forward path going from logits to the loss, but in PyTorch, everything can just be glued together into a single call of that cross-entropy. You just pass in the logits and the labels, and you get the exact same loss, as I verify here. Our previous loss and the fast loss, coming from a chunk of operations as a single mathematical expression, are the same. But it's much faster in a forward pass. It's also much faster in a backward pass. The reason for that is if you just look at the mathematical form of this and differentiate again, you will end up with a very small and short expression. That's what we want to do here. We want, in a single operation or in a single go, or very quickly, go directly to `dlogits`. We need to implement the `dlogits` as a function of logits and `yb`s, but it will be significantly shorter than whatever we did here. To get to `dlogits`, we had to go all the way here. All of this work can be skipped with a much simpler mathematical expression that you can implement here. You can give it a shot yourself. Look at what exactly is the mathematical expression of loss and differentiate with respect to the logits. Let me show you a hint. You can try it fully yourself, but if not, I can give you some hint on how to get started mathematically. Basically, what's happening here is we have the logits, then there's a softmax that takes the logits and gives you probabilities. Then we are using the identity of the correct next character to pluck out a row of probabilities, take the negative log of it to get our negative log probability, and then we average up all the log probabilities or negative log probabilities to get our loss. Basically, what we have is for a single individual example, the loss is equal to the negative log probability, where `p` here is thought of as a vector of all the probabilities. At the `y` position, where `y` is the label, we have that `p` is the softmax function. The ith component of `p` of this probability vector is just the softmax function: raising all the logits to the power of e and normalizing so everything sums to one. If you write out `p` of `y` here, you can just write out the softmax, and what we're interested in is the derivative of the loss with respect to the `i` logit. It's `d / dli` of this expression here, where we have `l` indexed with the specific label `y`. On the bottom, we have a sum over `j` of `e` to the power of `lj`, the negative log of all that. Potentially, give it a shot on pen and paper and see if you can derive the expression for the loss divided by `dli`, and then we're going to implement it here. Okay, I'm going to give away the result here. This is some of the math I did to derive the gradients analytically. We see that I'm just applying the rules of calculus from your first or second year of bachelor's degree, if you took it. The expression actually simplifies quite a bit. You have to separate out the analysis in the case where the ith index that you're interested in inside logits is either equal to the label or not equal to the label. The expression simplifies and cancels in a slightly different way. What we end up with is something very, very simple. We either end up with `p[i]`, where `p` is again this vector of probabilities after a softmax, or `p[i] - 1`, where we simply subtract a one. In any case, we just need to calculate the softmax `p`, and then, in the correct dimension, we need to subtract one. That's the gradient that it takes analytically. Let's implement this. We have to keep in mind that this is only done for a single example, but here, we are working with batches of examples, so we have to be careful of that. The loss for a batch is the average loss over all the examples. The loss for all the individual examples summed up, and then divided by `n`, and we have to backpropagate through that, as well. `dlogits` is going to be of that softmax. PyTorch has a softmax function that you can call. We want to apply the softmax on the logits, and we want to go in the dimension that is one. We want to do the softmax along the rows of these logits. At the correct positions, we need to subtract a one. `dlogits`, iterating over all the rows and indexing into the columns provided by the correct labels inside `yb`, we need to subtract one. Finally, it's the average loss that is the loss. In the average, there's a one over `n` of all the losses added up. We also need to propagate through that division, so the gradient has to be scaled down by `n` as well because of the mean. Otherwise, this should be the result. Now, if we verify this, we see that we don't get an exact match, but the maximum difference from the logits from PyTorch and the `dlogits` here is on the order of `5e-9`. It's a tiny, tiny number. Because of floating-point weirdness, we don't get the exact bitwise result, but we basically get the correct answer approximately. I'd like to pause here briefly before we move on to the next exercise because I'd like us to get an intuitive sense of what the logits are. It has a beautiful and very simple explanation. Here, I'm taking the logits, and I'm visualizing it. We see that we have a batch of 32 examples of 27 characters. What are the logits intuitively? The logits are the probabilities in the forward pass, but here, these black squares are the positions of the correct indices where we subtracted a one. What is this doing? These are the derivatives on the logits. Let's look at just the first row. I'm plotting the probabilities of these logits, and then I'm taking just the first row, and this is the probability row. Then the logits of the first row are multiplied by `n`, just for us, so that we don't have the scaling by `n`. Everything is more interpretable. We see that it's exactly equal to the probability. The position of the correct index has a minus equals one, so minus one on that position. Notice that if you take `dlogits[0]` and sum it, it sums to zero. You should think of these gradients here at each cell as a force. We are going to be pulling down on the probabilities of the incorrect characters, and we're going to be pulling up on the probability at the correct index. That's basically what's happening in each row. The amount of push and pull is exactly equalized because the sum is zero. The amount to which we pull down in the probabilities, and the amount that we push up on the probability of the correct character, is equal. The repulsion and the attraction are equal. Think of the neural net now as a massive pulley system. We're up here on top of the logits, and we're pulling down the probabilities of incorrect characters and pulling up the probability of the correct characters. In this complicated pulley system, because everything is mathematically determined, think of it as tension translating to this complicated pulling mechanism. Eventually, we get a tug on the weights and the biases. In each update, we just tug in the direction that we like for each of these elements. The parameters are slowly giving in to the tug. That's what training in a neural net looks like on a high level. I think the forces of push and pull in these gradients are intuitive. We're pushing and pulling on the correct answer and the incorrect answers. The amount of force that we're applying is proportional to the probabilities that came out in the forward pass. For example, if our probabilities came out exactly correct, they would have had zero everywhere except for one at the correct position. Then, the logits would be all a row of zeros for that example. There would be no push and pull. The amount to which your prediction is incorrect is exactly the amount by which you're going to get a pull or a push in that dimension. If you have a very confidently mispredicted element here, then that element is going to be pulled down very heavily, and the correct answer is going to be pulled up to the same amount. The other characters are not going to be influenced too much. The amounts to which you mispredict are proportional to the strength of the pole, and that's happening independently in all the dimensions of this tensor. It's intuitive and easy to think through. That's the magic of the cross-entropy loss and what it's doing dynamically in the backward pass of the neural net. Now we get to exercise number three, which is a very fun exercise, depending on your definition of fun. We're going to do for batch normalization exactly what we did for cross-entropy loss in exercise two. We're going to consider it as a glued single mathematical expression and backpropagate through it in a very efficient manner. We're going to derive a much simpler formula for the backward path of batch normalization. We're going to do that using pen and paper. Previously, we broke up batch normalization into all the little intermediate pieces and all the atomic operations inside it. We backpropagated through it one by one. Now, we just have a single forward pass of batch norm, and it's all glued together. We see that we get the exact same result as before. For the backward pass, we'd like to implement a single formula for backpropagating through this entire operation, which is the batch normalization. In the forward pass, previously, we took `hpbn`, the hidden states of the pre-batch normalization, and created `h_preact`, which is the hidden states just before the activation. In the batch normalization paper, `hpbn` is X, and `h_preact` is Y. In the backward pass, we have `dh_preact`, and we'd like to produce `d_hpbn` in an efficient manner. That's the name of the game: calculate `dh_pbn` given `dh_preact`. For the purposes of this exercise, we're going to ignore gamma and beta and their derivatives because they take on a very simple form, similar to what we did above. Let's calculate this given that. To help you a little bit, like I did before, I started off the implementation on pen and paper. I took two sheets of paper to derive the mathematical formulas for the backward pass. To set up the problem, write out `mu`, `sigma_squared` variance, `x_i_hat`, and `y_i`, exactly as in the paper, except for the Bessel's correction.
