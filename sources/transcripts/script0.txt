Hello, my name is Andre, and I have been training deep neural networks for a bit more than a decade. In this lecture, I would like to show you what neural network training looks like under the hood. In particular, we are going to start with a blank Jupyter notebook, and by the end of this lecture, we will define and train a neural net. You will get to see everything that goes on under the hood and exactly how that works on an intuitive level. Specifically, I would like to take you through the building of Micrograd. Micrograd is a library that I released on GitHub about two years ago. At the time, I only uploaded the source code, and you would have to go in by yourself and figure out how it works. In this lecture, I will take you through it step by step and comment on all the pieces of it. What is Micrograd, and why is it interesting? Micrograd is basically an autograd engine. Autograd is short for automatic gradient, and really, it implements backpropagation. Backpropagation is an algorithm that allows you to efficiently evaluate the gradient of a loss function with respect to the weights of a neural network. What that allows us to do is iteratively tune the weights of that neural network to minimize the loss function and therefore improve the accuracy of the network. Backpropagation would be at the mathematical core of any modern deep neural network library, like PyTorch or Jax. The functionality of Micrograd is best illustrated by an example. If we just scroll down here, you will see that Micrograd basically allows you to build out mathematical expressions. Here, we have an expression where we are building two inputs, A and B. You'll see that A and B are -4 and 2, but we are wrapping those values into this value object that we are going to build out as part of Micrograd. This value object will wrap the numbers themselves, and then we are going to build out a mathematical expression where A and B are transformed into C, D, and eventually E, F, and G. I'm showing some of the functions, some of the functionality of Micrograd, and the operations that it supports. You can add two value objects, you can multiply them, you can raise them to a constant power, you can offset by one, negate, squash at zero, square, divide by a constant, divide by it, etc. We're building out an expression graph with these two inputs, A and B, and we're creating an output value of G. Micrograd will, in the background, build out this entire mathematical expression. It will, for example, know that C is also a value, C was the result of an addition operation, and the child nodes of C are A and B. The object will maintain pointers to the A and B value objects. We'll basically know exactly how all of this is laid out. Not only can we do what we call the forward pass, where we actually look at the value of G, which is straightforward, we will access that using the dot data attribute. The output of the forward pass, the value of G, is 24.7. The big deal is that we can also take this G value object and call it backward. This will initialize backpropagation at the node G. Backpropagation will start at G and go backward through that expression graph. It's going to recursively apply the chain rule from calculus. What that allows us to do is evaluate the derivative of G with respect to all the internal nodes, like E, D, and C, but also with respect to the inputs, A and B. Then, we can actually query this derivative of G with respect to A. That is a.grad, which in this case, happens to be 138. The derivative of G with respect to B, which also happens to be here, is 645. This derivative will be very important because it tells us how A and B are affecting G through this mathematical expression. In particular, a.grad is 138. If we slightly nudge A and make it slightly larger, 138 is telling us that G will grow, and the slope of that growth will be 138. The slope of growth of B will be 645. That tells us about how G will respond if A and B get tweaked a tiny amount in a positive direction. You might be confused about what this expression is that we built out here. This expression is completely meaningless. I just made it up. I am just flexing about the kinds of operations that are supported by Micrograd. What we actually really care about are neural networks. It turns out that neural networks are just mathematical expressions, just like this one, but actually slightly less crazy. Neural networks are just mathematical expressions. They take the input data and the weights of a neural network as inputs. The output is the predictions of your neural net or the loss function. We'll see this in a bit. Basically, neural networks just happen to be a certain class of mathematical expressions. Backpropagation is significantly more general. It doesn't actually care about neural networks at all. It only tells us about arbitrary mathematical expressions. We use that machinery for training neural networks. One more note I would like to make at this stage is that, as you see here, Micrograd is a scalar-valued autograd engine. It's working on the level of individual scalars, like -4 and 2. We are taking neural nets and breaking them down to these atoms of individual scalars and all the little pluses and times. It is just excessive. Obviously, you would never do this in production. It's really just put down for pedagogical reasons. It allows us not to deal with these n-dimensional tensors that you would use in a modern deep neural network library. This is really done so that you understand and refactor backpropagation and the chain rule and understand neural net training. If you actually want to train bigger networks, you have to use these tensors, but none of the math changes. This is done purely for efficiency. We are basically taking all the scalar values and packaging them up into tensors, which are just arrays of these scalars. Because we have these large arrays, we're making operations on those large arrays. That allows us to take advantage of the parallelism in a computer. All those operations can be done in parallel, and then the whole thing runs faster, but really, none of the math changes. That is done purely for efficiency. I don't think it is pedagogically useful to be dealing with tensors from scratch. That is why I fundamentally wrote Micrograd. You can understand how things work at the fundamental level, and then you can speed it up later. Here's the fun part. My claim is that Micrograd is what you need to train your networks, and everything else is just efficiency. You'd think that Micrograd would be a very complex piece of code, but that turns out not to be the case. If we just go to Micrograd, you will see that there are only two files here. This is the actual engine, and it doesn't know anything about neural nets. This is the entire neural nets library on top of Micrograd: engine and nn.pi. The actual backpropagation autograd engine that gives you the power of neural networks is literally 100 lines of very simple Python, which we'll understand by the end of this lecture. Then, nn.pi, this neural network library built on top of the autograd engine, is a joke. We have to define what a neuron is, what a layer of neurons is, and what a multi-layer perceptron is, which is just a sequence of layers of neurons. It is just a total joke. Basically, there is a lot of power that comes from only 150 lines of code, and that's all you need to understand to understand neural network training. Everything else is just efficiency. Of course, there is a lot to efficiency, but fundamentally that's all that is happening. Now, let's dive right in and implement Micrograd step by step. The first thing I would like to do is make sure that you have a very good, intuitive understanding of what a derivative is and exactly what information it gives you. Let's start with some basic imports that I copy-paste in every Jupyter notebook. Let's define a function, a scalar-valued function f of x, as follows. I will make this up randomly. I just want a scalar-valued function that takes a single scalar x and returns a single scalar y. We can call this function, passing in, say, 3.0, and get 20 back. We can also plot this function to get a sense of its shape. You can tell from the mathematical expression that this is probably a parabola; it's a quadratic. If we create a set of scalar values that we can feed in, using, for example, a range from -5 to 5 in steps of 0.25, so axis is from -5 to 5, not including 5, in steps of 0.25, we can call this function on this NumPy array as well. We get a set of y's if we call f on the axis. These y's are basically also applying a function on every one of these elements independently. We can plot this using matplotlib: plt.plot x's and y's, and we get a nice parabola. Previously, here, we fed in 3.0, somewhere here, and we received 20 back, which is here, the y-coordinate. Now, I would like to think through what the derivative of this function at any single input point x is. What is the derivative at different points x of this function? If you remember back to your calculus class, you have probably derived derivatives. You would take this mathematical expression, 3x squared - 4x + 5, and write it out on a piece of paper. You would apply the product rule and all the other rules and derive the mathematical expression of the derivative of the original function. Then, you could plug in different x's and see what the derivative is. We are not going to do that because no one in neural networks actually writes out the expression for the neural net. It would be a massive expression, tens of thousands of terms. No one actually derives the derivative. We are not going to take this kind of symbolic approach. Instead, I would like to look at the definition of derivative and make sure that we really understand what a derivative is measuring and what it is telling you about the function. If we look up derivative, we see, okay, this is not a very good definition of derivative. This is a definition of what it means to be differentiable. But, if you remember from your calculus, it is the limit as h goes to zero of f(x + h) - f(x) / h. Basically, what it is saying is, if you slightly bump up, you are at some point x that you're interested in, or a. If you slightly increase it by a small number h, how does the function respond? With what sensitivity does it respond? What is the slope at that point? Does the function go up or does it go down, and by how much? That is the slope of that function, the slope of that response at that point. We can evaluate the derivative here numerically by taking a very small h. Of course, the definition would ask us to take h to zero. We are just going to pick a very small h, 0.001. Let's say we are interested in the point 3.0. We can look at f of x, which is 20. Now, f of x + h, so if we slightly nudge x in a positive direction, how is the function going to respond? Just looking at this, do you expect f of x + h to be slightly greater than 20, or do you expect it to be slightly lower than 20? Since this 3 is here, and this is 20, if we slightly go positively, the function will respond positively. You would expect this to be slightly greater than 20. By how much is it telling you the strength of that slope, the size of the slope? So f of x + h - f(x) is how much the function responded in the positive direction. We have to normalize by the run, so we have the rise over the run to get the slope. This is a numerical approximation of the slope because we have to make h very, very small to converge to the exact amount. If I'm doing too many zeros, I will get an incorrect answer because we are using floating-point arithmetic. The representations of all these numbers in computer memory are finite, and at some point, we get into trouble. We can converse towards the right answer with this approach, but basically at 3, the slope is 14. You can see that by taking 3x squared - 4x + 5 and differentiating it in our head. 3x squared would be 6x - 4, and then we plug in x equals 3, so that's 18 - 4, which is 14. That is correct. That is at 3. How about the slope at, say, -3? Would you expect for the slope now? Telling the exact value is hard, but what is the sign of that slope? At -3, if we slightly go in the positive direction at x, the function would actually go down. That tells you that the slope would be negative. We'll get a slight number below 20. If we take the slope, we expect something negative, -22. At some point here, the slope would be zero. For this specific function, I looked it up previously, and it's at the point two over three, so at roughly two over three, somewhere here, this derivative will be zero. At that precise point, if we nudge in a positive direction, the function doesn't respond; it stays the same. That is why the slope is zero. Now, let's look at a more complex case. We are going to complexify a bit. Now we have a function with an output variable d that is a function of three scalar inputs: a, b, and c. a, b, and c are some specific values, three inputs into our expression graph, and a single output d. If we just print d, we get four. Now, what I have to do is again look at the derivatives of d with respect to a, b, and c, and think through the intuition of what this derivative is telling us. In order to evaluate this derivative, we're going to get a bit hacky here. We will again have a very small value of h. Then we will fix the inputs at some values that we are interested in. This is the point abc at which we are going to be evaluating the derivative of d with respect to a, b, and c at that point. Those are the inputs. Now, we have d1, which is that expression. Then, we are going to, for example, look at the derivative of d with respect to a. We'll take a and bump it by h, and then we will get d2 to be the exact same function. Now, we are going to print d1 is d1, d2 is d2, and print the slope. The derivative, or slope here, will be d2 - d1 / h. d2 - d1 is how much the function increased when we bumped the specific input that we are interested in by a tiny amount, and this is then normalized by h to get the slope. If I just run this, we will print d1, which we know is four. Now, d2 will be bumped. a will be bumped by h. Let's just think through a little bit what d2 will be printed out here. In particular, d1 will be four. Will d2 be a number slightly greater than four, or slightly lower than four? That will tell us the sign of the derivative. We are bumping a by h. b is -3. c is 10. You can intuitively think through this derivative and what it's doing. a will be slightly more positive, but b is a negative number. If a is slightly more positive, because b is -3, we are actually going to be adding less to d. You would actually expect that the value of the function will go down. Let's see this. We went from 4 to 3.9996, and that tells you that the slope will be negative. The exact number of the slope is -3. You can also convince yourself that -3 is the right answer mathematically and analytically because, if you have a * b + c and you have calculus, then differentiating a * b + c with respect to a gives you just b. Indeed, the value of b is -3, which is the derivative that we have. You can tell that that's correct. Now, if we do this with b, so if we bump b by a little bit in a positive direction, we would get different slopes. What is the influence of b on the output d? If we bump b by a tiny amount in a positive direction, then because a is positive, we'll be adding more to d. What is the sensitivity, the slope of that addition? It might not surprise you that this should be 2. Why is it 2? Because d of d by db, differentiating with respect to b, would give us a, and the value of a is 2. That is also working well. If c gets bumped a tiny amount by h, then a * b is unaffected. c becomes slightly bit higher. What does that do to the function? It makes it slightly bit higher because we are simply adding c, and it makes it slightly bit higher by the exact same amount that we added to c. That tells you that the slope is 1. That will be the rate at which d will increase as we scale c. We now have some intuitive sense of what this derivative is telling you about the function. We would like to move to neural networks. As I mentioned, neural networks will be pretty massive mathematical expressions. We need some data structures that maintain these expressions, and that is what we are going to start to build out now. We are going to build out this value object that I showed you in the readme page of Micrograd. Let me copy-paste a skeleton of the first very simple value object. The class `Value` takes a single scalar value that it wraps and keeps track of. That's it. We can, for example, do `Value(2.0)`, and then we can look at its content. Python will internally use the wrapper function to return this string. This is a value object with data equal to 2 that we are creating here. We would like to be able to have not just two values, but we would like to be able to add them. Currently, you would get an error because Python doesn't know how to add two value objects, so we have to tell it. Here is addition. You have to use these special double-underscore methods in Python to define these operators for these objects. If we call the plus operator, Python will internally call a dot add of b. That's what will happen internally, and b will be the other, and self will be a. We see that what we're going to return is a new value object, and it is just going to be wrapping the plus of their data. Remember, data is the actual numbered Python number, so this operator here is just the typical floating-point plus addition. It is not an addition of value objects, and will return a new value. Now, a plus b should work, and it should print the value of -1 because that is 2 + -3. There we go. Let's now implement multiply, so we can recreate this expression here. Multiply, I think, will be fairly similar. Instead of add, we are going to be using mul, and here, we want to do times. Now, we can create a c value object which will be 10.0. We should be able to do a * b. Let's just do a * b first. That is a value of -6. By the way, I skipped over this a little bit. Suppose that I didn't have the wrapper function here; then you would get some kind of an ugly expression. What the wrapper is doing is providing a way to print out a nicer-looking expression in Python. We don't just have something cryptic; we actually are, you know, it's a value of -6. This gives us times, and we should be able to add c to it because we've defined and told Python how to do mul and add. This will be equivalent to a.mul(b), and this new value object will be .add(c). Let's see if that worked. Yes, that worked well. That gave us four, which is what we expect from before. I believe we can call them manually as well. There we go. So, what we are missing is the connective tissue of this expression. As I mentioned, we want to keep these expression graphs. We need to know and keep pointers about what values produce what other values. Here, for example, we are going to introduce a new variable, which we will call children, and by default, it will be an empty tuple. Then we are actually going to keep a slightly different variable in the class, which we will call _prev, which will be the set of children. This is how I did it in the original Micrograd. Looking at my code here, I can't remember exactly the reason. I believe it was for efficiency, but this _children will be a tuple for convenience. When we maintain it in the class, it will be just this set. I believe for efficiency. When we are creating a value like this with a constructor, children will be empty, and prev will be the empty set, but when we are creating a value through addition or multiplication, we are going to feed in the children of this value, which, in this case, is self and other. Those are the children here. Now we can do d.prev, and we'll see that the children of the object are this value of -6 and the value of 10. This is the value resulting from a * b and the c value, which is 10. The last piece of information we don't know is we know the children of every single value, but we don't know what operation created this value. We need one more element here. Let's call it _op. By default, this is the empty set for leaves. Then we'll maintain it here. The operation will be a simple string. In the case of addition, it's +, and in the case of multiplication, it's *. Now, we not just have d.prev; we also have a d.op, and we know that d was produced by an addition of those two values. Now we have the full mathematical expression. We are building out this data structure. We know exactly how each value came to be by what expression and from what other values. Because these expressions are about to get quite a bit larger, we would like a way to nicely visualize these expressions that we are building out. For that, I'm going to copy-paste a bunch of slightly scary code that will visualize these expression graphs for us. Here is the code, and I'll explain it in a bit. First, let me show you what this code does. Basically, what it does is it creates a new function, `draw_dot`, that we can call on some root node, and then it is going to visualize it. If we call `draw_dot` on d, which is this final value here that is a * b + c, it creates something like this. This is d, and you see that this is a * b creating an intermediate value, and plus c gives us this output node d. That is `draw_dot` of d. I'm not going to go through this in complete detail. You can take a look at graphvis and its API. Graphvis is an open-source graph visualization software. What we're doing here is we are building out this graph using graphvis's API. You can see that `trace` is a helper function that enumerates all of the nodes and edges in the graph. That just builds a set of all the nodes and edges. Then, we iterate for all the nodes, and we create special node objects for them using `dot.node`. We also create edges using `dot.edge`. The only thing that's slightly tricky here is you'll notice that I add these fake nodes, which are these operation nodes. For example, this node here is just a plus node. I create these special op nodes here and connect them accordingly. These nodes are not actual nodes in the original graph. They are not actually a value object. The only value objects here are the things in squares. Those are actual value objects, or representations thereof. These op nodes are just created in this `draw_dot` routine so that it looks nice. Let's also add labels to these graphs so we know what variables are where. Let's create a special _label, or let's just do `label = ""` by default and save it in each node. Here, we are going to do `label = label` and `label = c`. Then let's create a special `e = a * b`, `e.label = e`, and `d = e + c`. `d.label = d`. Nothing really changes. I just added this new e function, a new e variable. Here, when we are printing this, I'm going to print the label here. This will be `%s` and `node.label`. We have the label on the left here. It says a and b are creating e, and then e + c creates d, just like we have it here. Finally, let's make this expression one layer deeper. d will not be the final output node. After d, we are going to create a new value object called f. We are going to start running out of variables soon. f will be -2.0, and its label will be f. Then `L`, capital L, will be the output of our graph, and `L = d * f`. L will be -8 is the output. Now we don't just draw d; we draw L. Somehow the label of L was undefined. The label has to be explicitly given to it. There we go. L is the output. Let's quickly recap what we have done so far. We are able to build out mathematical expressions using only plus and times so far. They are scalar-valued along the way. We can do this forward pass and build out a mathematical expression. We have multiple inputs here: a, b, c, and f going into a mathematical expression that produces a single output, L. This here is visualizing the forward pass. The output of the forward pass is -8. That is the value. What we would like to do next is run backpropagation. In backpropagation, we are going to start here at the end. We are going to reverse and calculate the gradient along all these intermediate values. What we are computing for every single value here is the derivative of that node with respect to L. The derivative of L with respect to L is 1. We are going to derive what is the derivative of L with respect to f, with respect to d, with respect to c, with respect to e, with respect to b, and with respect to a. In the neural network setting, you'd be very interested in the derivative of basically this loss function, L, with respect to the weights of a neural network. Here we just have these variables: a, b, c, and f, but some of these will eventually represent the weights of a neural net. We will need to know how those weights are impacting the loss function. We will be interested in the derivative of the output with respect to some of its leaf nodes. Those leaf nodes will be the weights of the neural net. The other leaf nodes, of course, will be the data itself, but usually we will not want or use the derivative of the loss function with respect to data because the data is fixed. The weights will be iterated on using the gradient information. Next, we are going to create a variable inside the `Value` class that maintains the derivative of L with respect to that value. We will call this variable `grad`. There is a data, and there is a self.grad, and initially, it will be zero. Remember that zero basically means no effect. At initialization, we are assuming that every value does not impact or affect the output because if the gradient is zero, that means that changing this variable is not changing the loss function. By default, we assume that the gradient is zero. Now that we have grad, and it's 0.0, we are going to be able to visualize it here after data. Here, grad is 0.4f, and this will be in the graph. Now, we are going to be showing both the data and the grad initialized at zero. We are just about getting ready to calculate backpropagation. This grad, as I mentioned, is representing the derivative of the output, in this case, L, with respect to this value. This is the derivative of L with respect to f, with respect to d, and so on. Let's now fill in those gradients and actually do backpropagation manually. Let's start filling in these gradients and start all the way at the end. As I mentioned, we are interested in filling in this gradient here. What is the derivative of L with respect to L? In other words, if I change L by a tiny amount of h, how much does L change? It changes by h, so it is proportional. Therefore, the derivative will be 1. We can measure these or estimate these numerical gradients numerically, just like we've seen before. If I take this expression and I create a `def lol` function here, putting this here, the reason I am creating a function here is that I don't want to pollute or mess up the global scope. This is a staging area, and in Python, all these will be local variables to this function, so I am not changing any of the global scope here. Here, `l1` will be `l`. Copying and pasting this expression, we will add a small amount, `h`, to `a`. This will measure the derivative of `l` with respect to `a`. So, here, this will be `l2`, and then we want to print this derivative. Print `l2 - l1`, which is how much `l` changed, and then normalize it by `h`. This is the rise over run. We have to be careful because `l` is a value node, so we actually want its data so these are floats dividing by `h`. This should print the derivative of `l` with respect to `a` because `a` is the one that we bumped a little bit by `h`. The derivative of `l` with respect to `a` is six. If we change `l` by `h`, the derivative here is one. That is the base case of what we are doing here. We cannot come up here and manually set `l.grad` to one; this is our manual backpropagation. `l.grad` is one. Let's redraw, and we'll see that we filled in `grad` as 1 for `l`. We are now going to continue the backpropagation. Let's look at the derivatives of `l` with respect to `d` and `f`. Let’s do `d` first. We have that `l` is `d` times `f`, and we'd like to know what is `dl/dd`. If you know your calculus, `l` is `d` times `f`, so `dl/dd` would be `f`. We can also derive it. We go to the definition of the derivative, which is `f(x+h) - f(x)` divided by `h`, as a limit where `h` goes to zero. When we have `l` is `d` times `f`, increasing `d` by `h` would give us `d+h` times `f`, minus `d` times `f`, and then divide by `h`. Symbolically expanding, we would have `d*f + h*f - d*f` divided by `h`. The `d*f` cancels out, so you’re left with `h*f` divided by `h`, which is `f`. In the limit as `h` goes to zero, we get `f`. Symmetrically, `dl/df` will be `d`. `f.grad` is now just the value of `d`, which is 4. `d.grad` is just the value of `f`, so the value of `f` is negative two. We'll set those manually. Let me erase this markdown node, and let's redraw what we have. We seem to think that `dl/dd` is negative two, so let's double-check. Let me erase the plus `h` from before. Now, we want the derivative with respect to `f`, so when I create `f`, let's do a plus `h` here. This should print the derivative of `l` with respect to `f`. We expect to see four, and this is four, up to floating-point funkiness. `dl/dd` should be `f`, which is negative two, so `grad` is negative two. If we come here and change `d.data += h`, we've added a little `h`, and then we see how `l` changed. We expect to print negative two; there we go. We've numerically verified what we're doing here. This is like an inline gradient check. A gradient check is when we are deriving this backpropagation and getting the derivative with respect to all the intermediate results. Numerical gradient is estimating it using a small step size. Now, we're getting to the crux of backpropagation, so this will be the most important node to understand because if you understand the gradient for this node, you understand all of backpropagation and all of training of neural nets. We need to derive `dl/dc`, in other words, the derivative of `l` with respect to `c` because we've computed all these other gradients already. Now, we're coming here, and we're continuing the backpropagation manually. We want `dl/dc` and then we'll also derive `dl/de`. How do we derive `dl/dc`? We actually know the derivative of `l` with respect to `d`. We know how `l` is affected by `d`, but how is `l` sensitive to `c`? If we wiggle `c`, how does that impact `l` through `d`? We know `dl/dc`, and we also know how `c` impacts `d`. Intuitively, if you know the impact that `c` is having on `d` and the impact that `d` is having on `l`, you should be able to put that information together to figure out how `c` impacts `l`. We can do this. In particular, let's look at how what is the derivative of `d` with respect to `c`, or `dd/dc`. Here, we know that `d` is `c` times `c+e`. We're interested in `dd/dc`. If you know your calculus, differentiating `c+e` with respect to `c` gives you 1.0. We can also go back to the basics and derive this. Again, we can go to `f(x+h) - f(x)` divided by `h` as `h` goes to zero. Focusing on `c` and its effect on `d`, we can do `c + h + e` minus `c + e` divided by `h`. Expanding this out, this will be `c+h+e-c-e` divided by `h`. `c` minus `c` cancels, `e` minus `e` cancels, and we are left with `h/h`, which is 1.0. By symmetry, `dd/de` will also be 1.0. The derivative of a sum expression is simple. This is the local derivative. I call this the local derivative because we have the final output value all the way at the end of this graph, and we're now at a small node here. This is a little plus node, and it doesn't know anything about the rest of the graph. It only knows that it took `c` and `e`, added them, and created `d`. This plus node also knows the local influence of `c` on `d`, or rather, the derivative of `d` with respect to `c`. It also knows the derivative of `d` with respect to `e`, but that's not what we want. That's just a local derivative. What we actually want is `dl/dc`. `l` is one step away, but in a general case, this little plus node could be embedded in a massive graph. We know how `l` impacts `d`, and now we know how `c` and `e` impact `d`. How do we put that information together to write `dl/dc`? The answer is the chain rule in calculus. I pulled up the chain rule from Wikipedia. If a variable `z` depends on a variable `y`, which itself depends on the variable `x`, then `z` depends on `x` through the intermediate variable `y`. The chain rule is expressed as: if you want `dz/dx`, then you take `dz/dy` and multiply it by `dy/dx`. The chain rule tells you how we chain these derivatives together correctly. To differentiate through a function composition, we have to apply a multiplication of those derivatives. The chain rule says that knowing the instantaneous rate of change of `z` with respect to `y` and `y` relative to `x` allows one to calculate the instantaneous rate of change of `z` relative to `x` as a product of those two rates of change. Simply, the product of those two. If a car travels twice as fast as a bicycle, and the bicycle is four times as fast as a walking man, then the car travels two times four, eight times as fast as the man. It is clear that we need to multiply. We can take these intermediate rates of change and multiply them. That justifies the chain rule intuitively. For us, it means there is a simple recipe for deriving what we want, which is `dl/dc`. We know the impact of `d` on `l`, so we know `dl/dd`. That's negative two. Because of this local reasoning, we know `dd/dc`. How does `c` impact `d`? In particular, this is a plus node, so the local derivative is 1.0. The chain rule tells us that `dl/dc`, going through this intermediate variable, will simply be `dl/dd` times `dd/dc`. This is the chain rule. It is identical to what's happening here, except `z` is our `l`, `y` is our `d`, and `x` is our `c`. We multiply these, and because these local derivatives, like `dd/dc` are just one, we copy over `dl/dd` because this is just times one. Because `dl/dd` is negative two, what is `dl/dc`? It's the local gradient 1.0 times `dl/dd`, which is negative two. A plus node routes the gradient because the plus node's local derivatives are one, so in the chain rule, one times `dl/dd` is just `dl/dd`, and that derivative gets routed to both `c` and `e`. `c.grad` is negative two times one, or negative two. By symmetry, `e.grad` will be negative two. Let me set those, and redraw. We assigned negative two. This backpropagating signal is carrying the information of the derivative of `l` with respect to all the intermediate nodes. We can imagine it flowing backward through the graph. A plus node will distribute the derivative to all the children nodes. We have that `c.grad` is negative two. Let’s verify it. Let me remove the plus `h` from before. Instead, we're going to increment `c`, so `c.data` will be incremented by `h`. When I run this, we expect to see negative two. For `e`, `e.data += h`, and we expect to see negative two. Those are the derivatives of these internal nodes. Now, we're going to recurse our way backwards again, and we're going to apply the chain rule. Our second application of the chain rule, we will apply it all the way through the graph. We have that `dl/de`, as we have just calculated, is negative two. We know the derivative of `l` with respect to `e`. Now, we want `dl/da`. The chain rule tells us that that's just `dl/de`, negative two, times the local gradient. What is the local gradient, basically, `de/da`? We have to look at that. I'm a little times node inside a massive graph. I only know that I did `a` times `b` and produced `e`. What is `de/da` and `de/db`? That's all I know. That's my local gradient. Because `e` is `a` times `b`, we're asking what is `de/da`. If you differentiate this with respect to `a`, you will get `b`, which in this case is negative three. `dl/da`, or `a.grad`, applying the chain rule, is `dl/de`, which is negative two, times `de/da`, which is the value of `b`, negative three. Then, `b.grad` is again `dl/de`, which is negative two, times `de/db`, which is the value of `a`, which is two. These are our claimed derivatives. Let's redraw, and we see that `a.grad` is six because that is negative two times negative three, and `b.grad` is negative two times two, which is negative four. Let's verify them. We have `a` here, so `a.data += h`. The claim is that `a.grad` is six. Let's verify. Six. We have `b.data += h`, nudging `b` by `h` and looking at what happens. We claim it's negative four. It's negative four. That was the manual backpropagation all the way from here to all the leaf nodes. We did it piece by piece, and really all we did was iterate through all the nodes one by one and locally applied the chain rule. We always know what is the derivative of `l` with respect to this little output. Then, we look at how this output was produced. This output was produced through some operation. We have the pointers to the children nodes of this operation. In this little operation, we know what the local derivatives are, and we just multiply them onto the derivative. We go through and recursively multiply on the local derivatives. That's what backpropagation is, a recursive application of the chain rule backward through the computation graph. Let's see this power in action. We're going to nudge our inputs to try to make `l` go up. We want `a.data` to change. If we want `l` to go up, that means we just have to go in the direction of the gradient. `a` should increase in the direction of the gradient by some small step amount, this is the step size. We don't just want this for `a` but also for `b`, also for `c`, and also for `f`. Those are leaf nodes, which we usually have control over. If we nudge in the direction of the gradient, we expect a positive influence on `l`. We expect `l` to go up. It should become less negative, should go up to, say, negative six. We’d have to rewrite the forward pass. Let me do that here. This would be the forward pass. `f` would be unchanged. This is effectively the forward pass. Now, if we print `l.data`, we expect, because we nudged all the inputs in the rational gradient, a less negative `l`. We expect it to go up, maybe it’s negative six or so. Okay, negative seven, and this is one step of an optimization that we'll end up running. Gradient gives us some power because we know how to influence the final outcome, and this will be extremely useful for training knowledge. I would like to do one more example of manual backpropagation using a more complex and useful example. We are going to backpropagate through a neuron. We want to eventually build up neural networks. In the simplest case, these are multilayer perceptrons. This is a two-layer neural net, and it's got these hidden layers made up of neurons. These neurons are fully connected to each other. Biologically, neurons are very complicated, but we have simple mathematical models of them. This is a very simple mathematical model of a neuron. You have some inputs `x`, and then you have these synapses that have weights on them. The `w`'s are weights. The synapse interacts with the input to this neuron multiplicatively. What flows to the cell body of this neuron is `w` times `x`. There are multiple inputs, so there are many `w` times `x`'s flowing into the cell body. The cell body also has some bias. This bias can make it more or less trigger-happy regardless of the input. We are taking all the `w` times `x`'s of all the inputs, adding the bias, and then we take it through an activation function. This activation function is usually some kind of a squashing function like a sigmoid or tanh. We are going to use tanh in this example. NumPy has `np.tanh`. We can call it on a range and plot it. This is the tanh function. The inputs as they come in get squashed on the y-coordinate here. Right at zero, we're going to get exactly zero. As you go more positive, the function will only go up to one and plateau out. If you pass in very positive inputs, we're going to cap it smoothly at one. On the negative side, we're going to cap it smoothly to negative one. That's tanh, and that's the squashing or activation function. What comes out of this neuron is just the activation function applied to the dot product of the weights and the inputs. Let's write one out. Here, we have the inputs `x1` and `x2`. This is a two-dimensional neuron, so two inputs are going to come in. These are the weights of this neuron, `w1` and `w2`. These weights are the synaptic strengths for each input. This is the bias of the neuron, `b`. According to this model, we need to multiply `x1` times `w1` and `x2` times `w2`. We need to add bias on top of it. It gets messy, but all we are trying to do is `x1*w1 + x2*w2 + b`. These are multiplications here. I'm doing it in small steps so we have pointers to all these intermediate nodes. We have `x1 * w1` and `x2 * w2`. I'm also labeling them. `n` is the cell body, the raw activation without the activation function. This should be enough to plot it. Draw dot of `n` gives us `x1` times `w1`, `x2` times `w2`, being added, then the bias gets added on top of this, and this `n` is this sum. We're now going to take it through an activation function. Let’s use tanh to produce the output. The output, `o`, is `n.tanh()`. We haven't written the tanh function. The reason we need to implement another tanh function here is that tanh is a hyperbolic function. We've only implemented a plus and times. You can't make a tanh out of pluses and times. You also need exponentiation. Tanh is this formula. You see there's exponentiation involved, which we have not implemented yet for our low value node. We're not going to be able to produce tanh yet. We have to go back up and implement something like it. One option is we could implement exponentiation and return the `x` of a value instead of a `tanh` of a value. If we had `x`, then we have everything else we need. We know how to add and we know how to multiply, so we’d be able to create tanh if we knew how to exponentiate. For the purposes of this example, I wanted to show you that we don't necessarily need to have the most atomic pieces in this value object. We can create functions at arbitrary points of abstraction. They can be complicated functions, but they can also be very simple functions like a plus. The only thing that matters is that we know how to differentiate through any one function. We take some inputs and we make an output. The only thing that matters is if you know how to create the local derivative. If you know the local derivative of how the inputs impact the output, then that's all you need. We're going to cluster up all of this expression, and we're not going to break it down to its atomic pieces. We are going to directly implement tanh. Let's do that. Define `tanh`. `out` will be a value of. We need this expression here. Let me copy-paste. Let's grab `n`, which is `cell.data`, and then this is `math.exp(2*n)` minus one over `math.exp(2*n)` plus one. I can call this `x`, so it matches exactly. Now, this will be `t`, and the children of this node there’s just one child, wrapped in a tuple. The name of this operation will be `tanh`, and we're going to return that. Now, the value should be implementing `tanh`. Now, we can scroll down here and do `n.tanh()`. This will return the tanhd output of `n`. Now, we should be able to draw `o` not `n`. `n` went through `tanh` to produce this output. `tanh` is now our little micrograd-supported node as an operation. As long as we know the derivative of `tanh`, we'll be able to backpropagate through it. Let’s see this `tanh` in action. Currently, it's not squashing too much because the input to it is pretty low. If the bias was increased to, say, eight, we'll see that what's flowing into the `tanh` now is two. The `tanh` is squashing it to 0.96. We are already hitting the tail of this `tanh`. It will smoothly go up to one and plateau out. I'm going to do something slightly strange. I'm going to change this bias from 8 to 6.88, etc. I'm going to do this for specific reasons, because we're about to start backpropagation. I want to make sure that our numbers come out nice. They are not crazy numbers, they're nice numbers that we can understand in our head. Let me add a pose label. `o` is short for output. So, 0.88 flows into `tanh` and comes out 0.7. Now, we're going to do backpropagation. We're going to fill in all the gradients. What is the derivative of `o` with respect to all the inputs here? In a typical neural network setting, what we really care about is the derivative of these neurons on the weights, specifically `w2` and `w1`, because those are the weights that we're going to be changing as part of the optimization. We have only a single neuron, but in neural nets, we have many neurons, and they are connected. This is one small neuron, a piece of a much bigger puzzle. Eventually, there's a loss function that measures the accuracy of the neural net. We are backpropagating with respect to that accuracy and trying to increase it. Let’s start off with backpropagation. What is the derivative of `o` with respect to `o`? The base case is that the gradient is 1.0. Let me fill it in. Let me split out the drawing function here and clear this output. Now, when we draw `o`, we'll see that `o.grad` is one. Now, we're going to backpropagate through the tanh. To backpropagate through tanh, we need to know the local derivative of tanh. If we have that `o` is `tanh(n)`, then what is `do/dn`? You could take this expression and take the derivative, and that would work. We can scroll down Wikipedia to a section that tells us that the derivative of tanh is 1 minus tanh squared. `do/dn` is 1 minus `tanh(n)^2`. We already have `tanh(n)`, that's just `o`. So, it's one minus `o^2`. `o` is the output here. The output is this number, the data is this number, and this is saying that `do/dn` is 1 minus this squared. One minus that data squared is 0.5 conveniently. The local derivative of this tanh operation here is 0.5, and that would be `do/dn`. We can fill in that `n.grad` is 0.5. This is exactly 0.5, one half. Now, we're going to continue the backpropagation. This is 0.5, and this is a plus node. How is backprop going to work here? A plus is just a distributor of gradient. This gradient will simply flow to both of these equally. The local derivative of this operation is one for every one of its nodes. 1 times 0.5 is 0.5. Therefore, this node's grad is 0.5, and `b.grad` is also 0.5. Let's set those and redraw. So, 0.5. Continuing, another plus, 0.5 again will just distribute it. 0.5 will flow to both of these. We can set `x2w2.grad` is 0.5. Pluses are my favorite operations to backpropagate through because it's simple. It is flowing into these expressions is 0.5. Keep in mind what the derivative is telling us at every point. This is saying that if we want the output of this neuron to increase, then the influence on these expressions is positive. Both are positive contributions to the output. Backpropagating to `x2` and `w2` first, this is a times node. The local derivative is the other term. To calculate `x2.grad`, `x2.grad` will be `w2.data` times the `x2w2.grad`, and `w2.grad` will be `x2.data` times `x2w2.grad`. Let's set them and redraw. Here, the gradient on `w2` is 0 because `x2.data` was 0. `x2` will have the gradient 0.5 because `w2.data` was 1. Because the input `x2` was 0, because of the way times works, this gradient will be zero. If I wiggle `w2`, how is the output changing? It's not changing because we're multiplying by zero. Because it's not changing, there's no derivative, and zero is the correct answer. Let's do it here, 0.5 should come here and flow through times. So, we'll have that `x1.grad` is going to be the local derivative of times with respect to `x1`, which is `w1`, so `w1.data` times `x1w1.grad`, and `w1.grad` will be `x1.data` times `x1w1.grad`. Those came out to be -1.5 and 1. We've backpropagated through this expression. These are the actual final derivatives. If we want this neuron's output to increase, we know that `w2` has no gradient, `w2` doesn't actually matter to this neuron, but this weight, `w1`, should go up. If this weight goes up, then this neuron's output would have gone up proportionally because the gradient is one. Doing the backpropagation manually is ridiculous, so we are going to put an end to this suffering. We're going to see how we can implement the backward pass a bit more automatically. We're not going to be doing all of it manually. It's now obvious how these pluses and times are backpropagating gradients, so let's go to the value object. We're going to start codifying what we've seen in the examples. We're going to store a special `cell._backward`, and this will be a function which is going to do that little piece of the chain rule at each node that took inputs and produced output. We're going to store how we are going to chain the outputs gradient into the inputs gradients. By default, this will be a function that doesn't do anything. For a leaf node, there is nothing to do. When we are creating these `out` values, these `out` values are an addition of self and other. We will set `out.backward` to be the function that propagates the gradient. Let's define what should happen when we call `out.grad` in addition. Our job is to take `out.grad` and propagate it into `self.grad` and `other.grad`. We want to set `self.grad` to something, and `other.grad` to something. The way we saw how chain rule works, we want to take the local derivative times the global derivative, which is the derivative of the final output of the expression with respect to out. The local derivative of self in an addition is 1.0, so it's just 1.0 times `out.grad`. `other.grad` will be 1.0 times `out.grad`. `out.grad` will simply be copied onto `self.grad` and `other.grad` as we saw happens for an addition operation. We're going to later call this function to propagate the gradient, having done an addition. Let’s do multiplication. We're going to also define the backward and we're going to set its backward to be `backward`. We want to chain `outgrad` into `self.grad` and `others.grad`. This will be a small piece of the chain rule for multiplication. So, what should this be? Think through it. What is the local derivative here? The local derivative was `others.data`, and then times the grad, that's chained. Here, we have `self.data` times the grad. That's what we've been doing. Finally, for the tanh backward, we want to set out backward to be just backward. Here, we need to back propagate. We have `out.grad` and want to chain it into `self.grad`. `Self.grad` will be the local derivative of this operation, which is tanh. We saw that the local gradient is 1 minus the tanh of x squared, which here is `t`. That's the local derivative because `t` is the output of this tanh. So, 1 minus `t` squared is the local derivative, and then the gradient has to be multiplied because of the chain rule. Thus, `outgrad` is chained through the local gradient into `self.grad`. That should be basically it. We are going to redefine our value node. We are going to swing all the way down here and redefine our expression, making sure that all the grads are zero. Now, we don't have to do this manually anymore. We are going to be calling the `backward` method in the right order. First, we want to call `o.backward`. `O` was the outcome of tanh, right? Calling `o.backward` will execute this function. This is what it will do. Now we have to be careful because there's a times `out.grad`. Remember `out.grad` is initialized to zero. Here we see `grad` zero. As a base case, we need to set `o.grad` to 1.0 to initialize it. Once this is 1, we can call `o.backward`. This should propagate the grad through tanh, so the local derivative times the global derivative which is initialized at one. This should work. I thought about redoing it, but I figured I should just leave the error in here because it's pretty funny. Why is an object not callable? It's because I screwed up. We're trying to save these functions. This is correct. Here, we don't want to call the function because that returns `None`. These functions return `None`. We just want to store the function. Let me redefine the value object. Then we're going to come back, redefine the expression, and redraw. Everything is great. `o.grad` is one. `o.grad` is one. Now this should work. Okay, so `o.backward` should have this gradient as 0.5. If we redraw, and if everything went correctly, 0.5, yay! So, now we need to call `n.backward`. That seems to have worked. So, `n.backward` routed the gradient to both of these. This is looking great. Now, we could, of course, call `b.backward`. What's going to happen? Well, `b` doesn't have a backward. `B's` backward is by initialization the empty function, so nothing would happen. We can call it on it, but when we call this one's backward, then we expect this 0.5 to get further routed, right? So, there we go, 0.5, 0.5. Finally, we want to call it here on `x2`, `w2`, and on `x1`, `w1`. Do both of those, and there we go. We get 0, 0.5, -1.5, and 1, exactly as we did before, but now we've done it through calling that backward sort of manually. We have one last piece to get rid of, which is us calling `_backward` manually. Let's think through what we are actually doing. We've laid out a mathematical expression, and now we're trying to go backwards through that expression. Going backwards through the expression just means that we never want to call a `backward` for any node before we've done everything after it. So, we have to do everything after it before we're ever going to call `backward` on any node. We have to get all of its full dependencies. Everything that it depends on has to propagate to it before we can continue back propagation. This ordering of graphs can be achieved using something called topological sort. Topological sort is basically laying out a graph such that all the edges go only from left to right. Here we have a graph. It's a directed acyclic graph (DAG). This shows two different topological orders of it, where basically you'll see that it's laying out the nodes such that all the edges go only one way from left to right. Implementing topological sort, you can look in Wikipedia. I'm not going to go through it in detail. Basically, this is what builds a topological graph. We maintain a set of visited nodes, and then we are going through, starting at some root node, which for us is `o`. That's where we want to start the topological sort. Starting at `o`, we go through all of its children, and we need to lay them out from left to right. Basically, this starts at `o`. If it's not visited, then it marks it as visited. Then, it iterates through all of its children and calls `build_topological` on them. After it's gone through all the children, it adds itself. This node that we're going to call it on, like say `o`, is only going to add itself to the `topo_list` after all of the children have been processed. That's how this function is guaranteeing that you're only going to be in the list once all your children are in the list. That's the invariant that is being maintained. If we build upon `o` and then inspect this list, we're going to see that it ordered our value objects. The last one is the value of 0.707, which is the output, so this is `o`. Then, this is `n`, and then all the other nodes get laid out before it. That builds the topological graph. Really, what we're doing now is we're just calling `_backward` on all of the nodes in a topological order. If we just reset the gradients, they're all zero. What did we do? We started by setting `o.grad` to be 1. That's the base case. Then, we built the topological order, and then we went for `node` in the reversed `topo`. Now, in the reverse order because this list goes from you know, we need to go through it in reverse order, so starting at `o`, `node.backward`, and this should be it. There we go. Those are the correct derivatives. Finally, we are going to hide this functionality. I'm going to copy this, and we're going to hide it inside the `Value` class because we don't want to have all that code lying around. Instead of an `_backward`, we're now going to define an actual `backward`, so that's `backward` without the underscore. That's going to do all the stuff that we just arrived at. Let me just clean this up a little bit. So, we're first going to build a topological graph starting at `self`. `Build_topo(self)` will populate the topological order into the `topo_list`, which is a local variable. Then we set `self.grad` to be 1. Then, for each node in the reversed list, so starting at us and going to all the children, we call `_backward`. That should be it. Save. Come down here. Redefine. Okay, all the grads are zero. Now, what we can do is `o.backward`, without the underscore, and there we go. That's back propagation in place for one neuron. Now, we shouldn't be too happy with ourselves actually, because we have a bad bug, and we have not surfaced the bug because of some specific conditions that we are in. We have to think about it right now. Here's the simplest case that shows the bug. Say I create a single node `a`, and then I create a `b` that is `a` plus `a`, and then I call backward. What's going to happen is `a` is 3, and then `b` is `a` plus `a`, so there are two arrows on top of each other here. We can see that `b` is, of course, the forward pass works. `b` is just `a` plus `a`, which is 6. But, the gradient here is not actually correct that we calculate it automatically, and that's because just doing calculus in your head, the derivative of `b` with respect to `a` should be two, 1 plus 1. It's not one. Intuitively, what's happening here? `B` is the result of `a` plus `a`, and then we call `backward` on it. Let's go up and see what that does. `B` is a result of addition. So, out is `b`, and then when we called `backward`, what happened is `self.grad` was set to 1, and then `other.grad` was set to 1. But, because we're doing `a` plus `a`, self and other are actually the exact same object. We are overriding the gradient. We are setting it to 1, and then we are setting it again to 1. That's why it stays at 1. That's a problem. There's another way to see this in a little bit more complicated expression. Here we have `a` and `b`, and then `d` will be the multiplication of the two, and `e` will be the addition of the two, and then we multiply `e` times `d` to get `f`. Then, we called `f.backward`, and these gradients, if you check, will be incorrect. Fundamentally what's happening here again is basically we're going to see an issue anytime we use a variable more than once. Until now, in these expressions above, every variable is used exactly once, so we didn't see the issue. But, here, if a variable is used more than once, what's going to happen during the backward pass? We're backpropagating from `f` to `e` to `d`, so far so good. But now, `e.backward` and it deposits its gradients to `a` and `b`. Then, we come back to `d` and call `backward`, and it overwrites those gradients at `a` and `b`. That's obviously a problem. The solution here, if you look at the multivariate case of the chain rule and its generalization, the solution is basically that we have to accumulate these gradients. These gradients add, and so instead of setting those gradients, we can simply do `+=`. We need to accumulate those gradients: `+=`, `+=`, `+=`, `+=`. This will be okay, remember, because we are initializing them at zero, so they start at zero, and then any contribution that flows backwards will simply add. Now, if we redefine this one because of the `+=`, this now works because `a.grad` started at zero, and we called `b.backward`. We deposit one, and then we deposit one again, and now this is two, which is correct. Here, this will also work, and we'll get correct gradients. Because when we call `e.backward`, we will deposit the gradients from this branch, and then we get back into `d.backward`. It will deposit its own gradients, and those gradients simply add on top of each other, so we just accumulate those gradients and that fixes the issue. Okay, now before we move on, let me actually do a bit of cleanup here and delete some of this intermediate work, so we're not going to need any of this now that we've derived all of it. We are going to keep this because I want to come back to it. Delete the tanh, delete our morning example, delete the step, delete this, keep the code that draws, and then delete this example. Leave behind only the definition of value. Now, let's come back to this non-linearity here that we implemented, the tanh. Now, I told you that we could have broken down tanh into its explicit atoms in terms of other expressions if we had the `exp` function. If you remember, tanh is defined like this. We chose to develop tanh as a single function, and we can do that because we know its derivative and can back propagate through it. But we can also break down tanh and express it as a function of `exp`, and I would like to do that now because I want to prove to you that you get all the same results and all those gradients, but also because it forces us to implement a few more expressions. It forces us to do exponentiation, addition, subtraction, division, and things like that, and I think it's a good exercise to go through a few more of these. Okay, let's scroll up to the definition of value, and here one thing that we currently can't do is we can do like a value of say 2.0, but we can't do, you know, here for example, we want to add constant one, and we can't do something like this. We can't do it because it says object has no attribute data. That's because `a + 1` comes right here to `add`, and then `other` is the integer one. Then, here python is trying to access `1.data`, and that's not a thing. That's because basically one is not a value object, and we only have addition for value objects. As a matter of convenience, so that we can create expressions like this and make them make sense, we can simply do something like this. Basically, we let `other` alone if `other` is an instance of `Value`. If it's not an instance of `Value`, we're going to assume that it's a number like an integer or float. We're going to simply wrap it in `Value`, and then `other` will just become `Value(other)`. Then, `other` will have a `data` attribute, and this should work. If I just say this, redefine value, then this should work. There we go. Okay, now let's do the exact same thing for multiply because we can't do something like this again for the exact same reason, so we just have to go to `mul`. If `other` is not a value, then let's wrap it in `Value`. Let's redefine value, and now this works. Now, here's a kind of unfortunate and not obvious part. `a * 2` works. We saw that, but `2 * a`, is that going to work? You'd expect it to, right? But actually, it will not. The reason it won't is because python doesn't know. When you do `a * 2`, python will go and it will basically do something like `a.mul(2)`. That's basically what it will call. But, `2 * a` is the same as `2.mul(a)`, and 2 can't multiply a value, and so it's really confused about that. Instead, what happens is in python, the way this works is you are free to define something called the `rmul`. The `rmul` is kind of like a fallback. If python can't do `2 * a`, it will check if, by any chance, `a` knows how to multiply `2`, and that will be called into `rmul`. Because python can't do `2 * a`, it will check, is there an `rmul` in `Value`, and because there is, it will now call that. What we'll do here is we will swap the order of the operands. Basically, `2 * a` will redirect to `rmul`, and `rmul` will basically call `a * 2`, and that's how that will work. Redefining now with `rmul`, `2 * a` becomes 4. Okay, now looking at the other elements that we still need, we need to know how to exponentiate and how to divide. Let's first look at the exponentiation part. We're going to introduce a single function `exp` here, and `exp` is going to mirror tanh in the sense that it's a simple single function that transforms a single scalar value and outputs a single scalar value. We pop out the python number, we use `math.exp` to exponentiate it, create a new value object, everything that we've seen before. The tricky part, of course, is how do you propagate through `e` to the `x`, and so here you can potentially pause the video and think about what should go here. Basically, we need to know what is the local derivative of `e` to the `x`. The derivative with respect to x of `e` to the `x` is famously just `e` to the `x`. We've already just calculated `e` to the `x`, and it's inside `out.data`. We can do `out.data` times `out.grad`. That's the chain rule, so we're just chaining onto the current running grad, and this is what the expression looks like. It looks a little confusing, but this is what it is. That's the exponentiation. Redefining, we should now be able to call `a.exp`, and hopefully the backward pass works as well. Okay, and the last thing we'd like to do of course is we'd like to be able to divide. Now, I actually will implement something slightly more powerful than division because division is just a special case of something a bit more powerful. In particular, just by rearranging, if we have some kind of a `b = Value(4.0)` here, we'd like to basically be able to do `a / b`, and we'd like this to be able to give us 0.5. Division actually can be reshuffled as follows. If we have `a / b`, that's actually the same as `a * (1 / b)`. That's the same as `a` multiplying `b` to the power of negative one, and so what I'd like to do instead is I basically like to implement the operation of `x` to the `k` for some constant `k`. It's an integer or a float, and we would like to be able to differentiate this. Then, as a special case, negative one will be division. I'm doing that just because it's more general, and you might as well do it that way. Basically, what I'm saying is we can redefine division. We will put it here somewhere. Yeah, we can put it here somewhere. What I'm saying is that we can redefine division, so `self / other` can actually be rewritten as `self * (other ** -1)`. Now, a value raised to the power of negative one, we have now defined that. So, here's, so we need to implement the `pow` function. Where am I going to put the power function? Maybe here somewhere. This is the skeleton for it. This function will be called when we try to raise a value to some power, and `other` will be that power. Now, I'd like to make sure that other is only an `int` or a `float`. Usually, `other` is some kind of a different value object, but here, `other` will be forced to be an `int` or a `float`. Otherwise, the math won't work for or try to achieve in the specific case that would be a different derivative expression if we wanted other to be a value. So here, we create the output value which is just, you know, this `data` raised to the power of `other`. `Other` here could be, for example, negative one, that's what we are hoping to achieve. Then, this is the backwards stub, and this is the fun part, which is what is the chain rule expression here for back propagating through the power function where the power is to the power of some kind of a constant. This is the exercise. Maybe pause the video here and see if you can figure it out yourself as to what we should put here. You can actually go here and look at derivative rules as an example, and we see lots of derivatives that you can hopefully know from calculus. In particular, what we're looking for is the power rule because that's telling us that if we're trying to take the derivative of `x` to the `n`, which is what we're doing here, then that is just `n` times `x` to the `n` minus 1, right? Okay, so that's telling us about the local derivative of this power operation. All we want here, basically, `n` is now other, and `self.data` is `x`. This now becomes `other`, which is `n`, times `self.data`, which is now a python `int` or a float. It's not a value object. We're accessing the `data` attribute, raised to the power of `other` minus one, or `n` minus one. I can put brackets around this, but this doesn't matter because power takes precedence over multiply in python. That would have been okay. That's the local derivative only. Now, we have to chain it, and we chain just simply by multiplying by `out.grad`, that's the chain rule, and this should technically work, and we're going to find out soon. But now, if we do this, this should now work, and we get 0.5, so the forward pass works, but does the backward pass work? I realized that we actually also have to know how to subtract. Right now, `a - b` will not work. To make it work, we need one more piece of code here. Basically, this is the subtraction. The way we're going to implement subtraction is we're going to implement it by addition of a negation. To implement negation, we're going to multiply by negative one. Again, just using the stuff we've already built, expressing it in terms of what we have. `a - b` is now working. Okay, so now let's scroll again to this expression here for this neuron. Let's just compute the backward pass here once we've defined `o`, and let's draw it. Here are the gradients for all these leaf nodes for this two-dimensional neuron that has a tanh that we've seen before. Now, what I'd like to do is I'd like to break up this tanh into this expression here, so let me copy paste this here. Now, instead of, we'll preserve the label, and we will change how we define `o`. In particular, we're going to implement this formula here, so we need `e` to the `2x` minus 1 over `e` to the `x` plus 1. So, `e` to the `2x`, we need to take 2 times `n`, and we need to exponentiate it. That's `e` to the `2x`. Then, because we're using it twice, let's create an intermediate variable, `e`. Define `o` as `(e - 1) / (e + 1)`. That should be it. Then, we should be able to draw that of `o`. Before I run this, what do we expect to see? Number one, we're expecting to see a much longer graph here because we've broken up tanh into a bunch of other operations. Those operations are mathematically equivalent, and so what we're expecting to see is, number one, the same result here, so the forward pass works. Number two, because of that mathematical equivalence, we expect to see the same backward pass, the same gradients on these leaf nodes. These gradients should be identical. Let's run this. Number one, let's verify that instead of a single tanh node, we have now exp, we have plus, we have times negative one. This is the division. We end up with the same forward pass here. Then, the gradients, we have to be careful because they're in slightly different order. Potentially, the gradients for `w2x2` should be 0 and 0.5. `w2` and `x2` are 0 and 0.5. `w1` and `x1` are 1 and -1.5. 1 and -1.5. That means that both our forward passes and backward passes were correct because this turned out to be equivalent to tanh before. The reason I wanted to go through this exercise is number one, we got to practice a few more operations and writing more backward passes. Number two, I wanted to illustrate the point that the level at which you implement your operations is totally up to you. You can implement backward passes for tiny expressions like a single individual plus, or a single times, or you can implement them for say tanh, which is kind of a potentially, you can see it as a composite operation because it's made up of all these more atomic operations. Really, all of this is kind of like a fake concept. All that matters is we have some kind of inputs, some kind of output, and this output is a function of the inputs in some way. As long as you can do forward pass and the backward pass of that little operation, it doesn't matter what that operation is and how composite it is. If you can write the local gradients, you can chain the gradient, and you can continue back propagation. The design of what those functions are is completely up to you. Now, I would like to show you how you can do the exact same thing by using a modern deep neural network library like, for example, PyTorch, which I've roughly modeled micrograd by. PyTorch is something you would use in production. I'll show you how you can do the exact same thing but in PyTorch API. I'm just going to copy paste it in and walk you through it a little bit. This is what it looks like. We're going to import PyTorch. Then, we need to define these value objects like we have here. Micrograd is a scalar valued engine, so we only have scalar values like 2.0. But, in PyTorch, everything is based around tensors. Like I mentioned, tensors are just n-dimensional arrays of scalars. That's why things get a little bit more complicated here. I just need a scalar value to a tensor, a tensor with just a single element. By default, when you work with PyTorch, you would use more complicated tensors like this. If I import PyTorch, I can create tensors like this. This tensor, for example, is a two by three array of scalar scalars in a single compact representation. We can check its shape. We see that it's a two by three array, and so on. This is usually what you would work with in the actual libraries. Here, I'm creating a tensor that has only a single element, 2.0. Then, I'm casting it to be double because python is by default using double precision for its floating-point numbers. I'd like everything to be identical. By default, the data type of these tensors will be float32, so it's only using a single precision float. I'm casting it to double so that we have float64, just like in python. I'm casting to double, and then we get something similar to `Value(2)`. The next thing I have to do is because these are leaf nodes, by default PyTorch assumes that they do not require gradients. I need to explicitly say that all of these nodes require gradients. This is going to construct scalar valued one element tensors, making sure that PyTorch knows that they require gradients. By default, these are set to false, by the way, because of efficiency reasons. Usually, you would not want gradients for leaf nodes like the inputs to the network. This is just trying to be efficient in the most common cases. Once we've defined all of our values in python, we can perform arithmetic just like we can here in microgradland, so this will just work. There's a `torch.tanh` also. When we get back is a tensor again. We can just like in micrograd, it's got a data attribute and it's got a grad attribute. These tensor objects just like in micrograd have a data and a grad. The only difference here is that we need to call that item because otherwise, PyTorch that item basically takes a single tensor of one element, and it just returns that element, stripping out the tensor. Let me just run this, and hopefully we are going to get, this is going to print the forward pass, which is 0.707, and this will be the gradients, which hopefully are 0, 0.5, -1.5 and 1. If we just run this, there we go, 0.7. So, the forward pass agrees. Then, 0.5, 0, -1.5 and 1, so PyTorch agrees with us. Just to show you here, basically `o` here's a tensor with a single element, and it's a double. We can call that `item` on it to just get the single number out. That's what item does. `O` is a tensor object like I mentioned, and it's got a `backward` function just like we've implemented, and then all of these also have a `grad`. Like, `x2` for example, in the grad, it's a tensor, and we can pop out the individual number with that item. Basically, Torch can do what we did in micrograd. It's a special case when your tensors are all single element tensors. The big deal with PyTorch is that everything is significantly more efficient because we are working with these tensor objects, and we can do lots of operations in parallel on all of these tensors. Otherwise, what we've built very much agrees with the API of PyTorch. Now that we have some machinery to build out pretty complicated mathematical expressions, we can also start building out neural nets, and as I mentioned, neural nets are just a specific class of mathematical expressions. We're going to start building out a neural net piece by piece, and eventually, we'll build out a two-layer multi-layer layer perceptron, as it's called. I'll show you exactly what that means. Let's start with a single individual neuron. We've implemented one here, but here I'm going to implement one that also subscribes to the PyTorch API in how it designs its neural network modules. Just like we saw that we can match the API of PyTorch on the auto grad side, we're going to try to do that on the neural network modules. Here's the class `Neuron`. Just for the sake of efficiency, I'm going to copy paste some sections that are relatively straightforward. The constructor will take the number of inputs to this neuron, which is how many inputs come to a neuron. This one, for example, has three inputs. Then, it's going to create a weight, there is some random number between -1 and 1, for every one of those inputs, and a bias that controls the overall trigger happiness of this neuron. Then we're going to implement a `def __call__` of `self` and `x`, some input `x`. Really what we don't do here is `w * x + b`, where `w * x` here is a dot product specifically. If you haven't seen `call`, let me just return `0.0` here for now. The way this works now is we can have an `x` which is say like `2.0, 3.0`. Then, we can initialize a neuron that is two-dimensional because these are two numbers. Then, we can feed those two numbers into that neuron to get an output, and so when you use this notation `n(x)`, python will use `call`. Currently, `call` just returns `0.0`. Now, we'd like to actually do the forward pass of this neuron instead. We're going to do here first is we need to basically multiply all of the elements of `w` with all of the elements of `x` pairwise. We need to multiply them. The first thing we're going to do is we're going to zip up `self.w` and `x`. In python, zip takes two iterators, and it creates a new iterator that iterates over the tuples of the corresponding entries. For example, just to show you, we can print this list and still return 0.0 here. So, we see that these `w`s are paired up with the `x`s. `w` with `x`. Now what we want to do is for `wi`, `xi` in, we want to multiply `wi` times `xi`. Then, we want to sum all of that together to come up with an activation, and add also `self.b` on top. That's the raw activation. Then, of course, we need to pass that through a non-linearity, so what we're going to be returning is `act.tanh`. Here's out. Now, we see that we are getting some outputs, and we get a different output from a neuron each time because we are initializing different weights and biases. To be a bit more efficient here, actually, `sum` by the way takes a second optional parameter which is the `start`. By default, the start is zero, so these elements of this sum will be added on top of zero to begin with. We can just start with `self.b`. Then we just have an expression like this. The generator expression here must be parenthesized in python. There we go. Yep. So, now we can forward a single neuron. Next up, we are going to define a layer of neurons. Here we have a schematic for a multilayer perceptron (MLP). We see that each layer has a number of neurons. These neurons are not connected to each other, but all of them are fully connected to the input. What is a layer of neurons? It is just a set of neurons evaluated independently. In the interest of time, I am going to do something fairly straightforward. Literally, a layer is just a list of neurons. We take the number of neurons as an input argument. How many neurons do you want in your layer? This is the number of outputs in this layer. We just initialize completely independent neurons with this given dimensionality. When we call on it, we just independently evaluate them. Now, instead of a neuron, we can make a layer of neurons. They are two-dimensional neurons. Let's have three of them. We now see that we have three independent evaluations of three different neurons. Finally, let's complete this picture and define an entire multi-layer perceptron or MLP. As we can see here, in an MLP, these layers just feed into each other sequentially. I am going to copy the code here in the interest of time. An MLP is very similar. We are taking the number of inputs as before. Now, instead of taking a single n_out, which is the number of neurons in a single layer, we are going to take a list of n_outs. This list defines the sizes of all the layers that we want in our MLP. Here, we just put them all together and iterate over consecutive pairs of these sizes and create layer objects for them. In the call function, we are just calling them sequentially. That is an MLP. Let's actually re-implement this picture. We want three input neurons and then two layers of four and an output unit. We want a three-dimensional input. Say this is an example input. We want three inputs into two layers of four and one output. This is an MLP. There we go. That is a forward pass of an MLP. To make this a little bit nicer, you see how we have just a single element, but it is wrapped in a list. This is because the layer always returns lists. For convenience, we return outs at zero if the length of out is exactly a single element. Otherwise, return the full list. This will allow us to just get a single value out at the last layer that only has a single neuron. Finally, we should be able to draw the dot of n of x. As you might imagine, these expressions are now getting relatively involved. This is an entire MLP that we are defining now all the way until a single output. Obviously, you would never differentiate on pen and paper these expressions, but with micrograd, we will be able to back propagate all the way through this and back propagate into the weights of all these neurons. Let's see how that works. Let's create ourselves a very simple example dataset here. This dataset has four examples. We have four possible inputs into the neural net, and we have four desired targets. We would like the neural net to assign or output 1.0 when it is fed this example, -1 when it is fed these examples, and 1 when it is fed this example. It is a very simple binary classifier neural net that we would like here. Now let's think what the neural net currently thinks about these four examples. We can just get their predictions. Basically, we can just call n of x for x in axes and then we can print. These are the outputs of the neural net on those four examples. The first one is 0.91, but we would like it to be one, so we should push this one higher. This one we want to be higher. This one says 0.88, and we want this to be -1. This is 0.8, and we want it to be -1. This one is 0.8, and we want it to be one. How do we make the neural net and how do we tune the weights to better predict the desired targets? The trick used in deep learning to achieve this is to calculate a single number that somehow measures the total performance of your neural net. We call this single number the loss. The loss is a single number that we are going to define. It basically measures how well the neural net is performing. Right now, we have the intuitive sense that it is not performing very well because we are not very close to this. The loss will be high, and we will want to minimize the loss. In particular, in this case, we are going to implement the mean squared error loss. This is doing is we are going to iterate for y ground truth and y output in the zip of y's and y_pred. We are going to pair up the ground truths with the predictions. This zip iterates over tuples of them. For each y ground truth and y output, we are going to subtract them and square them. Let's first see what these losses are. These are individual loss components. For each one of the four, we are taking the prediction and the ground truth. We are subtracting them and squaring them. Because this one is so close to its target, 0.91 is almost one. Subtracting them gives a very small number. Here, we would get like a -0.1, and then squaring it just makes sure that, regardless of whether we are more negative or more positive, we always get a positive number. Instead of squaring, we could also take, for example, the absolute value. We need to discard the sign. You see that the expression is arranged so that you only get zero exactly when y out is equal to y ground truth. When those two are equal, so your prediction is exactly the target, you are going to get zero. If your prediction is not the target, you are going to get some other number. Here, for example, we are way off, and that is why the loss is quite high. The more off we are, the greater the loss will be. We do not want a high loss. We want a low loss. The final loss here will be just the sum of all of these numbers. This should be zero roughly, plus zero roughly, plus seven, so the loss should be about seven here. We want to minimize the loss. We want the loss to be low. If the loss is low, then every one of the predictions is equal to its target. The lowest it can be is zero, and the greater it is, the worse off the neural net is predicting. Now, if we do loss dot backward, something magical happened when I hit enter. The magical thing that happened is that we can look at n.layers, then the neuron, and then layers at say the first layer, the neurons at zero. Remember that MLP has the layers which is a list, and each layer has neurons which is a list. That gives us an individual neuron. It has some weights. We can, for example, look at the weights at zero. Oops, it is not called weights, it is called w, and that is a value. Now, this value also has a grad because of the backward pass. Because this gradient here on this particular weight of this particular neuron of this particular layer is negative, we see that its influence on the loss is also negative. Slightly increasing this particular weight of this neuron of this layer would make the loss go down. We actually have this information for every single one of our neurons and all their parameters. It is worth looking at also the draw dot loss, by the way. Previously, we looked at the draw dot of a single neural neuron forward pass, and that was already a large expression. What is this expression? We actually forwarded every one of those four examples, and then we have the loss on top of them with the mean squared error. This is a massive graph because this graph that we have built up, it is excessive because it has four forward passes of a neural net for every one of the examples, and then it has the loss on top. It ends with the value of the loss, which was 7.12. This loss will now back propagate through all the four forward passes all the way through every single intermediate value of the neural net all the way back to the parameters of the weights, which are the input. These weight parameters here are inputs to this neural net, and these numbers here, these scalars, are inputs to the neural net. If we went around here, we would probably find some of these examples. This 1.0 potentially, maybe this 1.0, or some of the others. You will see that they all have gradients as well. These gradients on the input data are not that useful to us. The input data seems to be not changeable. It is a given to the problem. It is a fixed input. We are not going to be changing it or messing with it, even though we do have gradients for it. Some of these gradients here will be for the neural network parameters, the w’s and the b’s. We, of course, want to change those. Now, we are going to want some convenience code to gather up all of the parameters of the neural net so that we can operate on all of them simultaneously. Every one of them we will nudge a tiny amount based on the gradient information. Let's collect the parameters of the neural net all in one array. Let's create the parameters of self that just returns self.w, which is a list, concatenated with a list of self.b. This will just return a list. A list plus list just gives you a list. That is parameters of a neuron. I am calling it this way because also PyTorch has a parameters on every single nn.module, and it does exactly what we are doing here. It just returns the parameter tensors for us as the parameter scalars. A layer is also a module, so it will have parameters itself. Basically, what we want to do here is something like this. Like params is here, and then for a neuron in self.neurons, we want to get neuron.parameters, and we want to params.extend, right? These are the parameters of this neuron, and then we want to put them on top of params, so params.extend of p’s, and then we want to return params. This is way too much code, so there is a way to simplify this, which is return p for neuron in self.neurons for p in neuron.parameters. It is a single list comprehension in Python. You can sort of nest them like this. You can then create the desired array. These are identical. We can take this out. Let's do the same here: def parameters self and return a parameter for layer in self.layers for p in layer.parameters, and that should be good. Let me pop out this so that we do not re-initialize our network because we need to re-initialize our... Unfortunately, we will probably have to re-initialize the network because we just added functionality. This class, of course, we want to get all the network parameters, but that is not going to work because this is the old class. We do have to re-initialize the network, which will change some of the numbers. Let me do that so that we pick up the new API. We can now do n.parameters, and these are all the weights and biases inside the entire neural net. In total, this MLP has 41 parameters. We will be able to change them. If we recalculate the loss here, we see that, unfortunately, we have slightly different predictions and slightly different losses, but that is okay. We see that this neuron’s gradient is slightly negative. We can also look at its data right now, which is 0.85. This is the current value of this neuron, and this is its gradient on the loss. What we want to do now is we want to iterate for every p in n.parameters. For all the 41 parameters in this neural net, we actually want to change p.data slightly according to the gradient information. We need to do a tiny update in this gradient descent scheme. In gradient descent, we are thinking of the gradient as a vector pointing in the direction of increased loss. In gradient descent, we are modifying p.data by a small step size in the direction of the gradient. The step size, as an example, could be a very small number like 0.01 is the step size times p.grad. We have to think through some of the signs here. Working with this specific example here, we see that if we just left it like this, then this neuron’s value would be currently increased by a tiny amount of the gradient. The gradient is negative, so this value of this neuron would go slightly down. It would become like 0.84 or something like that. If this neuron’s value goes lower, that would actually increase the loss. That is because the derivative of this neuron is negative. Increasing this makes the loss go down, so increasing it is what we want to do instead of decreasing it. What we are missing here is actually a negative sign. This other interpretation is that we want to minimize the loss. We do not want to maximize the loss. We want to decrease it. The other interpretation, as I mentioned, is that you can think of the gradient vector, the vector of all the gradients, as pointing in the direction of increasing the loss. We want to decrease it, so we want to go in the opposite direction. You can convince yourself that this sort of plug does the right thing here with the negative because we want to minimize the loss. If we nudge all the parameters by a tiny amount, then we will see that this data will have changed a little bit. This neuron is now a tiny amount greater in value. 0.854 went to 0.857, and that is a good thing. Slightly increasing this neuron’s data makes the loss go down, according to the gradient. The correct thing has happened sign wise. Because we have changed all these parameters, we expect that the loss should have gone down a bit. We want to re-evaluate the loss. Let me do that outside here so that we can compare the two loss values. If I recalculate the loss, we expect the new loss now to be slightly lower than this number. Hopefully, what we are getting now is a tiny bit lower than 4.84, which is 4.36. Remember, the way we have arranged this is that low loss means that our predictions are matching the targets. Our predictions now are probably slightly closer to the targets. All we have to do is iterate this process. We have done the forward pass, and this is the loss. Now we can loss.backward. Let me take these out. We can do a step size, and now we should have a slightly lower loss. 4.36 goes to 3.9. We have done the forward pass, here is the backward pass, nudge. Now the loss is 3.66, 3.47, and you get the idea. We just continue doing this. This is gradient descent. We are iteratively doing a forward pass, backward pass, update, forward pass, backward pass, update. The neural net is improving its predictions. If we look at y_pred now, we see that this value should be getting closer to one. This value should be getting more positive. These should be getting more negative, and this one should be also getting more positive. If we iterate this a few more times, we may be able to go a bit faster. Let's try a slightly higher learning rate. There we go. Now we are at 0.31. If you go too fast, by the way, if you try to make it too big of a step, you may actually overstep it. It is overconfidence. Remember, we do not actually know exactly about the loss function. The loss function has all kinds of structure. We only know about the very local dependence of all these parameters on the loss. If we step too far, we may step into a part of the loss that is completely different, and that can destabilize training and make your loss actually blow up. The loss is now 0.04, so the predictions should be really quite close. Let's take a look. This is almost one, almost negative one, almost one. We can continue going. Backward, update. There we go. We went way too fast, and we actually overstepped. We got too eager. Where are we now? 7e-9, so this is very very low loss. The predictions are basically perfect. Somehow, we were doing way too big updates, and we briefly exploded. Somehow, we ended up getting into a really good spot. Usually, this learning rate and the tuning of it is a subtle art. You want to set your learning rate. If it is too low, you are going to take way too long to converge. If it is too high, the whole thing gets unstable, and you might actually even explode the loss, depending on your loss function. Finding the step size to be just right is a pretty subtle art sometimes when you are using vanilla gradient descent, but we happened to get into a good spot. We can look at n.parameters. This is the setting of weights and biases that makes our network predict the desired targets very very close. We have successfully trained the neural net. Let's make this a tiny bit more respectable and implement an actual training loop. This is the data definition that stays. This is the forward pass. For k in range, we are going to take a bunch of steps. First, you do the forward pass, we validate the loss. Let's re-initialize the neural net from scratch. Here is the data, and we first do a forward pass. Then, we do the backward pass, and then we do an update. That is gradient descent. We should be able to iterate this, and we should be able to print the current step, the current loss. Let's just print the number of the loss, and that should be it. The learning rate 0.01 is a little too small. 0.1 we saw is a little bit dangerously too high. Let's go somewhere in between. We will optimize this for not 10 steps, but let's go for say 20 steps. Let me erase all of this junk and run the optimization. You see how we have actually converged slower in a more controlled manner and got to a loss that is very low. I expect y_pred to be quite good. There we go. That is it. This is kind of embarrassing, but we actually have a really terrible bug in here. It is a subtle bug, and it is a very common bug. I cannot believe I have done it for the 20th time in my life, especially on camera. I could have reshot the whole thing, but I think it is pretty funny. You get to appreciate a bit what working with neural nets is like. Sometimes we are guilty of this bug. I have actually tweeted the most common neural net mistakes a long time ago. I am not really going to explain any of these, except that we are guilty of number three. You forgot to zero grad before the backward. What is that? Basically, what is happening, and it is a subtle bug, and I am not sure if you saw it, is that all of these weights here have a dot data and a dot grad. The grad starts at zero. Then, we do a backward, and we fill in the gradients. Then, we do an update on the data, but we do not flush the grad. It stays there. When we do the second forward pass and we do a backward again, remember that all the backward operations do a plus equals on the grad. These gradients just add up, and they never get reset to zero. We did not zero grad. Here is how we zero grad. Before the backward, we need to iterate over all the parameters, and we need to make sure that p.grad is set to zero. We need to reset it to zero just like it is in the constructor. Remember, all the way here, for all these value nodes, the grad is reset to zero. All these backward passes do a plus equals on that grad. We need to make sure that we reset these grads to zero so that when we do backward, all of them start at zero. The actual backward pass accumulates the loss derivatives into the grads. This is zero grad in PyTorch. We will get a slightly different optimization. Let's reset the neural net. The data is the same. This is now, I think, correct. We get a much more slower descent. We still end up with pretty good results. We can continue this a bit more to get down lower and lower and lower. The only reason that the previous thing worked, it is extremely buggy. The only reason that worked is that this is a very very simple problem, and it is very easy for this neural net to fit this data. The grads ended up accumulating, and it effectively gave us a massive step size, and it made us converge extremely fast. We have to do more steps to get to very low values of loss and get y_pred to be really good. We can try to step a bit greater. We are going to get closer and closer to one, minus one, and one. Working with neural nets is sometimes tricky because you may have lots of bugs in the code. Your network might actually work just like ours worked, but the chances are that if we had a more complex problem, then this bug would have made us not optimize the loss very well. We were only able to get away with it because the problem is very simple. Let's now bring everything together and summarize what we learned. What are neural nets? Neural nets are these mathematical expressions. Fairly simple mathematical expressions in the case of a multi-layer perceptron that take the data as input and they take the weights and the parameters of the neural net as input. A mathematical expression for the forward pass followed by a loss function. The loss function tries to measure the accuracy of the predictions. Usually, the loss will be low when your predictions are matching your targets, or where the network is behaving well. We manipulate the loss function so that when the loss is low, the network is doing what you want it to do on your problem. We backward the loss. We use backpropagation to get the gradient. We know how to tune all the parameters to decrease the loss locally, but then we have to iterate that process many times in what is called gradient descent. We simply follow the gradient information. That minimizes the loss. The loss is arranged so that when the loss is minimized, the network is doing what you want it to do. We just have a blob of neural stuff, and we can make it do arbitrary things. That gives neural nets their power. This is a very tiny network with 41 parameters, but you can build significantly more complicated neural nets with billions, at this point almost trillions of parameters. It is a massive blob of neural tissue, simulated neural tissue roughly speaking. You can make it do extremely complex problems. These neurons then have all kinds of very fascinating emergent properties when you try to make them do significantly hard problems, as in the case of GPT, for example. We have massive amounts of text from the internet. We are trying to get a neural net to predict, to take a few words and try to predict the next word in a sequence. That is the learning problem. It turns out that when you train this on all of the internet, the neural net actually has really remarkable emergent properties. That neural net would have hundreds of billions of parameters, but it works on fundamentally the exact same principles. The neural net of course will be a bit more complex, but otherwise the value in the gradient would be identical. The gradient descent would be there and would be basically identical. People usually use slightly different updates. This is a very simple stochastic gradient descent update. The loss function would not be mean squared error. They would be using something called the cross-entropy loss for predicting the next token. There are a few more details, but fundamentally the neural network setup and neural network training is identical and pervasive. You now understand intuitively how that works under the hood. In the beginning of this video, I told you that by the end of it you would understand everything in micrograd, and then we would slowly build it up. Let me briefly prove that to you. I am going to step through all the code that is in micrograd as of today. Some of the code will change by the time you watch this video because I intend to continue developing micrograd. Let's look at what we have. Init.py is empty. When you go to engine.py, that has the value. Everything here you should mostly recognize. We have the data, grad attributes, we have the backward function, we have the previous set of children and the operation that produced this value. We have addition, multiplication, and raising to a scalar power. We have the ReLU non-linearity, which is a slightly different type of non-linearity than tanh that we used in this video. Both of them are non-linearities. Tanh is not actually present in micrograd as of right now, but I intend to add it later with the backward, which is identical. All these other operations are built up on top of the operations here. Values should be very recognizable except for the non-linearity used in this video. There is no massive difference between ReLU, tanh, sigmoid, and these other non-linearities. They are all roughly equivalent and can be used in MLPs. I used tanh because it is a bit smoother, and because it is a little bit more complicated than ReLU. It stressed a little bit more the local gradients and working with those derivatives, which I thought would be useful. nn.py is the neural networks library as I mentioned. You should recognize identical implementation of neuron, layer, and MLP. Notably, we have a class module here. There is a parent class of all these modules. I did that because there is an nn.module class in PyTorch, and so this exactly matches that API. nn.module in PyTorch also has a zero grad, which I have refactored out here. That is the end of micrograd. There is a test, which you will see basically creates two chunks of code, one in micrograd and one in PyTorch. We will make sure that the forward and the backward pass agree identically for a slightly less complicated expression. A slightly more complicated expression, everything agrees. We agree with PyTorch on all of these operations. Finally, there is a demo.ipynb here. It is a bit more complicated binary classification demo than the one I covered in this lecture. We only had a tiny data set of four examples. Here, we have a bit more complicated example with lots of blue points and lots of red points. We are trying to build a binary classifier to distinguish two-dimensional points as red or blue. It is a bit more complicated MLP. It is a bigger MLP. The loss is a bit more complicated because it supports batches. Because our dataset was so tiny, we always did a forward pass on the entire dataset of four examples. When your dataset is like a million examples, we pick out some random subset. We call that a batch, and then we only process the batch forward, backward, and update. We do not have to forward the entire training set. This supports batching because there are a lot more examples here. We do a forward pass. The loss is slightly more different. This is a max margin loss that I implement here. The one that we used was the mean squared error loss because it is the simplest one. There is also the binary cross-entropy loss. All of them can be used for binary classification and do not make too much of a difference in the simple examples that we looked at so far. There is something called L2 regularization used here. This has to do with generalization of the neural net and controls the overfitting in machine learning settings, but I did not cover these concepts in this video. Potentially later. The training loop, you should recognize. Forward, backward with zero grad, and update. In the update, the learning rate is scaled as a function of the number of iterations, and it shrinks. This is something called learning rate decay. In the beginning, you have a high learning rate. As the network stabilizes near the end, you bring down the learning rate to get some of the fine details in the end. We see the decision surface of the neural net. It learns to separate out the red and the blue area based on the data points. That is the slightly more complicated example and the demo that you are free to go over. As of today, that is micrograd. I also wanted to show you a little bit of real stuff so that you get to see how this is actually implemented in a production-grade library like PyTorch. I wanted to find and show you the backward pass for tanh in PyTorch. In micrograd, we see that the backward pass for tanh is one minus t squared, where t is the output of tanh of x times of that grad, which is the chain rule. We are looking for something that looks like this. I went to PyTorch, which has an open-source GitHub codebase. I looked through a lot of its code. I spent about 15 minutes and could not find tanh. These libraries unfortunately grow in size and entropy. If you just search for tanh, you get apparently 2,800 results in 406 files. I do not know what these files are doing and why there are so many mentions of tanh. These libraries are quite complex, and they are meant to be used, not really inspected. Eventually, I did stumble on someone who tries to change the tanh backward... The code pointed to the CPU kernel and the CUDA kernel for `tanh` backward, which depends on whether you're using PyTorch on a CPU or GPU device. These are different devices. This is the `tanh` backward kernel for the CPU. It's large because it handles complex types, a specific data type of b-float 16, and also a general case. Deep inside, we see a backward pass with `a * (1 - b^2)`, where `b` is the output of `tanh` and `health.grad` is present. This was found inside the `binaryops` kernel, even though `tanh` is not a binary operation. Here is the GPU kernel, which is just one line of code for the non-complex case. While we found the code, the PyTorch implementations are very large, unlike Micrograd. PyTorch allows you to register a new type of function to add as a building block. For example, to add a Legendre polynomial, you would register a class that subclasses `storage.org.function`. You must define the forward pass and the backward pass of the function. PyTorch will backpropagate through your function if you implement the forward and local gradient (backward). Then, you can use this function as a component with other PyTorch components. This is the only information that you have to provide. You can register new function types this way. That is everything I wanted to cover in this lecture. I hope you enjoyed building out Micrograd and found it interesting. I will post relevant links in the video description. I may also post a link to a discussion forum where you can ask questions. I might also do a follow-up video to answer common questions. If you enjoyed this video, please like and subscribe. Now we know `dl` by wait. And that's everything I wanted to cover in this lecture. I hope you enjoyed us building up Micrograd. 
