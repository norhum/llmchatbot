All the tokens in all the positions in the B by T arrangement produce a key and a query in parallel and independently. No communication has happened yet. All the queries will do a dot product with all the keys. We want the affinities between these to be the query multiplying the key. We cannot matrix multiply this directly; we need to transpose the key. We also need to be careful of the batch dimension. Specifically, we want to transpose the last two dimensions, -2 and -1. This matrix multiplication will do B by T by 16 matrix multiplies B by 16 by T, giving us B by T by T. For every row of B, we will have a T-squared matrix giving us the affinities. These are the weights, not zeros, coming from this dot product between the keys and the queries. The weighted aggregation is a data-dependent function between the keys and queries of these nodes. Before, the weight was a constant applied the same way to all the batch elements. Now, every single batch element will have a different weight because every single batch element contains different tokens at different positions. This is now data-dependent. For the zeroth row of the input, these are the weights that came out and they are not uniform. For example, the last row, the eighth token, knows what content it has and its position. The eighth token creates a query based on that. For example, “I'm a vowel in the E position, looking for a consonant at positions up to four.” All the nodes emit keys. One channel could be "I am a consonant, and I am in a position up to four.” That key would have a high number in that specific channel. When the query and key do a dot product, they can find each other and create a high affinity. If a high affinity exists, say this token was interesting to the eighth token, then, through the softmax, we aggregate a lot of its information into the position, allowing us to learn a lot about it. This is looking at the weight after this has already happened. Let’s erase the masking and the softmax to see the under-the-hood internals. Without masking and softmax, the weight comes out like this. These are the raw outputs of the dot products. They range from negative two to positive two. These are the raw interactions and raw affinities between all the nodes. If we are the fifth node, we do not want to aggregate anything from the sixth, seventh, or eighth nodes. So, we use upper triangular masking; those are not allowed to communicate. We want a nice distribution so we do not aggregate a negative value. Instead, we exponentiate and normalize to get a distribution that sums to one. This tells us, in a data-dependent manner, how much information to aggregate from any of these past tokens. This is the weight, and it is not zeros, but calculated this way. There is one more part to a single self-attention head. When we do the aggregation, we do not aggregate the tokens directly. We produce a value. In the same way that we produced the key and query, we create a value. Instead of aggregating X, we calculate a V, which is achieved by propagating a linear layer on top of X. We output the weight multiplied by V. V is the elements, the vectors, that we aggregate, instead of the raw X. This makes the output of this single head 16-dimensional, because that is the head size. X is like private information to this token. If you think of it this way, X is private to the token. The fifth token has some identity, and that information is in vector X. For the purposes of the single head, this is what the token is interested in, and if you find it interesting, this is what it will communicate to you. That information is stored in V. V is what gets aggregated for this single head between the different nodes. That is the self-attention mechanism. Attention is a communication mechanism. You can think of it as a communication mechanism where you have nodes in a directed graph with edges pointing between the nodes. Every node has a vector of information. It aggregates information via a weighted sum from all the nodes that point to it. This is done in a data-dependent manner. Our graph has a different structure. We have eight nodes because the block size is eight. There are always eight tokens. The first node is only pointed to by itself. The second node is pointed to by the first node and itself, and so on up to the eighth node, which is pointed to by all the previous nodes and itself. This is the structure of the directed graph in an autoregressive scenario like language modeling. In principle, attention can be applied to any arbitrary directed graph. It is just a communication mechanism between the nodes. There is no notion of space. Attention acts over a set of vectors in this graph. By default, the nodes have no idea where they are positioned. We encode them positionally to give them some information anchored to a specific position, so they know where they are. This is different than convolution. Convolution acts on the information in space. Attention is a set of vectors out in space that communicate. If you want them to have a notion of space, you need to add it. That is what we did when we calculated the positional encodings and added that information to the vectors. The elements across the batch dimension, which are independent examples, never talk to each other. They are always processed independently. This is a batched matrix multiply that applies matrix multiplication in parallel across the batch dimension. In this directed graph analogy, we have four separate pools of eight nodes because the batch size is four. Those eight nodes only talk to each other. In total, there are 32 nodes being processed in four separate pools of eight. In language modeling, the future tokens will not communicate to the past tokens. This doesn't have to be the constraint in the general case. In many cases, you may want to have all of the nodes talk to each other fully. For example, in sentiment analysis, you may want all the tokens to talk to each other because you are predicting the sentiment of the sentence. In those cases, you would use an encoder block of self-attention. An encoder block allows all the nodes to completely talk to each other. What we are implementing here is called a decoder block because it is a decoding language, an autoregressive format, where you mask with a triangular matrix. Nodes from the future never talk to the past because they would give away the answer. In encoder blocks, all the nodes talk. In decoder blocks, this triangular structure is present. Attention supports arbitrary connectivity between nodes. Self-attention means that the keys, queries, and values all come from the same source, X. The nodes are self-attending. In principle, attention is more general. In encoder-decoder transformers, the queries are produced from X, but the keys and values come from a separate external source. Sometimes, they come from encoder blocks that encode context. Cross-attention is used when there is a separate source of nodes to pull information from. Self-attention is when nodes look at each other. Our attention here is self-attention, but attention is more general. In the “Attention Is All You Need” paper, we have already implemented the attention. Given query, key, and value, we multiply the query and key, soft-max it, and aggregate the values. We are missing dividing by one over the square root of the head size. They call this scaled attention. It is important normalization because, if you have unit Gaussian inputs, then the variance of the weight will be on the order of the head size, which is 16. If you multiply by one over the square root of the head size, then the variance will be one. The weight feeds into the softmax, so it is important that we are fairly diffuse. Because of softmax, if the weight takes on very positive and negative numbers, then softmax will converge towards one-hot vectors. If applying softmax to small values, you get a diffuse thing. If you sharpen it by multiplying by eight, the softmax will start to sharpen towards the max. We don't want the values to be extreme at initialization. Otherwise, softmax will be too peaky, and each node will aggregate information from a single node. The scaling is used to control variance at initialization. In the code, the “head” module implements a single head of self-attention. You give it a head size, and it creates the key, query, and value linear layers, which typically do not have biases. These are the linear projections applied to the nodes. The “Trill” variable is not a parameter of the module; it is a buffer. You must assign it to the module using “register buffer.” This creates the lower triangular matrix. Given the input X, we calculate the keys and queries. We calculate the attention scores in the weight. We normalize it using scaled attention. We mask the future from the past, which makes it a decoder block. Then softmax and aggregate the value and output. In the language model, a head is created in the constructor. The head size is the same as the embedding, for now. Once we have encoded the information, we feed it into the self-attention head. The output goes into the decoder language modeling head to create the logits. This is the simplest way to plug in a self-attention component into the network. We must ensure that the input "idx" to the model has no more than the block size, or the positional embedding table will run out of scope. We crop the context to ensure no more than the block size elements are passed in. The learning rate was decreased, and the number of iterations was increased. We saw a little bit of improvement, going from 2.5 to 2.4, but the text is still not amazing. The self-attention head is doing some communication. There is still a long way to go. We have implemented scaled dot product attention. In the “Attention is All You Need” paper, there is multi-head attention. Multi-head attention is just applying multiple attentions in parallel and concatenating their results. It is multiple attentions in parallel. To implement multi-head attention, we create multiple heads, and we run them in parallel into a list and concatenate their outputs over the channel dimension. Instead of having one communication channel with a head size of 32, we have four communication channels in parallel. Each of these communication channels will be smaller. Because we have four channels, we have eight-dimensional self-attention. From each channel, we get eight-dimensional vectors, and the four are concatenated to give 32, the original embedding. This is like a group convolution. Instead of one large convolution, we do convolution in groups. This is multi-headed self-attention. We use the self-attention heads instead. Multi-head attention improves the validation loss from 2.4 to 2.28. The generation is still not amazing, but the tokens have a lot to talk about. They want to find the consonants, vowels, and different things. Creating multiple channels helps to gather data and decode the output. We have implemented positional encodings, token encodings, addition, and masked multi-headed attention. There is a feed-forward part. This is a simple multi-layer perceptron. The position-wise feed-forward networks are a simple MLP. We will add computation into the network. This computation is on a per-node level. The multi-headed self-attention did the communication, but we went too fast to calculate the logits. The tokens looked at each other but didn't think about what they found. We implemented a small, single-layer feed-forward. It is a linear followed by a ReLU nonlinearity. The feed-forward is called sequentially after the self-attention. The self-attention is the communication. Once they have gathered the data, they need to think on it individually. That is what feed-forward is doing. When trained, the validation loss went down to 2.24 from 2.28. The output still looks terrible, but we have improved the situation. We are interspersing the communication with the computation. That is also what the Transformer does. We have a block that intersperses communication and then computation. The communication is done using multi-headed self-attention, and computation is done with the feed-forward network on all tokens independently. The block uses the embedding dimension and number of heads, which is like group size in group convolution. The number of heads is four. Because this is 32, the head size should be eight. This is how the Transformer structures the sizes. The head size will be eight. We want to intersperse them. We create blocks, which are a sequential application of the block, to intersperse communication and feed-forward many times and decode. This does not actually give a very good answer. We are starting to get a pretty deep neural net. Deep neural nets suffer from optimization issues. We need one more idea from the transformer paper to resolve those difficulties. Skip connections dramatically help with the depth of these networks and make sure that the networks remain optimizable. These are skip or residual connections. You transform the data, but have a skip connection with addition from the previous features. The computation happens from top to bottom. You have a residual pathway and can fork off from it, perform computation, and project back via addition. You go from input to targets only via plus. During back propagation, addition distributes gradients equally to both branches. Supervision, or gradients from the loss, hop through every addition node to the input and fork off into the residual blocks. This gradient Super Highway goes directly from the supervision to the input unimpeded. The blocks are initialized in the beginning, so they contribute very little to the residual pathway. They are initialized that way. In the beginning, they are almost not there. They come online over time. This helps with optimization. To implement this in our block, we do x = x + self-attention and x = x + self.feed-forward. X forks off for communication, then comes back. X forks off for computation, then comes back. Those are the residual connections. We also introduce the projection, which is a linear transformation of the concatenated self-attention outputs and projects back into the residual pathway. In feed-forward, it is the same thing. We have a self.projection. I simplified it by coupling it inside the same container. This is the projection layer back to the residual pathway. The inner layer of the feed-forward network should be multiplied by four in terms of channel sizes. We multiply the embedding by four for the feed-forward and then project back down to the embedding for the projection. The validation loss goes down to 2.08. The network is getting big enough that the train loss is ahead of the validation loss. We see a little overfitting. Generations are still not amazing, but we are seeing some patterns. The second innovation helpful for deep neural networks is layer norm. Layer norm is in pytorch, similar to batch norm. Batch norm ensured that across the batch dimension, any individual neuron had a unit Gaussian distribution; zero mean and unit standard deviation. We copied the batch norm 1D from the make more series. It guarantees that, if we look at the zeroth column, it is zero mean and one standard deviation. It is normalizing every single column. Rows are not normalized. To implement layer norm, we change from zero to one. Now, we are normalizing the rows. Columns are not normalized. Rows are normalized for every individual example. This 100-dimensional vector is normalized this way. Our computation does not span across examples, so we can delete all the buffers stuff. We don't maintain any buffers and don’t need the distinction between training and test time. We keep the gamma and beta. We don’t care about momentum. This is a layer norm that normalizes the rows instead of the columns. Before incorporating layer norm, I noted that very few details about the transformer have changed in the last five years, but this slightly departs from the original paper. The add and norm are applied after the transformation, but it is more common to apply the layer norm before the transformation. This is the pre-norm formulation. We need two layer norms: layer norm one and layer norm two. The layer norms are applied immediately on X, before self-attention and feed-forward. The size of the layer norm is the embedding size, 32. When the layer norm normalizes our features, the mean and variance are taken over 32 numbers. The batch and time act as batch dimensions. The normalization is per-token, which normalizes features and makes them unit Gaussian at initialization.
