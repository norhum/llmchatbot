Okay, everyone, today we are continuing our implementation of our favorite character-level language model, "make more." You'll notice that the background is different because I am in Kyoto, and it is awesome. I'm in a hotel room here. Over the last few lectures, we've built up to this architecture, which is a multi-layer perceptron character-level language model. It receives three previous characters and tries to predict the fourth character in a sequence, using a simple multi-perceptron with one hidden layer of neurons and 10h nonlinearities. Now, we want to make this architecture more complex. In particular, we want to take more characters in a sequence as input, not just three. In addition, we don't just want to feed them all into a single hidden layer because that squashes too much information too quickly. Instead, we would like to make a deeper model that progressively fuses this information to make its guess about the next character in a sequence. As we make this architecture more complex, we will arrive at something that looks very much like a WaveNet. WaveNet is a paper published by DeepMind in 2016. It is also a language model, but it tries to predict audio sequences instead of character-level or word-level sequences. Fundamentally, the modeling setup is identical. It is an auto-regressive model, and it tries to predict the next character in a sequence. The architecture takes this interesting hierarchical approach to predicting the next character in a sequence with a tree-like structure. We are going to implement this architecture in this video.

Let's get started. The starter code for part five is very similar to where we ended up in part three. Part four was the manual backpropagation exercise, which was kind of an aside. We are coming back to part three, copy-pasting chunks out of it, and that is our starter code for part five. I've changed very few things otherwise, so a lot of this should look familiar if you've gone through part three. In particular, we are importing libraries, reading our dataset of words, and processing that set of words into individual examples. None of this data generation code has changed. We have lots of examples, specifically 182,000 examples of three characters trying to predict the fourth. We've broken up every word into little problems of "given three characters, predict the fourth." This is our dataset, and this is what we're trying to get the neural net to do. In part three, we started to develop our code around these layer modules, for example, the `class Linear`. We're doing this because we want to think of these modules as building blocks, like Lego bricks, that we can stack up into neural networks. We can feed data between these layers and stack them up into graphs. We also developed these layers to have APIs and signatures very similar to those in PyTorch. PyTorch has `torch.nn` with all these layer building blocks that you would use in practice. We were developing our layers to mimic the APIs of those. For example, we have linear, so there will also be a `torch.nn.linear`, and its signature will be very similar to our signature, and the functionality will be also quite identical as far as I'm aware. We have the linear layer with the base from 1D layer and the `tanh` layer that we developed previously. Linear is just a matrix multiply in the forward pass of this module. Batch norm, of course, is this crazy layer that we developed in the previous lecture. It has these running means and variances that are trained outside of backpropagation. They are trained using an exponential moving average inside this layer when we call the forward pass. In addition, there's this training flag because the behavior of batch norm is different during training time and evaluation time. So, we have to be very careful that batch norm is in the correct state, that it's in the evaluation state or training state. That is something to now keep track of, something that sometimes introduces bugs because you forget to put it into the right mode. Finally, we saw that batch norm couples the statistics, or the activations, across the examples in the batch. Normally, we thought of the batch as just an efficiency thing, but now we are coupling the computation across batch elements for the purposes of controlling the activation statistics. It's a very weird layer, and it can be a source of bugs, partly because you have to modulate the training and eval phase and so on. In addition, you have to wait for the mean and variance to settle and reach a steady state. There is state in this layer, and state is usually harmful.

I previously had a generator object, where `generator = g` inside these layers. I've discarded that in favor of just initializing the torch RNG outside here and using it just once globally for simplicity. Here, we are starting to build out some of the neural network elements. This should look very familiar. We have our embedding table, `C`, and then a list of layers. It's a linear layer that feeds to a batch norm layer that feeds to a `tanh` layer, and then a linear output layer. Its weights are scaled down, so we are not confidently wrong at the initialization. This is about 12,000 parameters. We're telling PyTorch that the parameters require gradients. The optimization is identical and should look very familiar. Nothing changed here. The loss function looks crazy, and we should probably fix this. That's because 32 batch elements are too few, and you can get very lucky or unlucky in any one of these batches, which creates a very spiky loss function. We are going to fix that soon. When we evaluate the trained neural network, we need to remember, because of the batch norm layers, to set all the layers to be `training=False`. This only matters for the batch norm layer so far. We evaluate, and we see that we currently have a validation loss of 2.10, which is fairly good, but there's still ways to go. Even at 2.10, when we sample from the model, we get relatively name-like results that do not exist in the training set, for example, "Yvonne," "kilo," "Pros," "Alaia," etc. Certainly not unreasonable, I would say, but not amazing. We can still push this validation loss even lower and get much better samples that are even more name-like. So, let's improve this model.

First, let's fix this graph because it is daggers in my eyes, and I just can't take it anymore. `lossi`, if you recall, is a Python list of floats. For example, the first 10 elements. We need to average up some of these values to get a more representative value. One way to do this is the following: In PyTorch, if I create a tensor of the first 10 numbers, this is currently a one-dimensional array, but I can view this array as two-dimensional. For example, I can use it as a 2x5 array, and this is a 2D tensor, now 2x5. You see what PyTorch has done is that the first row of this tensor is the first five elements, and the second row is the second five elements. I can also view it as a 5x2 as an example. Recall that I can also use negative one in place of one of these numbers, and PyTorch will calculate what that number must be to make the number of elements work out. This allows us to spread out some of the consecutive values into rows. That is very helpful because what we can do now is first create a torch tensor out of the list of floats and then view it as whatever it is. We are going to stretch it out into rows of 1,000 consecutive elements. The shape of this now becomes 200x1000, and each row is 1,000 consecutive elements in this list. Now, we can do a mean along the rows, and the shape of this will just be 200. We've taken the mean on every row.  `plt.plot` of that should be something nicer, much better. We see that we made a lot of progress, and then here, this is the learning rate decay. We see that the learning rate decay subtracted a ton of energy out of the system and allowed us to settle into a local minimum in this optimization. This is a much nicer plot. Let me come up and delete the monster, and we're going to be using this going forward.

Next, I'm bothered by the fact that our forward pass is a little bit gnarly and takes way too many lines of code. We've organized some of the layers inside the `layers` list, but not all of them, for no reason. We still have the embedding table as a special case outside of the layers. The viewing operation here is also outside of our layers. Let's create layers for these, and then we can add those layers to our list. In particular, we have this embedding table, and we are indexing at the integers inside the batch `XB` inside the tensor `XB`. That's an embedding table lookup, just done with indexing. We have this view operation, which, if you recall from the previous video, simply rearranges the character embeddings and stretches them out into a row. That effectively is a concatenation operation, except it's free because viewing is very cheap in PyTorch; no memory is being copied; we're just re-representing how we view that tensor. Let's create modules for both of these operations: the embedding operation and flattening operation. I wrote the code to save some time. We have a module `Embedding` and a module `Flatten`. Both of them simply do the indexing operation in the forward pass and the flattening operation here. `C` will now just become `self.weight` inside an embedding module. I'm calling these layers specifically `Embedding` and `Flatten` because both exist in PyTorch. In PyTorch, we have `nn.Embedding`, which takes the number of embeddings and the dimensionality of the embedding, just like we have here. In addition, it takes a lot of other keyword arguments that we are not using for our purposes yet. `nn.Flatten` also exists in PyTorch, and it also takes additional keyword arguments that we are not using. We have a very simple flatten, but both of them exist in PyTorch; they're just a bit simpler. Now that we have these, we can take out some of these special-cased things. Instead of `C`, we're just going to have an `Embedding` of the vocabulary size and `n_embed`. After the embedding, we are going to flatten. Let's construct those modules. Now, I can take out this, and here I don't have to special-case anymore because now `C` is the embedding's weight, and it's inside the layers. This should just work. Here, our forward pass simplifies substantially because we don't need to do these operations outside of these layers. They're now inside layers, so we can delete those. To kick things off, we want this little `X`, which, in the beginning, is just `XB`, the tensor of integers specifying the identities of these characters at the input. These characters can now directly feed into the first layer, and this should just work. Let me insert a break to make sure the first iteration runs and that there is no mistake. That ran properly. We substantially simplified the forward pass here.

I changed my microphone, so hopefully, the audio is a little bit better now. One more thing that I would like to do to “PyTorchify” our code even further is that right now we are maintaining all of our modules in a naked list of layers. We can also simplify this because we can introduce the concept of PyTorch containers. In `torch.nn`, which we are basically rebuilding from scratch here, there's a concept of containers. These containers are a way of organizing layers into lists or dictionaries. In particular, there's a `Sequential` which maintains a list of layers and is a module class in PyTorch. It simply passes a given input through all the layers sequentially, exactly as we are doing here. Let's write our own `Sequential`. I've written the code here. The code for `Sequential` is straightforward. We pass in a list of layers, which we keep here. Given any input in a forward pass, we call all the layers sequentially and return the result. In terms of parameters, it's just all the parameters of the child modules. We can run this and simplify this substantially. We don't maintain a naked list of layers anymore. We have a notion of a `model`, which is a module. In particular, it's a `Sequential` of all these layers. Parameters are simply just `model.parameters`, and that list comprehension now lives here. Here, we are doing all the things we used to do. The code again simplifies substantially because we don't have to do this forwarding here. Instead of that, we can just call the `model` on the input data. The input data here are the integers inside `XB`. We can simply do `logits`, which are the outputs of our model, by simply calling the `model` on `XB`. The cross entropy here takes the `logits` and the targets. This simplifies substantially, and this looks good. Let's make sure this runs. That looks good. Here, we actually have some work to do still, but I'm going to come back later. For now, there are no more layers. There is a `model` that has layers, but it's not a way to access attributes of these classes directly. We'll come back and fix this later. Here, of course, this simplifies substantially as well because `logits` are the `model` called on `X`, and these `logits` come here. We can evaluate the train and validation loss, which currently is terrible because we just initialized the neural net. We can also sample from the model, and this simplifies dramatically as well because we just want to call the `model` onto the context. The outcome is `logits`. These `logits` go into softmax and get the probabilities, etc. We can sample from this model. What did I screw up?

I fixed the issue. We now get the result we expect, which is gibberish because the model is not trained because we re-initialized it from scratch. The problem was that when I fixed this cell to be `model.layers` instead of just `layers`, I did not run the cell. Our neural net was in training mode. The issue was caused by the batch norm layer because the batch norm was in training mode. Here, we are passing in an input, which is a batch of just a single example made up of the context. If you are passing in a single example into a batch norm layer that is in the training mode, you're going to end up estimating the variance using that input. The variance of a single number is not a number because it is a measure of a spread. The variance of just the single number five, you can see, is not a number. That happened in the batch norm layer, which basically caused an issue and polluted all further processing. All we had to do was make sure that this cell runs. We saw the issue again; we didn't actually see the issue with the loss. We could have evaluated the loss, but we got the wrong result because batch norm was in training mode. We still get a result; it's just the wrong result because it's using the sample statistics of the batch, whereas we want to use the running mean and running variance inside the batch norm layer. This is an example of introducing a bug inline because we did not properly maintain the state of what is training or not.

I have rewritten everything, and here's where we are. As a reminder, we have a training loss of 2.05 and a validation loss of 2.10. Because these losses are very similar to each other, we have a sense that we are not overfitting too much on this task. We can make additional progress in our performance by scaling up the size of the neural network and making everything bigger and deeper. Currently, we are using this architecture, where we are taking in some number of characters, going into a single hidden layer, and then going to the prediction of the next character. The problem here is we don't have a naive way of making this bigger in a productive way. We could use our layers as building blocks to introduce additional layers here and make the network deeper. We are still crushing all the characters into a single layer at the very beginning. Even if we make this a bigger layer and add neurons, it's still silly to squash all that information so fast in a single step. Instead, we want our network to look a lot more like this in the WaveNet case. In WaveNet, when we are trying to make the prediction for the next character in the sequence, it is a function of the previous characters. However, not all of these different characters are crushed to a single layer. Instead, they are crushed slowly. We take two characters and fuse them into a bigram representation. We do that for all these characters consecutively. Then we take the bigrams and fuse them into four-character level chunks, and then fuse that again. We do that in a tree-like hierarchical manner. We fuse information from the previous context slowly into the network as it gets deeper. This is the kind of architecture we want to implement. In the WaveNet case, this is a visualization of a stack of dilated causal convolution layers. This makes it sound scary, but the idea is simple. The fact that it's a dilated causal convolution layer is really just an implementation detail to make everything fast. We will see that later, but for now, let's keep the basic idea of progressive fusion. We want to make the network deeper, and at each level, we want to fuse only two consecutive elements, then two bigrams, then two four-grams, and so on. Let's implement this.

First, let me scroll to where we built the dataset and change the block size from 3 to 8. We will be taking eight characters of context to predict the ninth character. The dataset now looks like this. We have more context to predict the next character, and these eight characters will be processed in this tree-like structure. If we scroll here, everything here should just work. We should be able to redefine the network. The number of parameters has increased by 10,000. That is because the block size has grown. This first linear layer is much bigger. Our linear layer now takes eight characters into this middle layer, so there are more parameters there. This should run. Let me just break right after the very first iteration. You see that this runs fine, but this network doesn't make too much sense. We are crushing too much information too fast. Let's see how we could implement the hierarchical scheme. Before we dive into the detail of the re-implementation, I was curious to actually run it and see where we are in terms of the baseline performance of just lazily scaling up the context length. I'll let it run, and we get a nice loss curve. Evaluating the loss, we actually see quite an improvement just from increasing the context length. I started a performance log here. Previously, we were getting a performance of 2.10 on the validation loss. Now, simply scaling up the context length from 3 to 8 gives us a performance of 2.02, quite a bit of improvement. Also, when you sample from the model, you see that the names are definitely improving qualitatively as well. We could spend a lot of time here tuning things and making it even bigger and scaling up the network further, even with this simple setup. Let's continue and implement our model and treat this as just a rough baseline performance. There's a lot of optimization left on the table in terms of some of the hyperparameters that you're hopefully getting a sense of now.

Let's scroll up and come back up. Here, I've created a scratch space for us to look at the forward pass of the neural net and inspect the shape of the tensor along the way as the neural net forwards. Here, I am temporarily for debugging, creating a batch of just four examples, four random integers. I'm plucking out those rows from our training set and passing the input `XB` into the model. The shape of `XB` here, because we have only four examples, is 4x8. This 8 is the current block size. Inspecting `XB`, we see we have four examples, each one a row of `XB`, and we have eight characters here. This integer tensor just contains the identities of those characters. The first layer of our neural net is the embedding layer. Passing `XB`, this integer tensor, through the embedding layer creates an output that is 4x8x10. Our embedding table has a 10-dimensional vector for each character, which we are trying to learn. The embedding layer plucks out the embedding vector for each one of these integers and organizes it all in a 4x8x10 tensor. All these integers are translated into 10-dimensional vectors inside this three-dimensional tensor. Passing that through the flattened layer, as you recall, views this tensor as just a 4x80 tensor. All these 10-dimensional embeddings for all eight characters are stretched out into a long row. It looks like a concatenation operation. By viewing the tensor differently, we now have a 4x80. Inside this 80, it's all the 10-dimensional vectors concatenated next to each other. The linear layer, of course, takes 80 and creates 200 channels via matrix multiplication. So far, so good. Now, I'd like to show you something surprising. Let's look at the insides of the linear layer and remind ourselves how it works. The linear layer here, in the forward pass, takes the input `X`, multiplies it with a weight, and then optionally adds bias. The weight here is two-dimensional as defined here, and the bias is one-dimensional. In terms of the shapes involved, what's happening inside the linear layer looks like this. I'm using random numbers here, but I'm illustrating the shapes. A 4x80 input comes into the linear layer, multiplied by this 80x200 weight matrix. Then there's a plus 200 bias. The shape of the whole thing that comes out of the linear layer is 4x200, as we see here. Notice that this will create a 4x200 tensor, and then plus 200, broadcasting occurs. So everything works here. The surprising thing I'd like to show you that you may not expect is that this input here that is being multiplied doesn't actually have to be two-dimensional. The matrix multiply operator in PyTorch is powerful. You can pass in higher-dimensional arrays or tensors, and everything works fine. For example, this could be 4x5x80, and the result will become 4x5x200. You can add as many dimensions as you like on the left here. The matrix multiplication only works on the last dimension, and the dimensions before it are left unchanged. These dimensions on the left are treated as a batch dimension. We can have multiple batch dimensions, and in parallel over all those dimensions, we are doing the matrix multiplication on the last dimension. This is convenient because we can use it in our network. Remember, we have these eight characters coming in. We don't want to flatten all of it out into a large 80-dimensional vector because we don't want to matrix multiply 80 into a weight matrix immediately. Instead, we want to group these like this. Every consecutive two elements, one and two, three and four, five and six, and seven and eight. All of these should now be flattened out and multiplied by a weight matrix, but all of these four groups, we want to process in parallel. It's like a batch dimension that we can introduce, and then we can process all of these bigram groups in the four batch dimensions of an individual example and also over the actual batch dimension of the four examples.

Effectively, what we want is that we take a 4x80 and multiply it by 80x200 in the linear layer. This is what happens, but instead, we don't want 80 characters or 80 numbers to come in. We only want two characters to come in on the very first layer. Those two characters should be fused. We want 20 numbers to come in. We don't want a 4x80 to feed into the linear layer. We want these groups of two to feed in. Instead of 4x80, we want this to be a 4x4x20. These are the four groups of two, and each one is a 10-dimensional vector. We need to change the flattened layer so it doesn't output a 4x80, but outputs a 4x4x20. Every two consecutive characters are packed in on the last dimension. Then these four is the first batch dimension, and this four is the second batch dimension, referring to the four groups inside every one of these examples. Then this will just multiply like this. This is what we want to get to. We're going to have to change the linear layer in terms of how many inputs it expects. It shouldn't expect 80, it should expect 20 numbers. We have to change our flattened layer so it doesn't just fully flatten out this entire example. It needs to create a 4x4x20 instead of a 4x80. Let's see how this could be implemented. We have an input that is a 4x8x10 that feeds into the flattened layer. Currently, the flattened layer stretches it out. The implementation of `flatten` takes `X` and views it as whatever the batch dimension is and then negative one. Effectively, what it does now is `E.view` of 4, negative one. The shape of this is 4x80. That's what currently happens. We want this to be a 4x4x20 where these consecutive 10-dimensional vectors get concatenated. In Python, you can take a list of a range of 10, which has numbers from 0 to 9. You can index like this to get all the even parts. You can index starting at one and going in steps of two to get all the odd parts. One way to implement this would be as follows: we can take `E` and index into it for all the batch elements, and then just even elements in this dimension at indexes 0, 2, 4, and 8, and then all the parts here from this last dimension. This gives us the even characters. This gives us all the odd characters. What we want to do is make sure these get concatenated in PyTorch. We want to concatenate these two tensors along the second dimension. This will have a shape of 4x4x20. This is definitely the result we want. We are explicitly grabbing the even parts and the odd parts and arranging them so they are right next to each other and concatenated. This works, but it turns out that you can simply use a view again and request the right shape. It just so happens that in this case, those vectors will again end up being arranged in exactly the way we want. If we take `E` and view it as a 4x4x20, which is what we want, we can check that this is exactly equal to, let me call this the explicit concatenation. The `explicit.shape` is 4x4x20. If you just view it as 4x4x20, you can check that when you compare to explicit, you get that this is element-wise, so you make sure that all of them are true, which is true. Long story short, we don't need to explicitly...
