The chain rule states that the local derivative multiplied by the derivative of B1. Since the local derivative is one on these three elements, multiplying by the derivative of B1 results in the derivative of B1. This can be viewed as a router where the gradient from above is routed equally to all elements participating in the addition. Therefore, the derivative of B1 flows equally to the derivatives of a11, a12, and a13. If we have the derivatives of all the elements of B, represented as a column tensor, which is the D counts sum calculated earlier, we see that these flow to all the elements of a horizontally. We want to take the D counts sum, which is of size 30 by 1, and replicate it 27 times horizontally to create a 30 by 27 array. There are multiple ways to implement this, including replicating the tensor. A cleaner approach involves using a two-dimensional array of ones with the shape of counts, 30 by 27, multiplied by the D counts sum. This uses broadcasting to implement the replication. However, D counts was already calculated for the first branch, and we are now finishing the second branch. So, these gradients need to be added together using plus equals. Let's comment out the comparison and verify that we have the correct result. PyTorch agrees with this gradient. Now, consider 'counts' as an element-wise exponential of 'norm logits'. To calculate 'D norm logits', since this is an element-wise operation, we use the local derivative of e to the x, which is just e to the x. We already calculated this, and it is stored in 'counts'. So, the local derivative times the 'D counts' is equivalent to 'counts' times 'D counts'. Let's erase the previous calculation and verify the new one. The verification looks good, confirming the 'norm logits' calculation. We are now at the line that computes 'norm logits' and want to back propagate through it to calculate the derivatives of logits and logit Maxes. The shapes are not the same, leading to implicit broadcasting. 'Norm logits' and 'logits' have shapes of 32 by 27, but 'logit Maxes' is 32 by 1. In the subtraction, every element of 'C' (result) comes from the corresponding element of 'A' minus the associated element of 'B'. The derivatives of each element of 'C' with respect to the inputs are one for 'A' and negative one for 'B'. Thus, the derivatives on 'C' flow to the corresponding 'A's and 'B's. The 'B's are broadcast, requiring an additional sum, similar to previous operations. The derivatives for 'B's undergo a minus sign because the local derivative is negative one. We implement this by setting 'delogits' to a copy of 'D norm logits' using 'clone'. The 'loaded Maxes' will be the negative of 'D norm logits', due to the negative sign. 'Logit Maxes' is a column and is replicated. So, we do a sum along axis one with 'keepdims=True' to avoid dimension destruction. This makes 'logic Maxes' the same shape. This 'delogits' is not final, because we have a second gradient signal coming into 'logits'. For now, the 'logic Maxes' is the final derivative. Let's verify that the 'logit Maxes' derivative matches PyTorch's. PyTorch agrees with the 'logit Maxes' gradient. This was the derivative through this line. Now, let's examine 'logic Maxes' and their gradients. This is primarily for the numerical stability of softmax. If you add or subtract any value equally to all elements of the logits row, it won’t change softmax value. This ensures that values do not overflow and that the highest value in each logits row is zero, which is numerically safe. Therefore, changing 'logit Maxes' shouldn't change the probs, the loss or its gradient. These values should be zero. Due to floating-point issues, the values are very small, but not exactly zero, like 1e-9 or 1e-10. This reflects that 'logit Maxes' doesn't impact the loss. It may seem unusual to backpropagate through this branch since cross-entropy implementations typically skip this branch for numerical stability. However, even with a breakdown into full atoms, the computation for numerical stability yields the correct outcome with extremely small gradients, demonstrating that the values of these do not matter with respect to the final loss. Now, we proceed with back propagation through this line. We have the 'logit Maxes' derivative, and we now back propagate into logits through the second branch. In PyTorch, the max operation returns both the maximum values and the indices at which they occurred. In the backward pass, knowing where the maximum values came from is useful for back propagation. The derivative flowing through this should be one for the appropriate entry that was plucked out, times the derivative of the 'logit Maxes'. We take the derivative of 'logit Maxes' and scatter it to the correct positions in the 'logits' where the maximum values came from. One way to do this is with a one-hot vector. The 'max' over the first dimension’s indices are converted into one-hot vectors with a dimension of 27. This one-hot vector is then multiplied by the 'logit Maxes', which is a 32 by 1 column. Broadcasting replicates this column, and an element-wise multiplication routes the 'logit Maxes' to the correct position in 'logits'. We are using plus equals because we already calculated the 'logits' derivative, and this is a second branch. Let's verify that the new 'logits' derivative is correct. We have the correct answer. Next, we continue with logits, an outcome of a matrix multiplication and a bias offset. Logits is 32 by 27, H is 32 by 64, and W projects the 64-dimensional vector to 27 dimensions. There is a 27-dimensional bias. The plus operation here broadcasts. The 27-dimensional bias is padded to the left with a dimension of one, making it a row vector replicated vertically 32 times, resulting in a 32 by 27 matrix for element-wise multiplication. We need to back propagate from 'logits' to the hidden states, the weight matrix, and the bias. We can derive the derivative using first principles by working through a small example. We have A multiplied by B plus C resulting in D and the loss derivative with respect to D. We need to determine the derivative of the loss with respect to A, B, and C. Consider a simple two-dimensional example of a matrix multiplication of a two-by-two matrix with another two-by-two matrix, plus a two element bias vector. The bias vector is replicated vertically. By breaking down the matrix multiplication and examining how each element of D is calculated from its components, we can derive the formulas for the gradients. d11 is the result of a dot product between the first row of a and the first column of b. It is obvious that the derivatives for each element of A can be found by differentiating the simple formulas. For example, we want to find the derivative of the loss with respect to a11. We see that a11 appears twice in our expression and influences d11 and d12. Therefore, the derivative of the loss with respect to a11 is the derivative of the loss with respect to d11 times the local derivative of d11 with respect to a11, plus the derivative of the loss with respect to d12 times the local derivative of d12 with respect to a11. The local derivative in the first case is b11 and in the second case, it is b12. We add the contributions of both. We can perform the same analysis for all other elements of A.  The derivative of the loss with respect to matrix A, arranged as the same shape as A, can be expressed as a matrix multiplication of the derivative of the loss with respect to matrix D, times the transpose of B.  If we perform similar analysis for the derivative with respect to matrix B, we find that the derivative of the loss with respect to B is also a matrix multiplication. It is the transpose of matrix A, multiplied by the derivative of the loss with respect to D. For derivatives of vector C with respect to individual elements, the result is the sum of the derivatives of the loss with respect to D across the columns. In summary, the backward path of a matrix multiplication is also a matrix multiplication.  In the scalar case we have D equals A times B plus C, and in the matrix case we have a matrix multiply. The derivative of D with respect to A is the derivative of the loss with respect to D, matrix-multiplied by the transpose of B. The derivative of D with respect to B is the transpose of A multiplied by the derivative of the loss with respect to D. And, the derivative of D with respect to C is the sum of the loss derivative with respect to D across the columns. I often don’t remember those formulas, but I can back propagate using dimensional analysis. For example, if I want to find DH, I know its shape must be the same as the shape of H, which is 32 by 64. I also know DH must result from a matrix multiplication of 'logits' with W2. 'Logits' is 32 by 27, and W2 is 64 by 27. There is only one way for the shapes to work out correctly: the matrix multiplication of delogits and the transpose of W2. Similarly, if we want DW2, which has a shape of 64 by 27, it must come from some matrix multiplication of 'delogits' and H. The only way to get 64 by 27 is the transpose of H multiplied by delogits. Finally, DB2 is the sum, where the only way for the shapes to work out is to sum along the zero axis of delogits. With this method, we do not have to memorize the back propagation formulas. The hacky method of figuring out the matrix multiplications is successful. Now, let’s move the implementation of the backward pass for a linear layer here. We can now uncomment and verify that all three gradients are correct. H, W2, and B2 are correct, so we have back propagated through a linear layer. We have the derivative for H and need to back propagate through the tanH function into H preact. We need to calculate DH preact by back propagating through tanH. We know from micrograd that the derivative of tanH is simple. In particular, if A is equal to tanH of Z, the derivative of A with respect to Z is equal to one minus A squared, where A is the output of the tanH. So the local derivative is 1 minus the output of tanH squared, which is 1 minus H squared in our case. That is the local derivative times the chain rule derivative of H. Let’s come here and see if this gives us the right answer. The result is correct. Now, we have DH preact and need to back propagate to the gain, the B, the raw, and the B and bias of the batch norm. This is an element-wise multiply and must be handled differently than matrix multiplies. The broadcasting behavior of this line of code is important. The B and gain and the B and bias are 1 by 64, while H preact and B and raw are 32 by 64. Let's start with DB and gain. Whenever we have A times B equals C, the local derivative is B. Therefore, the local derivative here is B and raw, and chain rule derivative DH preact. Because B and gain has the shape 1 by 64, but the result is 32 by 64, we sum the rows to maintain the 1 by 64 shape. Therefore, we sum across the examples, which is dimension zero. We also need to keep the dimensions to maintain the tensor shapes. Next is DB and raw, which is B and gain multiplied by DH preact. Here, the shapes work out correctly using broadcasting. Finally, the bias term has a similar behavior. It's the sum of DH preact along the zeroth dimension. We need to sum across the examples, keeping dimensions true, giving us the 1 by 64 result. This is our candidate implementation. Let’s uncomment the check and see if the gradients are correct. All three derivatives are correct. We have completed the back propagation through the batch norm parameters. B and raw is the output of the standardization. Batch norm is broken into manageable pieces for back propagation. We calculate B and mean, the sum of inputs across the examples. B and diff is X minus mu. B and diff squared is x minus mu squared. B and VAR is the variance, the sum of squares. I normalize by 1 over m-1 instead of n to use Bessel’s correction. BNvar inv is one over the square root of variance plus epsilon. Finally, B and raw is equal to B and diff times BNvar inv. We already have back propagated through the line creating H preact, so we have B and raw, and need to back propagate into B and diff and B and VAR M. Now we need to back propagate through the line where B and raw equals B and diff times BN var m. The shapes are as follows: B and VAR m is of shape 1 by 64, which requires broadcasting. It is an element-wise multiplication. To get DB and diff, we multiply B and varm with DB and raw. Conversely, to get DB and VAR inv, we multiply B and diff with DB and raw. Broadcasting needs to be considered. B and VAR m times DB and raw works, but DB and VAR inv is a 32 by 64 result and B and VAR inv is 1 by 64. We need to sum across the examples using keepdim true to keep the shape. Here is the candidate implementation. Let's uncomment these lines. The BMR m derivative is correct, but DB and diff is incorrect, which is expected, because we are not done with it. DB and diff branches into multiple branches and we have only done one of them so far. We still need to come back to the DB and diff. It is good to see the comparison is detecting incorrect gradients. Now we want to back propagate through the line B and VAR inv equals B and VAR plus epsilon to the power of negative point five. The power rule says that the derivative is equal to bringing the exponent down, which is negative 0.5, times x to the power of minus 1.5, in this case the expression to the negative 1.5 power. We need to apply chain rule in our head since the B and VAR with respect to what's inside the brackets in the expression. Because it is an element-wise operation, this derivative is simple.
