We want to use bias, but here you don't want to add it because it's spurious. Instead, we have this batch normalization bias, and that batch normalization bias is now in charge of biasing this distribution instead of the B1 that we had originally. Basically, a batch normalization layer has its own bias, and there's no need to have a bias in the layer before it because that bias is going to be subtracted out anyway. That's another small detail to be careful with. Sometimes, it's not going to do anything catastrophic; this B1 will just be useless. It will never get any gradient, it will not learn, it will stay constant, and it's just wasteful, but it doesn't actually impact anything otherwise.

I rearranged the code a little bit with comments, and I just wanted to give a quick summary of the batch normalization layer. We are using batch normalization to control the statistics of activations in the neural net. It is common to sprinkle batch normalization layers across the neural net, and usually, we will place it after layers that have multiplications, like, for example, a linear layer or a convolutional layer, which we may cover in the future. Now, the batch normalization internally has parameters for the gain and the bias, and these are trained using backpropagation. It also has two buffers: the mean and the standard deviation. These are the running mean and the running mean of the standard deviation, and these are not trained using backpropagation. These are trained using this janky update of a kind of like a running mean update. So, these are sort of the parameters and the buffers of the batch normalization layer. Really what it's doing is it's calculating the mean and a standard deviation of the activations that are feeding into the batch normalization layer over that batch. Then, it's centering that batch to be unit Gaussian, and then it's offsetting and scaling it by the learned bias and gain. On top of that, it's keeping track of the mean and the standard deviation of the inputs, and it's maintaining this running mean and standard deviation. This will later be used at inference so that we don't have to re-estimate the mean and standard deviation all the time. In addition, that allows us to basically forward individual examples at test time. So, that's the batch normalization layer. It's a fairly complicated layer, but this is what it's doing internally.

Now, I wanted to show you a little bit of a real example, so you can search ResNet, which is a residual neural network. These are common types of neural networks used for image classification. Of course, we haven't covered ResNets in detail, so I'm not going to explain all the pieces of it. For now, just note that the image feeds into a ResNet on the top here, and there are many, many layers with repeating structure all the way to predictions of what's inside that image. This repeating structure is made up of these blocks, and these blocks are just sequentially stacked up in this deep neural network. Now, the code for this block that's used and repeated sequentially in series is called this bottleneck block. There's a lot here; this is all PyTorch. Of course, we haven't covered all of it, but I want to point out some small pieces of it. In the `__init__` is where we initialize the neural network, so this code of the block here is basically the kind of stuff we're doing here. We're initializing all the layers, and in the `forward`, we are specifying how the neural network acts once you actually have the input. So, this code here is along the lines of what we're doing here. Now, these blocks are replicated and stacked up serially, and that's what a residual network would be. So, notice what's happening here. `conv1`â€”these are convolution layers, and these convolution layers are basically the same thing as a linear layer, except convolutional layers don't apply. Convolutional layers are used for images, and so they have spatial structure, and basically, this linear multiplication and bias offset are done on patches instead of the full input. So, because these images have spatial structure, convolutions just basically do WX + B, but they do it on overlapping patches of the input. But otherwise, it's WX + B. Then, we have the `norm` layer, which by default here is initialized to be a batch normalization in 2D, so a two-dimensional batch normalization layer, and then we have a non-linearity, like ReLU. So, instead of using tanh, they use ReLU in this case. We are using tanh, but both are just non-linearities, and you can use them relatively interchangeably. For very deep networks, ReLU typically, empirically, works a bit better. So, see the motif that's being repeated here. We have convolution, batch normalization, ReLU, convolution, batch normalization, ReLU, etc. Then, here, this is a residual connection that we haven't covered yet, but basically that's the exact same pattern we have here. We have a weight layer, like a convolution or a linear layer, batch normalization, and then tanh, which is a non-linearity. But basically, a weight layer, a normalization layer, and a non-linearity, and that's the motif that you would be stacking up when you create these deep neural networks, exactly as it's done here.

One more thing I'd like you to notice is that here, when they are initializing the `conv` layers, like `conv1by1`, the depth for that is right here. It's initializing an `nn.Conv2d`, which is a convolution layer in PyTorch, and there are a bunch of keyword arguments here that I'm not going to explain yet. But you see how there's `bias=False`. The `bias=False` is exactly for the same reason as the bias is not used in our case. You see how I erased the use of bias? The use of bias is spurious because after this weight layer, there's a batch normalization, and the batch normalization subtracts that bias and then has its own bias. So, there's no need to introduce these spurious parameters. It wouldn't hurt performance; it's just useless. So, because they have this motif of C, Batch Norm, ReLU, they don't need a bias here because there's a bias inside here. By the way, this example here is very easy to find. Just search ResNet PyTorch, and it's this example here. So, this is kind of like the stock implementation of a residual neural network in PyTorch, and you can find that here. But of course, I haven't covered many of these parts yet.

I would also like to briefly descend into the definitions of these PyTorch layers and the parameters that they take. Now, instead of a convolutional layer, we're going to look at a linear layer because that's the one that we're using here. This is a linear layer, and I haven't covered convolutions yet, but as I mentioned, convolutions are basically linear layers except on patches. A linear layer performs a WX + B, except here, they're calling the W a transpose. So, to calculate WX + B, very much like we did here, to initialize this layer, you need to know the fan-in and the fan-out, and that's so that they can initialize this W. This is the fan-in and the fan-out, so they know how big the weight matrix should be. You also need to pass in whether or not you want a bias. If you set it to `false`, then no bias will be inside this layer, and you may want to do that, exactly like in our case, if your layer is followed by a normalization layer, such as batch normalization. This allows you to basically disable a bias.

Now, in terms of the initialization, if we swing down here, this is reporting the variables used inside this linear layer. Our linear layer here has two parameters, the weight and the bias. In the same way, they have a weight and a bias, and they're talking about how they initialize it by default. So, by default, PyTorch will initialize your weights by taking the fan-in and then doing one over fan-in square root, and then instead of a normal distribution, they are using a uniform distribution. So, it's very much the same thing, but they are using a one instead of five over three, so there's no gain being calculated here; the gain is just one. But otherwise, it's exactly one over the square root of fan-in, exactly as we have here. So, one over the square root of K is the scale of the weights, but when they are drawing the numbers, they're not using a Gaussian by default; they're using a uniform distribution by default. So, they draw uniformly from the negative square root of K to the square root of K, but it's the exact same thing and the same motivation with respect to what we've seen in this lecture. The reason they're doing this is that if you have a roughly Gaussian input, this will ensure that, out of this layer, you will have a roughly Gaussian output, and you basically achieve that by scaling the weights by one over the square root of fan-in. So, that's what this is doing.

The second thing is the batch normalization layer. So, let's look at what that looks like in PyTorch. Here we have a one-dimensional batch normalization layer, exactly as we are using here, and there are a number of keyword arguments going into it as well. So, we need to know the number of features. For us, that is 200, and that is needed so that we can initialize these parameters here: the gain, the bias, and the buffers for the running mean and standard deviation. Then, they need to know the value of epsilon here, and by default, this is 1e-5. You don't typically change this too much. Then, they need to know the momentum, and the momentum here, as they explain, is basically used for these running mean and running standard deviations. So, by default, the momentum here is 0.1. The momentum we are using here in this example is 0.001. Basically, you may want to change this sometimes. Roughly speaking, if you have a very large batch size, then typically what you'll see is that when you estimate the mean and the standard deviation for every single batch size, if it's large enough, you're going to get roughly the same result. Therefore, you can use a slightly higher momentum, like 0.1. But for a batch size as small as 32, the mean and standard deviation here might take on slightly different numbers because there are only 32 examples we are using to estimate the mean and standard deviation, so the value is changing around a lot. If your momentum is 0.1, that might not be good enough for this value to settle and converge to the actual mean and standard deviation over the entire training set. So, basically, if your batch size is very small, a momentum of 0.1 is potentially dangerous, and it might make it so that the running mean and standard deviation are thrashing too much during training and it's not actually converging properly.

`affine=True` determines whether this batch normalization layer has these learnable affine parameters: the gain and the bias. This is almost always kept to `true`. I'm not actually sure why you would want to change this to `false`. Then, `track_running_stats` is determining whether or not the batch normalization layer of PyTorch will be doing this. One reason you may want to skip the running stats is because you may want to, for example, estimate them at the end as a stage two. In that case, you don't want the batch normalization layer to be doing all this extra compute that you're not going to use. Finally, we need to know which device we're going to run this batch normalization on, a CPU or a GPU, and what the data type should be: half precision, single precision, double precision, and so on.

So, that's the batch normalization layer. Otherwise, the link to the paper is the same formula we've implemented, and everything is the same, exactly as we've done here. Okay, so that's everything that I wanted to cover for this lecture. Really what I wanted to talk about is the importance of understanding the activations and the gradients and their statistics in neural networks. This becomes increasingly important, especially as you make your neural networks bigger, larger, and deeper. We looked at the distributions basically at the output layer, and we saw that if you have two confident mispredictions because the activations are too messed up at the last layer, you can end up with these hockey stick losses. If you fix this, you get a better loss at the end of training because your training is not doing wasteful work. Then, we also saw that we need to control the activations. We don't want them to squash to zero or explode to infinity because you can run into a lot of trouble with all of these non-linearities and these neural nets. Basically, you want everything to be fairly homogeneous throughout the neural net. You want roughly Gaussian activations throughout the neural net. Then, we talked about: if we want roughly Gaussian activations, how do we scale these weight matrices and biases during initialization of the neural network so that we don't get, you know, so everything is as controlled as possible? This gave us a large boost in improvement.

Then, I talked about how that strategy is not actually possible for much, much deeper neural networks because when you have much deeper neural networks with lots of different types of layers, it becomes really, really hard to precisely set the weights and the biases in such a way that the activations are roughly uniform throughout the neural network. So then, I introduced the notion of a normalization layer. Now, there are many normalization layers that people use in practice: batch normalization, layer normalization, instance normalization, group normalization. We haven't covered most of them, but I've introduced the first one, and also the one that I believe came out first, and that's called batch normalization. We saw how batch normalization works. This is a layer that you can sprinkle throughout your deep neural net, and the basic idea is that if you want roughly Gaussian activations, then take your activations and take the mean and the standard deviation and center your data, and you can do that because the centering operation is differentiable. On top of that, we actually had to add a lot of bells and whistles, and that gave you a sense of the complexities of the batch normalization layer because now we're centering the data; that's great, but suddenly we need the gain and the bias, and now those are trainable. Then, because we are coupling all of the training examples, suddenly the question is how do you do the inference? To do the inference, we need to now estimate these mean and standard deviations over the entire training set and then use those at inference. But then, no one likes to do stage two, so instead, we fold everything into the batch normalization layer during training and try to estimate these in a running manner so that everything is a bit simpler, and that gives us the batch normalization layer. As I mentioned, no one likes this layer. It causes a huge amount of bugs. Intuitively, it's because it is coupling examples in the forward pass of a neural network, and I've shot myself in the foot with this layer over and over again in my life, and I don't want you to suffer the same. Basically, try to avoid it as much as possible. Some of the other alternatives to these layers are, for example, group normalization or layer normalization, and those have become more common in more recent deep learning. But we haven't covered those yet. But definitely batch normalization was very influential at the time when it came out in roughly 2015 because it was kind of the first time that you could train reliably much deeper neural networks, and fundamentally, the reason for that is because this layer was very effective at controlling the statistics of the activations in the neural network. So, that's the story so far, and that's all I wanted to cover. In the future lectures, hopefully, we can start going into recurrent neural networks, and recurring neural networks, as we'll see, are just very, very deep networks because you unroll the loop, and when you actually optimize these neural networks, that's where a lot of this analysis around the activation statistics and all these normalization layers will become very, very important for good performance. So, we'll see that next time. Bye.

Okay, so I lied. I would like us to do one more summary here as a bonus, and I think it's useful to have one more summary of everything I've presented in this lecture. But also, I would like us to start to Pytorch-ify our code a little bit so it looks much more like what you would encounter in PyTorch. You'll see that I will structure our code into these modules like a `Lin` module and a `BatchNorm` module, and I'm putting the code inside these modules so that we can construct neural networks very much like we would construct them in PyTorch. I will go through this in detail. So, we'll create our neural net, then we will do the optimization loop as we did before, and then one more thing that I want to do here is I want to look at the activation statistics, both in the forward pass and in the backward pass. Then here we have the evaluation and sampling just like before.

Let me rewind all the way up here and go a little bit slower. Here, I am creating a linear layer. You'll notice that `torch.nn` has lots of different types of layers, and one of those layers is the linear layer. `torch.nn.Linear` takes a number of input features, output features, whether or not we should have a bias, and then the device that we want to place this layer on and the data type. I will emit these two, but otherwise, we have the exact same thing. We have the fan-in, which is the number of inputs, fan-out, the number of outputs, and whether or not we want to use a bias. Internally, inside this layer, there's a weight and a bias if you'd like it. It is typical to initialize the weight using random numbers drawn from a Gaussian, and then here is the Kaiming initialization that we discussed already in this lecture, and that's a good default, and also the default that I believe PyTorch chooses. By default, the bias is usually initialized to zeros. Now, when you call this module, this will basically calculate W * X + B if you have a B, and then when you also call `parameters` on this module, it will return the tensors that are the parameters of this layer.

Next, we have the batch normalization layer. I've written that here, and this is very similar to PyTorch's `nn.BatchNorm1d` layer, as shown here. I'm kind of taking these three parameters here: the dimensionality, the epsilon that we will use in the division, and the momentum that we will use in keeping track of these running stats, the running mean, and the running variance. PyTorch actually takes quite a few more things, but I'm assuming some of their settings. So, for us, affine will be true, which means that we will be using a gamma and beta after the normalization. `track_running_stats` will be true, so we will be keeping track of the running mean and the running variance in the batch norm. Our device by default is the CPU, and the data type by default is float32. So, those are the defaults; otherwise, we are taking all the same parameters in this batch norm layer. First, I'm just saving them. Now, here's something new: there's a `dot_training`, which by default is true. PyTorch `nn.Module` also has this `training` attribute, and that's because many modules, and batch norm is included in that, have a different behavior whether you are training your network or whether you are running it in an evaluation mode and calculating your evaluation loss or using it for inference on some test examples. Batch norm is an example of this because when we are training, we are going to be using the mean and the variance estimated from the current batch, but during inference, we are using the running mean and running variance. Also, if we are training, we are updating the mean and variance, but if we are testing, then these are not being updated; they're kept fixed. So, this flag is necessary, and by default, it is true, just like in PyTorch. Now, the parameters of batch norm 1D are the gamma and the beta here, and then the running mean and running variance are called buffers in PyTorch nomenclature. These buffers are trained using an exponential moving average here explicitly, and they are not part of the backpropagation and stochastic gradient descent. So, they are not sort of like parameters of this layer, and that's why when we call parameters here, we only return gamma and beta. We do not return the mean and the variance. This is trained sort of like internally here every forward pass using the exponential moving average. So, that's the initialization.

Now, in a forward pass, if we are training, then we use the mean and the variance estimated by the batch. Let me pull up the paper here. We calculate the mean and the variance. Now, up above, I was estimating the standard deviation and keeping track of the standard deviation here in the running standard deviation instead of the running variance. But let's follow the paper exactly. Here, they calculate the variance, which is the standard deviation squared, and that's what gets tracked as a running variance instead of a running standard deviation, but those two would be very, very similar, I believe. If we are not training, then we use the running mean and variance. We normalize, and then here, I am calculating the output of this layer, and I'm also assigning it to an attribute called `out`. Now, `out` is something that I'm using in our modules here. This is not what you would find in PyTorch. We are slightly deviating from it. I'm creating a `dot_out` because I would like to very easily maintain all those variables so that we can create statistics of them and plot them. But PyTorch nn modules will not have a `dot_out` attribute. Finally, here we are updating the buffers using, again, as I mentioned, the exponential moving average given the provided momentum. Importantly, you'll notice that I'm using the `torch.no_grad` context manager, and I'm doing this because if we don't use this, then PyTorch will start building out an entire computational graph out of these tensors because it is expecting that we will eventually call `dot_backward`, but we are never going to be calling `dot_backward` on anything that includes the running mean and running variance. So, that's why we need to use this context manager so that we are not sort of maintaining them using all this additional memory. So, this will make it more efficient, and it's just telling PyTorch that there will be no backward. We just have a bunch of tensors, and we want to update them; that's it. Then, we return.

Okay, now scrolling down, we have the tanh layer. This is very, very similar to `torch.tanh`, and it doesn't do too much; it just calculates tanh as you might expect. So, that's `torch.tanh`, and there are no parameters in this layer, but because these are layers, it now becomes very easy to sort of like stack them up into basically just a list, and we can do all the initializations that we're used to. So, we have the initial sort of embedding matrix, we have our layers, and we can call them sequentially, and then again, with torch no grad. But there are some initializations here. So, we want to make the output softmax a bit less confident, like we saw, and in addition to that, because we are using a six-layer multilayer perceptron here, so you see how I'm stacking linear, tanh, linear, tanh, etc., I'm going to be using the gain here, and I'm going to play with this in a second, so you'll see how, when we change this, what happens to the statistics. Finally, the parameters are basically the embedding matrix and all the parameters in all the layers, and notice here, I'm using a double list comprehension, if you want to call it that. But for every layer in layers and for every parameter in each of those layers, we are just stacking up all those pieces, all those parameters. In total, we have 46,000 parameters, and I'm telling PyTorch that all of them require a gradient. Then here, we have everything here we are actually mostly used to. We are sampling a batch, we are doing a forward pass. The forward pass now is just the linear application of all the layers in order followed by the cross-entropy. Then, in the backward pass, you'll notice that for every single layer, I now iterate over all the outputs, and I'm telling PyTorch to retain the gradient of them. Then here, we are already used to all the gradient set to none, do the backward to fill in the gradients, do an update using stochastic gradient descent, and then track some statistics, and then I am going to break after a single iteration.

Now, here in this cell, in this diagram, I'm visualizing the histogram, the histograms of the forward pass activations, and I'm specifically doing it at the tanh layers. So, I'm iterating over all the layers except for the very last one, which is basically just the soft max layer. If it is a tanh layer, and I'm using a tanh layer just because they have a finite output of -1 to 1, and so it's very easy to visualize here, so you see -1 to 1, and it's a finite range and easy to work with. I take the `out` tensor from that layer into T, and then I'm calculating the mean, the standard deviation, and the percent saturation of T. The way I define the percent saturation is that `t.absolute value` is greater than .97. So, that means we are here at the tails of the tanh. Remember that when we are in the tails of the tanh, that will actually stop the gradients, so we don't want this to be too high. Now here, I'm calling `torch.histogram`, and then I am plotting this histogram. So, basically, what this is doing is that every different type of layer, and they have a different color, we are looking at how many values in these tensors take on any of the values below on this axis here. So, the first layer is fairly saturated here at 20%, so you can see that it's got tails here. But then everything sort of stabilizes, and if we had more layers here, it would actually just stabilize at around a standard deviation of about .65, and the saturation would be roughly 5%. The reason that this stabilizes and gives us a nice distribution here is because the gain is set to 5 over 3. Now, this gain, you see that by default, we initialize with 1 / the square root of fan-in, but then here during initialization, I come in and I iterate over all the layers, and if it's a linear layer, I boost that by the gain. We saw previously that if we just do not use a gain, then what happens if I redraw this, you will see that the standard deviation is shrinking, and the saturation is coming to zero, and basically, what's happening is the first layer is pretty decent, but then the further layers are just kind of like shrinking down to zero, and it's happening slowly, but it's shrinking to zero. The reason for that is when you just have a sandwich of linear layers alone, then initializing our weights in this manner, we saw previously, would have conserved the standard deviation of one. But because we have these interspersed tanh layers in there, these tanh layers are squashing functions, and so they take your distribution and they slightly squash it. Some gain is necessary to keep expanding it, to fight the squashing. It just turns out that 5 over 3 is a good value. If we have something too small, like one, we saw that things will come toward zero, but if it's something too high, let's do two, then here, we see that, well, let me do something a bit more extreme so it's a bit more visible. Let's try three. Okay, so we see here that the saturations are going to be way too large. Okay, so three would create way too saturated activations, so 5 over 3 is a good setting for a sandwich of linear layers with tanh activations, and it roughly stabilizes the standard deviation at a reasonable point.

Honestly, I have no idea where 5 over 3 came from in PyTorch. When we were looking at the Kaiming initialization, I see empirically that it stabilizes this sandwich of linear and tanh, and that the saturation is in a good range, but I don't actually know if this came out of some math formula. I tried searching briefly for where this comes from, but I wasn't able to find anything. But certainly, we see that empirically, these are very nice ranges. Our saturation is roughly 5%, which is a pretty good number, and this is a good setting of the gain in this context. Similarly, we can do the exact same thing with the gradients. So, here is a very same loop, if it's a tanh, but instead of taking a layer `dot_out`, I'm taking the `grad`, and then I'm also showing the mean and the standard deviation, and I'm plotting the histogram of these values, and so you'll see that the gradient distribution is fairly reasonable. In particular, what we're looking for is that all the different layers in this sandwich have roughly the same gradient. Things are not shrinking or exploding. So, we can, for example, come here, and we can take a look at what happens if this gain was way too small, so this was 0.5. Then you see, first of all, the activations are shrinking to zero, but also, the gradients are doing something weird. The gradients started out here, and then now they're like expanding out. Similarly, if we, for example, have a too high of a gain, like three, then we see that also the gradients have some asymmetry going on, where as you go into deeper and deeper layers, the activation distributions are changing, and so that's not what we want. In this case, we saw that without the use of batch norm, as we are going through right now, we had to very carefully set those gains to get nice activations in both the forward pass and the backward pass. Now, before we move on to batch normalization, I would also like to take a look at what happens when we have no tanh units here, so erasing all the tanh non-linearities, but keeping the gain at 5 over 3. We now have just a giant linear sandwich. So, let's see what happens to the activations. As we saw before, the correct gain here is one. That is the standard deviation preserving gain. So, 1.667 is too high, and so what's going to happen now is the following: I have to change this to be linear, because there's no more tanh.
