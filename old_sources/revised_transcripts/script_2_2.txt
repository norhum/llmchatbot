It is not easy and not very well documented, so you have to be careful. We want integers because that makes sense. Lowercase `tensor` is what we are using. Now, we want to think through how we're going to feed in these examples into a neural network. It's not quite as straightforward as plugging it in because these examples are integers, like 0, 5, or 13, giving us the index of the character. You can't just plug an integer index into a neural net. Neural nets are made up of neurons with weights. As you saw in micrograd, these weights act multiplicatively on the inputs, `w*x + b`. It doesn't make sense to have an input neuron take on integer values and then multiply with weights. Instead, a common way of encoding integers is one-hot encoding. In one-hot encoding, we take an integer like 13 and create a vector that is all zeros except for the 13th dimension, which is set to one. This vector can then feed into a neural net. Conveniently, PyTorch has a `one_hot` function inside `torch.nn.functional`. It takes a tensor of integers, where `long` is an integer type, and the number of classes, which is how large you want your vector to be. Let’s import `torch.nn.functional as F`. Then, let’s do `F.one_hot`, feeding in the integers we want to encode, the entire array of `x`s, and specifying `num_classes` as 27. This ensures that it does not guess incorrectly that the max index is 13. We will call this `x_enc` for x-encoded.  The shape of `x_enc` is 5 by 27. We can also visualize it with `plt.imshow(x_enc)` to make it a little clearer. We see that we've encoded all five examples into vectors. We have five examples, so we have five rows, and each row is now an example. The appropriate bit is turned on as a one, and everything else is zero. For example, the zeroth bit is turned on, the fifth bit is turned on, and the 13th bit is turned on for both of these examples, and then the first bit here is turned on. That's how we can encode integers into vectors, and these vectors can then feed into neural nets. One more issue to be careful with is the data type of encoding. We always want to be careful with data types. When plugging numbers into neural nets, we don't want integers; we want floating-point numbers that can take on various values. But, the data type here is actually 64-bit integer.  I suspect the reason is that `one_hot` received a 64-bit integer and returned the same data type. When you look at the signature of `one_hot`, it does not take a desired data type of the output tensor. In many functions in torch, we could do something like `dtype=torch.float32`, which is what we want, but `one_hot` does not support that. Instead, we want to cast this to float, like this, so that everything looks the same, but the `dtype` is `float32`.  Floats can feed into neural nets. Now, let's construct our first neuron. This neuron will look at these input vectors and perform a very simple function, `w*x + b`, where `w*x` is a dot product. We can achieve the same thing here. Let’s first define the weights of this neuron, what are the initial weights at initialization for this neuron. Let’s initialize them with `torch.randn`, which fills a tensor with random numbers drawn from a normal distribution. A normal distribution has a probability density function like this. Most of the numbers drawn from this distribution will be around zero, but some will be as high as almost three. Very few numbers will be above three in magnitude. We need to take a size as an input here, and I'm going to use size as 27 by 1. Let’s visualize `w`. `w` is a column vector of 27 numbers. These weights are then multiplied by the inputs.  To perform this multiplication, we can take `x_enc` and multiply it with `w`.  This is a matrix multiplication operator in PyTorch. The output is 5 by 1. The reason is that we took `x_enc`, which is 5 by 27, and multiplied it by 27 by 1. In matrix multiplication, the output will become 5 by 1 because the 27s multiply and add. We are seeing the five activations of this neuron on these five inputs, and we've evaluated all of them in parallel. We didn't feed in a single input; we fed in all five inputs into the same neuron. In parallel, PyTorch evaluated `w*x + b`, but here is just `w*x`; there’s no bias. It is `w` times `x` for all of them independently. Instead of a single neuron, I would like to have 27 neurons. I will show you why in a second. Instead of having just a 1 here, which indicates the presence of one single neuron, we can use 27. Then, when `w` is 27 by 27, this will, in parallel, evaluate all the 27 neurons on all the 5 inputs, giving us a much bigger result.  Now what we’ve done is 5 by 27 multiplied by 27 by 27. The output of this is now 5 by 27. What is every element here telling us? For every one of the 27 neurons, it's telling us the firing rate of those neurons on every one of those five examples. The element at 3, 13 is giving us the firing rate of the 13th neuron looking at the third input. The way this was achieved is by a dot product between the third input and the 13th column of the `w` matrix. Using matrix multiplication, we can very efficiently evaluate the dot product between lots of input examples in a batch and lots of neurons, where all those neurons have weights in the columns of those `w`s. In matrix multiplication, we're just doing those dot products in parallel.  To show you that this is the case, we can take `x`, take the third row, take `w`, and take its 13th column. Then, we can do `x[3]` element-wise multiplied with `w[:,13]` and sum that up, which is `w*x`, a dot product. That's the same number. You see that this is being done efficiently by the matrix multiplication operation for all the input examples and all the output neurons of this first layer. We fed our 27-dimensional inputs into a first layer of a neural net that has 27 neurons. We have 27 inputs and 27 neurons. These neurons perform `w*x`. They don't have a bias, and they don't have a non-linearity like `tanh`. We're going to leave them as a linear layer. In addition, we're not going to have any other layers. It's going to be the dumbest, simplest neural net, which is just a single linear layer. Now, I'd like to explain what I want those 27 outputs to be, intuitively. We're trying to produce some kind of probability distribution for the next character in a sequence. There are 27 of them, but we have to come up with precise semantics for how we're going to interpret these 27 numbers. Intuitively, you see that these numbers are negative and some of them are positive, because these are coming out of a neural net layer initialized with these normal distribution parameters.  We want something like we had here, where each row told us the counts, and we normalized them to get probabilities. We want something similar to come out of the neural net.  What we have right now are some negative and positive numbers. We want those numbers to somehow represent the probabilities for the next character. Probabilities have a special structure; they are positive numbers and sum to one. That doesn't just come out of a neural net. They can't be counts because these counts are positive integers. Counts are not really a good thing to output from a neural net.  Instead, the neural net will output 27 numbers that we are going to interpret as log counts. Instead of giving us counts directly, like in this table, they are giving us log counts. To get the counts, we're going to take the log counts and exponentiate them. Exponentiation takes the following form: it takes numbers that are negative or positive, it takes the entire real line. If you plug in negative numbers, you're going to get `e^x`, which is always below one. You get numbers lower than one. If you plug in numbers greater than zero, you're getting numbers greater than one, all the way to infinity. This part here grows to zero. We're going to take these numbers here, and instead of them being positive and negative, we're going to interpret them as log counts. Then, we're going to element-wise exponentiate these numbers. Exponentiating them gives us something like this. All the negative numbers turned into numbers below one, like 0.338.  All the positive numbers turned into even more positive numbers, greater than one. Seven is some positive number over here that is greater than zero, but exponentiated outputs give us something that we can use and interpret as the equivalent of counts. The neural net is kind of predicting counts. These counts are positive numbers, they can never be below zero, and they can take on various values depending on the settings of `w`. We're going to interpret these to be log counts, often called logits.  These will be the counts, log counts exponentiated. This is equivalent to the `n` matrix, the array of counts we used previously. This was the `n` matrix, the array of counts, and each row here are the counts for the next character. The probabilities are just the counts normalized.  I'm not going to scroll all over the place. We've already done this. We want the counts that sum along the first dimension, and we want to keep them as true. We've went over this, and this is how we normalize the rows of our counts matrix to get our probabilities, `probs`. These are now the probabilities, and these are the counts. When I show the probabilities, every row will sum to one because they are normalized. The shape of this is 5 by 27. For every one of our five examples, we now have a row that came out of a neural net. Because of the transformations here, we made sure that this output of this neural net are probabilities. Our `w*x` here gave us logits, then we interpreted those to be log counts. We exponentiate to get something that looks like counts, and then we normalize those counts to get a probability distribution. All of these are differentiable operations, so we can back propagate through. We're getting out probability distributions. For the zeroth example, which was a one-hot vector of zero, corresponding to feeding in this example here, a dot into a neural net, we first got its index, then one-hot encoded it. It went into the neural net, and out came this distribution of probabilities. Its shape is 27. We're going to interpret this as the neural net's assignment for how likely every one of these 27 characters are to come next. As we tune the weights `w`, we'll get different probabilities out for any character you input. The question is, can we optimize and find a good `w` such that the probabilities coming out are good?  We measure "pretty good" using the loss function. Here is a summary. We start with the input data set. We have inputs to the neural net and labels for the correct next character in a sequence. These are integers. I’m using torch generators so you see the same numbers as I do. I’m generating 27 neurons weights, and each neuron receives 27 inputs.  We plug in all the input examples, `x`s, into the neural net. This is a forward pass. First, we have to encode all the inputs into one-hot representations. We have 27 classes. We pass in these integers, and `x_enc` becomes an array that is 5 by 27, with zeros except for a few ones. We then multiply this in the first layer of a neural net to get logits, exponentiate the logits to get fake counts, and normalize these counts to get probabilities. These last two lines are called the softmax, which takes these `z`s, which are logits, exponentiates them, and divides and normalizes them. It's a way of taking outputs of a neural net layer. These outputs can be positive or negative, and it outputs probability distributions. It outputs something that sums to one and are positive numbers, just like probabilities. It's a normalization function. You can put it on top of any other linear layer inside a neural net, and it makes a neural net output probabilities. It is very often used, and we used it as well here. That is the forward pass.  That's how we made a neural net output a probability.  Notice that the entire forward pass is made up of differentiable layers. Everything here we can back propagate through. We saw some back propagation in micrograd. This is multiplication and addition. Exponentiation we know how to back propagate through. We are summing and sum is easily back-propagable, and division as well. Everything here is a differentiable operation, and we can back propagate through it.  We achieve these probabilities, which are 5 by 27. For every single example, we have a vector of probabilities that sums to one. We have five examples making up "emma." There are five bigrams inside "emma."  Bigram example 1 is that `e` is the beginning character after a dot. The indices are 0 and 5. We feed in a 0, that is the input of the neural net. We get probabilities that are 27 numbers. The label is 5, because `e` comes after the dot.  We use this label 5 to index into the probability distribution. Index 5 here, `0 1 2 3 4 5`, is the number here. That's the probability assigned by the neural net to the actual correct character. The network thinks that the next character `e` following the dot is only one percent likely. That's not good because it is a training example. The log likelihood is very negative, and the negative log likelihood is very positive. Four is a very high negative log likelihood, which means we're going to have a high loss, because the loss is the average negative log likelihood. The second character is `em`. The network thought that `m` following `e` is very unlikely, one percent. For `m` following `m` it was two percent, and for `a` following `m` it was seven percent likely. Finally, it thought that the last one was one percent likely. The average negative log likelihood, which is the loss, is 3.76, which is a high loss. That is not a good setting of `w`s. We're getting 3.76. We can change `w` and resample it. I'll add one to have a different seed. We get a different `w`. Rerunning this, with this different setting of `w`s, we get 3.37. This is a much better `w` because the probabilities come out higher for the characters that actually are next.  We can resample. Two is not very good, let's try three. This was terrible with a very high loss.  What I'm doing here is guess-and-check, randomly assigning parameters and seeing if the network is good. That is amateur hour. That's not how you optimize a neural net. We start with some random guess, and we will commit to this one, even though it's not good.  The big deal is, we have a loss function. This loss is made of differentiable operations. We can minimize it by tuning `w` by computing the gradients of the loss with respect to these `w` matrices. Then we can tune `w` to minimize the loss and find a good setting of `w` using gradient-based optimization. Now, things are going to look almost identical to what we had in micrograd. I pulled up the lecture from micrograd, the notebook from this repository. When I scroll to the end, we had something very similar. We had input examples, four input examples inside `xs`, and their targets. Just like here, we have our `xs`, but we have five of them, and they're integers instead of vectors, but we will convert our integers to vectors. Our vectors will be 27 large instead of 3 large. First, we did a forward pass, where we ran a neural net on all the inputs to get predictions. Our neural net at the time, `nfx`, was a multi-layer perceptron. Our neural net is going to look different because our neural net is just a single linear layer followed by a softmax. The loss was the mean squared error. We subtracted the prediction from the ground truth, squared it, and summed it up. That was the loss, a single number summarizing the quality of the neural net. When loss is low, almost zero, the neural net is predicting correctly.  We had a single number summarizing the performance, and everything was differentiable and was stored in a massive compute graph. We iterated over all the parameters, we set the gradients to zero, and we called `loss.backward`. This initiated back propagation at the final output node of loss. We start back propagation and go all the way back, populating all the `parameter.grad`. The graph started at zero, but back propagation filled it in.  In the update, we iterated over all the parameters, and we did a parameter update where every single element of our parameters was nudged in the opposite direction of the gradient. We're going to do the exact same thing here. I'll pull this up on the side, and we will do the exact same thing. This was the forward pass where we did this, and `probs` is our `y_pred`. Now, we evaluate the loss. We're not using mean squared error; we are using the negative log likelihood because we are doing classification, not regression. We calculate the loss, it's this average negative log likelihood. The shape of `probs` is 5 by 27. We want to pluck out the probabilities at the correct indices.  The labels are stored in the array `ys`. We are after the probability at index five for the first example. For the second example, we are interested in the probability assigned to index 13. The second example, also at 13. At the third, it’s one, and at the last row, which is four, we want zero. These are the probabilities we're interested in. They're not amazing as we saw above. These are the probabilities we want.  We can create that using `torch.arange(5)`.  So we index here with `torch.arange(5)` and here we index with `ys`.  That gives us exactly these numbers, plucking out the probabilities of the neural network assigning to the correct next character.  We look at the log probability with `torch.log`. Then we take the mean, and the negative mean log likelihood is the loss. The loss is 3.7 something. This loss, 3.76, is exactly what we obtained before, but this is a vectorized form of that expression. We get the same loss, and the same loss we consider as part of the forward pass, so we have achieved the loss. We made our way to loss.  We defined the forward pass, we forward the network, and now we’re ready for the backward pass. First we make sure all the gradients are reset to zero. In PyTorch, you can set the gradients to zero, or you can set it to `None`, which is more efficient. PyTorch interprets `None` as a lack of gradient, which is the same as zero. This is a way to set the gradient to zero. Now, we do `loss.backward()`. Before that, we need one more thing.  PyTorch requires that we set `requires_grad = True` so that when we tell PyTorch that we are interested in calculating gradients for this leaf tensor. By default, this is false. Let me recalculate with that, and set to `None`.  Then, call `loss.backward()`. Something magical happened when `loss.backward` was run. PyTorch, just like micrograd, when we did the forward pass here, it keeps track of all the operations.  It builds a computational graph. Those graphs exist inside PyTorch, so it knows all the dependencies and all the mathematical operations. When you calculate the loss, we call `backward` on it, which fills in the gradients of all the intermediates back to `w`s, which are the parameters of our neural net. Now we can do `w.grad`, and we see that it has a structure with values inside. These gradients, every single element, have a shape of 27 by 27. Every element of `w.grad` tells us the influence of that weight on the loss function.  For example, this number here, the zero-zero element of `w`, because the gradient is positive, tells us that this has a positive influence on the loss. Slightly nudging the `w[0][0]`, adding a small h to it, would increase the loss mildly because this gradient is positive. Some of these gradients are negative. We can use the gradient information to update the weights of this neural network. Let's now do the update. It’s going to be very similar to what we had in micrograd. We need a loop over the parameters because we only have one parameter tensor, `w`. We do `w.data += -0.1 * w.grad`, and that updates the tensor.  Because the tensor is updated, we would expect that now the loss should decrease. The printed loss was 3.76. We’ve updated `w`. If I recalculate forward pass and loss, the loss should be slightly lower, so 3.76 goes to 3.74. We can set the grad to `None`, backward, update, and now the parameters changed again. If we recalculate the forward pass, we expect a lower loss again, 3.72. We are now doing gradient descent. When we achieve a low loss, the network assigns high probabilities to the correct characters. I rearranged everything and put it together from scratch. Here is where we construct our data set of bigrams. We are iterating only on the first word, “emma.” I will change that in a second.  I added a number that counts the elements in the `x`s, so that we explicitly see that the number of examples is five, because currently, we're working with “emma,” and there are five bigrams. I added a loop with what we had before. Ten iterations of gradient descent, of forward pass, backward pass, and an update. Running the initialization and gradient descent gives us some improvement on the loss function.  Now I want to use all the words. There are not five, but 228,000 bigrams. This should require no modification whatsoever; everything should just run because all the code we wrote doesn't care if there are five or 228,000 bigrams. You see that this will just run, but now we are optimizing over the entire training set of all the bigrams. We are decreasing slightly. We can afford a larger learning rate.  A learning rate of 50 seems to work on this example. Let me re-initialize and let's run 100 iterations. We seem to be getting good losses here, 2.47. Let me run 100 more.  What do we expect the loss to be? We expect something around what we had originally. In the beginning of this video when we optimized by counting, our loss was roughly 2.47 after smoothing. Before smoothing we had a loss of roughly 2.45. That's roughly what we expect to achieve. We achieved it by counting, and here we are achieving roughly the same result with gradient-based optimization. We come to about 2.46, 2.45. That makes sense because, fundamentally, we're not taking any additional information. We are still just taking the previous character and trying to predict the next one. Instead of doing it explicitly by counting and normalizing, we are doing it with gradient-based learning. The explicit approach happens to optimize the loss function very well without gradient-based optimization because the setup for bigram language models is so simple. We can afford to estimate the probabilities directly and maintain them in a table. The gradient-based approach is significantly more flexible. We've gained a lot because we can now expand this approach and complexify the neural net. We are taking a single character and feeding it into a neural net.
