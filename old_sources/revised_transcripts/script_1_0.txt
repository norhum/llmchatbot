Hello, my name is Andre, and I have been training deep neural networks for a bit more than a decade. In this lecture, I would like to show you what neural network training looks like under the hood. In particular, we are going to start with a blank Jupyter notebook, and by the end of this lecture, we will define and train a neural net. You will get to see everything that goes on under the hood and exactly how that works on an intuitive level. Specifically, I would like to take you through the building of Micrograd. Micrograd is a library that I released on GitHub about two years ago. At the time, I only uploaded the source code, and you would have to go in by yourself and figure out how it works. In this lecture, I will take you through it step by step and comment on all the pieces of it. What is Micrograd, and why is it interesting? Micrograd is basically an autograd engine. Autograd is short for automatic gradient, and really, it implements backpropagation. Backpropagation is an algorithm that allows you to efficiently evaluate the gradient of a loss function with respect to the weights of a neural network. What that allows us to do is iteratively tune the weights of that neural network to minimize the loss function and therefore improve the accuracy of the network. Backpropagation would be at the mathematical core of any modern deep neural network library, like PyTorch or Jax. The functionality of Micrograd is best illustrated by an example. If we just scroll down here, you will see that Micrograd basically allows you to build out mathematical expressions. Here, we have an expression where we are building two inputs, A and B. You'll see that A and B are -4 and 2, but we are wrapping those values into this value object that we are going to build out as part of Micrograd. This value object will wrap the numbers themselves, and then we are going to build out a mathematical expression where A and B are transformed into C, D, and eventually E, F, and G. I'm showing some of the functions, some of the functionality of Micrograd, and the operations that it supports. You can add two value objects, you can multiply them, you can raise them to a constant power, you can offset by one, negate, squash at zero, square, divide by a constant, divide by it, etc. We're building out an expression graph with these two inputs, A and B, and we're creating an output value of G. Micrograd will, in the background, build out this entire mathematical expression. It will, for example, know that C is also a value, C was the result of an addition operation, and the child nodes of C are A and B. The object will maintain pointers to the A and B value objects. We'll basically know exactly how all of this is laid out. Not only can we do what we call the forward pass, where we actually look at the value of G, which is straightforward, we will access that using the dot data attribute. The output of the forward pass, the value of G, is 24.7. The big deal is that we can also take this G value object and call it backward. This will initialize backpropagation at the node G. Backpropagation will start at G and go backward through that expression graph. It's going to recursively apply the chain rule from calculus. What that allows us to do is evaluate the derivative of G with respect to all the internal nodes, like E, D, and C, but also with respect to the inputs, A and B. Then, we can actually query this derivative of G with respect to A. That is a.grad, which in this case, happens to be 138. The derivative of G with respect to B, which also happens to be here, is 645. This derivative will be very important because it tells us how A and B are affecting G through this mathematical expression. In particular, a.grad is 138. If we slightly nudge A and make it slightly larger, 138 is telling us that G will grow, and the slope of that growth will be 138. The slope of growth of B will be 645. That tells us about how G will respond if A and B get tweaked a tiny amount in a positive direction. You might be confused about what this expression is that we built out here. This expression is completely meaningless. I just made it up. I am just flexing about the kinds of operations that are supported by Micrograd. What we actually really care about are neural networks. It turns out that neural networks are just mathematical expressions, just like this one, but actually slightly less crazy. Neural networks are just mathematical expressions. They take the input data and the weights of a neural network as inputs. The output is the predictions of your neural net or the loss function. We'll see this in a bit. Basically, neural networks just happen to be a certain class of mathematical expressions. Backpropagation is significantly more general. It doesn't actually care about neural networks at all. It only tells us about arbitrary mathematical expressions. We use that machinery for training neural networks. One more note I would like to make at this stage is that, as you see here, Micrograd is a scalar-valued autograd engine. It's working on the level of individual scalars, like -4 and 2. We are taking neural nets and breaking them down to these atoms of individual scalars and all the little pluses and times. It is just excessive. Obviously, you would never do this in production. It's really just put down for pedagogical reasons. It allows us not to deal with these n-dimensional tensors that you would use in a modern deep neural network library. This is really done so that you understand and refactor backpropagation and the chain rule and understand neural net training. If you actually want to train bigger networks, you have to use these tensors, but none of the math changes. This is done purely for efficiency. We are basically taking all the scalar values and packaging them up into tensors, which are just arrays of these scalars. Because we have these large arrays, we're making operations on those large arrays. That allows us to take advantage of the parallelism in a computer. All those operations can be done in parallel, and then the whole thing runs faster, but really, none of the math changes. That is done purely for efficiency. I don't think it is pedagogically useful to be dealing with tensors from scratch. That is why I fundamentally wrote Micrograd. You can understand how things work at the fundamental level, and then you can speed it up later. Here's the fun part. My claim is that Micrograd is what you need to train your networks, and everything else is just efficiency. You'd think that Micrograd would be a very complex piece of code, but that turns out not to be the case. If we just go to Micrograd, you will see that there are only two files here. This is the actual engine, and it doesn't know anything about neural nets. This is the entire neural nets library on top of Micrograd: engine and nn.pi. The actual backpropagation autograd engine that gives you the power of neural networks is literally 100 lines of very simple Python, which we'll understand by the end of this lecture. Then, nn.pi, this neural network library built on top of the autograd engine, is a joke. We have to define what a neuron is, what a layer of neurons is, and what a multi-layer perceptron is, which is just a sequence of layers of neurons. It is just a total joke. Basically, there is a lot of power that comes from only 150 lines of code, and that's all you need to understand to understand neural network training. Everything else is just efficiency. Of course, there is a lot to efficiency, but fundamentally that's all that is happening. Now, let's dive right in and implement Micrograd step by step. The first thing I would like to do is make sure that you have a very good, intuitive understanding of what a derivative is and exactly what information it gives you. Let's start with some basic imports that I copy-paste in every Jupyter notebook. Let's define a function, a scalar-valued function f of x, as follows. I will make this up randomly. I just want a scalar-valued function that takes a single scalar x and returns a single scalar y. We can call this function, passing in, say, 3.0, and get 20 back. We can also plot this function to get a sense of its shape. You can tell from the mathematical expression that this is probably a parabola; it's a quadratic. If we create a set of scalar values that we can feed in, using, for example, a range from -5 to 5 in steps of 0.25, so axis is from -5 to 5, not including 5, in steps of 0.25, we can call this function on this NumPy array as well. We get a set of y's if we call f on the axis. These y's are basically also applying a function on every one of these elements independently. We can plot this using matplotlib: plt.plot x's and y's, and we get a nice parabola. Previously, here, we fed in 3.0, somewhere here, and we received 20 back, which is here, the y-coordinate. Now, I would like to think through what the derivative of this function at any single input point x is. What is the derivative at different points x of this function? If you remember back to your calculus class, you have probably derived derivatives. You would take this mathematical expression, 3x squared - 4x + 5, and write it out on a piece of paper. You would apply the product rule and all the other rules and derive the mathematical expression of the derivative of the original function. Then, you could plug in different x's and see what the derivative is. We are not going to do that because no one in neural networks actually writes out the expression for the neural net. It would be a massive expression, tens of thousands of terms. No one actually derives the derivative. We are not going to take this kind of symbolic approach. Instead, I would like to look at the definition of derivative and make sure that we really understand what a derivative is measuring and what it is telling you about the function. If we look up derivative, we see, okay, this is not a very good definition of derivative. This is a definition of what it means to be differentiable. But, if you remember from your calculus, it is the limit as h goes to zero of f(x + h) - f(x) / h. Basically, what it is saying is, if you slightly bump up, you are at some point x that you're interested in, or a. If you slightly increase it by a small number h, how does the function respond? With what sensitivity does it respond? What is the slope at that point? Does the function go up or does it go down, and by how much? That is the slope of that function, the slope of that response at that point. We can evaluate the derivative here numerically by taking a very small h. Of course, the definition would ask us to take h to zero. We are just going to pick a very small h, 0.001. Let's say we are interested in the point 3.0. We can look at f of x, which is 20. Now, f of x + h, so if we slightly nudge x in a positive direction, how is the function going to respond? Just looking at this, do you expect f of x + h to be slightly greater than 20, or do you expect it to be slightly lower than 20? Since this 3 is here, and this is 20, if we slightly go positively, the function will respond positively. You would expect this to be slightly greater than 20. By how much is it telling you the strength of that slope, the size of the slope? So f of x + h - f(x) is how much the function responded in the positive direction. We have to normalize by the run, so we have the rise over the run to get the slope. This is a numerical approximation of the slope because we have to make h very, very small to converge to the exact amount. If I'm doing too many zeros, I will get an incorrect answer because we are using floating-point arithmetic. The representations of all these numbers in computer memory are finite, and at some point, we get into trouble. We can converse towards the right answer with this approach, but basically at 3, the slope is 14. You can see that by taking 3x squared - 4x + 5 and differentiating it in our head. 3x squared would be 6x - 4, and then we plug in x equals 3, so that's 18 - 4, which is 14. That is correct. That is at 3. How about the slope at, say, -3? Would you expect for the slope now? Telling the exact value is hard, but what is the sign of that slope? At -3, if we slightly go in the positive direction at x, the function would actually go down. That tells you that the slope would be negative. We'll get a slight number below 20. If we take the slope, we expect something negative, -22. At some point here, the slope would be zero. For this specific function, I looked it up previously, and it's at the point two over three, so at roughly two over three, somewhere here, this derivative will be zero. At that precise point, if we nudge in a positive direction, the function doesn't respond; it stays the same. That is why the slope is zero. Now, let's look at a more complex case. We are going to complexify a bit. Now we have a function with an output variable d that is a function of three scalar inputs: a, b, and c. a, b, and c are some specific values, three inputs into our expression graph, and a single output d. If we just print d, we get four. Now, what I have to do is again look at the derivatives of d with respect to a, b, and c, and think through the intuition of what this derivative is telling us. In order to evaluate this derivative, we're going to get a bit hacky here. We will again have a very small value of h. Then we will fix the inputs at some values that we are interested in. This is the point abc at which we are going to be evaluating the derivative of d with respect to a, b, and c at that point. Those are the inputs. Now, we have d1, which is that expression. Then, we are going to, for example, look at the derivative of d with respect to a. We'll take a and bump it by h, and then we will get d2 to be the exact same function. Now, we are going to print d1 is d1, d2 is d2, and print the slope. The derivative, or slope here, will be d2 - d1 / h. d2 - d1 is how much the function increased when we bumped the specific input that we are interested in by a tiny amount, and this is then normalized by h to get the slope. If I just run this, we will print d1, which we know is four. Now, d2 will be bumped. a will be bumped by h. Let's just think through a little bit what d2 will be printed out here. In particular, d1 will be four. Will d2 be a number slightly greater than four, or slightly lower than four? That will tell us the sign of the derivative. We are bumping a by h. b is -3. c is 10. You can intuitively think through this derivative and what it's doing. a will be slightly more positive, but b is a negative number. If a is slightly more positive, because b is -3, we are actually going to be adding less to d. You would actually expect that the value of the function will go down. Let's see this. We went from 4 to 3.9996, and that tells you that the slope will be negative. The exact number of the slope is -3. You can also convince yourself that -3 is the right answer mathematically and analytically because, if you have a * b + c and you have calculus, then differentiating a * b + c with respect to a gives you just b. Indeed, the value of b is -3, which is the derivative that we have. You can tell that that's correct. Now, if we do this with b, so if we bump b by a little bit in a positive direction, we would get different slopes. What is the influence of b on the output d? If we bump b by a tiny amount in a positive direction, then because a is positive, we'll be adding more to d. What is the sensitivity, the slope of that addition? It might not surprise you that this should be 2. Why is it 2? Because d of d by db, differentiating with respect to b, would give us a, and the value of a is 2. That is also working well. If c gets bumped a tiny amount by h, then a * b is unaffected. c becomes slightly bit higher. What does that do to the function? It makes it slightly bit higher because we are simply adding c, and it makes it slightly bit higher by the exact same amount that we added to c. That tells you that the slope is 1. That will be the rate at which d will increase as we scale c. We now have some intuitive sense of what this derivative is telling you about the function. We would like to move to neural networks. As I mentioned, neural networks will be pretty massive mathematical expressions. We need some data structures that maintain these expressions, and that is what we are going to start to build out now. We are going to build out this value object that I showed you in the readme page of Micrograd. Let me copy-paste a skeleton of the first very simple value object. The class `Value` takes a single scalar value that it wraps and keeps track of. That's it. We can, for example, do `Value(2.0)`, and then we can look at its content. Python will internally use the wrapper function to return this string. This is a value object with data equal to 2 that we are creating here. We would like to be able to have not just two values, but we would like to be able to add them. Currently, you would get an error because Python doesn't know how to add two value objects, so we have to tell it. Here is addition. You have to use these special double-underscore methods in Python to define these operators for these objects. If we call the plus operator, Python will internally call a dot add of b. That's what will happen internally, and b will be the other, and self will be a. We see that what we're going to return is a new value object, and it is just going to be wrapping the plus of their data. Remember, data is the actual numbered Python number, so this operator here is just the typical floating-point plus addition. It is not an addition of value objects, and will return a new value. Now, a plus b should work, and it should print the value of -1 because that is 2 + -3. There we go. Let's now implement multiply, so we can recreate this expression here. Multiply, I think, will be fairly similar. Instead of add, we are going to be using mul, and here, we want to do times. Now, we can create a c value object which will be 10.0. We should be able to do a * b. Let's just do a * b first. That is a value of -6. By the way, I skipped over this a little bit. Suppose that I didn't have the wrapper function here; then you would get some kind of an ugly expression. What the wrapper is doing is providing a way to print out a nicer-looking expression in Python. We don't just have something cryptic; we actually are, you know, it's a value of -6. This gives us times, and we should be able to add c to it because we've defined and told Python how to do mul and add. This will be equivalent to a.mul(b), and this new value object will be .add(c). Let's see if that worked. Yes, that worked well. That gave us four, which is what we expect from before. I believe we can call them manually as well. There we go. So, what we are missing is the connective tissue of this expression. As I mentioned, we want to keep these expression graphs. We need to know and keep pointers about what values produce what other values. Here, for example, we are going to introduce a new variable, which we will call children, and by default, it will be an empty tuple. Then we are actually going to keep a slightly different variable in the class, which we will call _prev, which will be the set of children. This is how I did it in the original Micrograd. Looking at my code here, I can't remember exactly the reason. I believe it was for efficiency, but this _children will be a tuple for convenience. When we maintain it in the class, it will be just this set. I believe for efficiency. When we are creating a value like this with a constructor, children will be empty, and prev will be the empty set, but when we are creating a value through addition or multiplication, we are going to feed in the children of this value, which, in this case, is self and other. Those are the children here. Now we can do d.prev, and we'll see that the children of the object are this value of -6 and the value of 10. This is the value resulting from a * b and the c value, which is 10. The last piece of information we don't know is we know the children of every single value, but we don't know what operation created this value. We need one more element here. Let's call it _op. By default, this is the empty set for leaves. Then we'll maintain it here. The operation will be a simple string. In the case of addition, it's +, and in the case of multiplication, it's *. Now, we not just have d.prev; we also have a d.op, and we know that d was produced by an addition of those two values. Now we have the full mathematical expression. We are building out this data structure. We know exactly how each value came to be by what expression and from what other values. Because these expressions are about to get quite a bit larger, we would like a way to nicely visualize these expressions that we are building out. For that, I'm going to copy-paste a bunch of slightly scary code that will visualize these expression graphs for us. Here is the code, and I'll explain it in a bit. First, let me show you what this code does. Basically, what it does is it creates a new function, `draw_dot`, that we can call on some root node, and then it is going to visualize it. If we call `draw_dot` on d, which is this final value here that is a * b + c, it creates something like this. This is d, and you see that this is a * b creating an intermediate value, and plus c gives us this output node d. That is `draw_dot` of d. I'm not going to go through this in complete detail. You can take a look at graphvis and its API. Graphvis is an open-source graph visualization software. What we're doing here is we are building out this graph using graphvis's API. You can see that `trace` is a helper function that enumerates all of the nodes and edges in the graph. That just builds a set of all the nodes and edges. Then, we iterate for all the nodes, and we create special node objects for them using `dot.node`. We also create edges using `dot.edge`. The only thing that's slightly tricky here is you'll notice that I add these fake nodes, which are these operation nodes. For example, this node here is just a plus node. I create these special op nodes here and connect them accordingly. These nodes are not actual nodes in the original graph. They are not actually a value object. The only value objects here are the things in squares. Those are actual value objects, or representations thereof. These op nodes are just created in this `draw_dot` routine so that it looks nice. Let's also add labels to these graphs so we know what variables are where. Let's create a special _label, or let's just do `label = ""` by default and save it in each node. Here, we are going to do `label = label` and `label = c`. Then let's create a special `e = a * b`, `e.label = e`, and `d = e + c`. `d.label = d`. Nothing really changes. I just added this new e function, a new e variable. Here, when we are printing this, I'm going to print the label here. This will be `%s` and `node.label`. We have the label on the left here. It says a and b are creating e, and then e + c creates d, just like we have it here. Finally, let's make this expression one layer deeper. d will not be the final output node. After d, we are going to create a new value object called f. We are going to start running out of variables soon. f will be -2.0, and its label will be f. Then `L`, capital L, will be the output of our graph, and `L = d * f`. L will be -8 is the output. Now we don't just draw d; we draw L. Somehow the label of L was undefined. The label has to be explicitly given to it. There we go. L is the output. Let's quickly recap what we have done so far. We are able to build out mathematical expressions using only plus and times so far. They are scalar-valued along the way. We can do this forward pass and build out a mathematical expression. We have multiple inputs here: a, b, c, and f going into a mathematical expression that produces a single output, L. This here is visualizing the forward pass. The output of the forward pass is -8. That is the value. What we would like to do next is run backpropagation. In backpropagation, we are going to start here at the end. We are going to reverse and calculate the gradient along all these intermediate values. What we are computing for every single value here is the derivative of that node with respect to L. The derivative of L with respect to L is 1. We are going to derive what is the derivative of L with respect to f, with respect to d, with respect to c, with respect to e, with respect to b, and with respect to a. In the neural network setting, you'd be very interested in the derivative of basically this loss function, L, with respect to the weights of a neural network. Here we just have these variables: a, b, c, and f, but some of these will eventually represent the weights of a neural net. We will need to know how those weights are impacting the loss function. We will be interested in the derivative of the output with respect to some of its leaf nodes. Those leaf nodes will be the weights of the neural net. The other leaf nodes, of course, will be the data itself, but usually we will not want or use the derivative of the loss function with respect to data because the data is fixed. The weights will be iterated on using the gradient information. Next, we are going to create a variable inside the `Value` class that maintains the derivative of L with respect to that value. We will call this variable `grad`. There is a data, and there is a self.grad, and initially, it will be zero. Remember that zero basically means no effect. At initialization, we are assuming that every value does not impact or affect the output because if the gradient is zero, that means that changing this variable is not changing the loss function. By default, we assume that the gradient is zero. Now that we have grad, and it's 0.0, we are going to be able to visualize it here after data. Here, grad is 0.4f, and this will be in the graph. Now, we are going to be showing both the data and the grad initialized at zero. We are just about getting ready to calculate backpropagation. This grad, as I mentioned, is representing the derivative of the output, in this case, L, with respect to this value. This is the derivative of L with respect to f, with respect to d, and so on. Let's now fill in those gradients and actually do backpropagation manually. Let's start filling in these gradients and start all the way at the end. As I mentioned, we are interested in filling in this gradient here. What is the derivative of L with respect to L? In other words, if I change L by a tiny amount of h, how much does L change? It changes by h, so it is proportional. Therefore, the derivative will be 1. We can measure these or estimate these numerical gradients numerically, just like we've seen before. If I take this expression and I create a `def lol` function here, putting this here, the reason I am creating a function here is that I don't want to pollute or mess up the global scope. This is.
