Okay, let's start. Today, we will continue our Zero to Hero series, and specifically, we will reproduce the 124 million parameter version of the GPT-2 model. OpenAI released GPT-2 in 2019, along with a blog post, a research paper, and code on GitHub. When we talk about reproducing GPT-2, we need to be careful because in this video, we are reproducing the 124 million parameter model. There was a miniseries of GPT-2 models released at different sizes. Typically, the largest model is called "GPT-2." The reason for this miniseries is to plot model size on the x-axis and downstream metrics on the y-axis to chart scaling laws. As model size increases, performance on downstream metrics improves. The GPT-2 paper includes four models, from 124 million to 1558 million parameters. These numbers disagree with the actual model sizes found on the GitHub repository, due to an error in the paper. The 124 million parameter model has 12 layers in the Transformer and 768 channels. I am assuming familiarity with these terms from my previous video, “Let's Build GPT From Scratch." If everything goes well, by the end of this video, we should see the validation loss decrease as the model improves at predicting the next token in a sequence on validation data. We will hopefully beat the original 124 million parameter GPT-2 model. Five years ago, this was a complex optimization, and compute resources were smaller. Today, you can reproduce this model in about an hour for roughly 10 dollars on a cloud compute service. You can achieve a model as good as the one released by OpenAI. Unlike many other models, OpenAI did release the weights for GPT-2 in their repository. The GPT-2 paper does not contain all the training details. We will also refer to the GPT-3 paper, which provides more concrete details on hyperparameters and optimization settings. The GPT-3 architecture is not a huge departure from the GPT-2 model. So, we will use both papers to reproduce the GPT-2 124M model. First, let's load the GPT-2 124M model as released by OpenAI and sample some tokens. The original GPT-2 code was written in TensorFlow, which is not as commonly used today. We want to use PyTorch because it is easier and more user-friendly. However, the initial code is in TensorFlow. To get our target model, we will use the Hugging Face Transformers library. They have a PyTorch implementation of GPT-2, including converted weights from TensorFlow, making it easier to load and work with. We can load the GPT-2 model using Hugging Face Transformers by importing `GPT2LMHeadModel` and loading the `pretrained` model with the string "gpt2." When you load "gpt2," you get the 124 million parameter model. To get the 1.5 billion parameter model you would load "gpt2-xl." From this loaded model, we can obtain the state dictionary, which is a raw set of tensors. This state dictionary is just a dictionary, so we can print the keys and the shapes of the corresponding tensors. The weight for the token embedding is of size 50,257 by 768, reflecting the 50,257 tokens in the GPT-2 vocabulary and a 768-dimensional embedding for each. The tokenization was discussed in detail in the previous videos on tokenization. This is the lookup table for tokens, where each token has a corresponding 768-dimensional vector. We also have the lookup table for positions, which, with a maximum sequence length of 1024, has 1024 positions, each with a fixed 768-dimensional vector learned during optimization. This is the position embedding. The remaining weights and biases belong to the Transformer. If we take the positional embeddings, flatten them out, and select the first 20 elements, we can see the parameters. These parameters, when plotted, show structure, with each row representing a different position in the sequence from 0 to 1023. The representation of each position creates a learned sinusoid, allowing the Transformer to understand relative token positions. If we look at individual columns in the positional embeddings, representing specific channels, we see that different channels respond to different parts of the position spectrum. This can look noisy, suggesting the model is not fully trained. The more trained the model, the smoother these curves should be. These curves do not have to be smooth, however, and should be just random noise at the start of optimization. The fact that they do have structure is a result of the optimization. The original Transformer paper used fixed sinusoids for positional encoding, but in GPT-2 these are trained from scratch, which works just as well, with these sinusoids being recovered during the optimization. We can also examine other matrices within the model, and see some structure. We are primarily interested in loading the weights from the released model using Hugging Face Transformers. We can use their pipeline to sample text. Using the prefix "hello, I am a language model," and sampling 30 tokens for 5 different sequences, it produces coherent text. I tried this example and sadly the results were slightly different from their example, presumably due to changes in the code. The important thing is that we are generating coherent text. We loaded the model, examined its parameters, and can generate text. We want to create our own GPT-2 class so we have a full understanding of what is happening. We do not want to use the Hugging Face implementation, so we will write this from scratch. Our first task will be loading the GPT-2 124M parameters into our class, giving us confidence that we can recreate the model. We will initialize the model from scratch and train it on documents, hoping to surpass the released model. We will obtain different weights but maintain the confidence that we can recreate the original model. So, let's write the GPT-2 model and load the weights to generate coherent text. Let's go back to the "Attention is All You Need" paper, which introduced the original Transformer. GPT-2 is a modified version of the Transformer. It is a decoder-only Transformer, missing the encoder. Therefore, the cross-attention mechanism, which used the encoder, is also missing. The rest of the model remains almost the same but with a few differences. In the GPT-2 paper, we see that the layer normalizations have been reshuffled and an additional layer normalization was added to the final self-attention block. Instead of being after the MLP or after attention, the layer normalizations are placed before it. An extra layer norm is added right before the classifier. Let's implement the skeleton NN modules in our GPT NN module. We will try to match the schema used by Hugging Face Transformers to make it easier to load weights from the state dictionary. The main container module is called "Transformer," and we reflect this using an NN module dict. This module allows indexing into sub-modules using string keys. Inside, we have "wte" for token embeddings, which is an NN embedding, and "wpe" for position embeddings, also an NN embedding. An NN embedding is a wrapper module around a tensor that allows access to its rows by indexing. In addition, we have "h," which is indexed using numbers, 0, 1, 2, up to 11, since there are 12 layers in the Transformer. We will use a module list to reflect that, allowing us to index using integers. The module list contains 'n_layer' blocks. We also need an additional final layer norm following the GPT-2 paper, and the final classifier, the language model head, which projects from 768 dimensions to the vocabulary size (50,257). GPT-2 uses no bias in this projection. This skeleton reflects the hugging face schema: WTE is token embeddings, PE is position embeddings, H is the layers, LNF is the added final normalization layer, and LM head is the linear output projection. Let’s implement the block. The block's forward pass is defined here. There is another change mentioned in the GPT-2 paper: layer normalizations are after attention or the feed forward network. Additionally, the normalizations are inside the residual stream. This places normalizations inside the residual pathway, which is not ideal. Ideally, the residual pathway should be clean from the top to the inputs. The gradient flows through the top and down to the inputs, unchanged through the residual pathway. The gradients also flow through the blocks, so the optimization changes over time, but a clean residual pathway is desirable for optimization purposes. In the pre-normalization version, "x" goes through layer normalization first, then attention, then another layer norm, then multi-layer perceptron (feed-forward), which feeds into the residual stream. Attention is a communication operation where all the tokens exchange information, acting as a pooling function. The MLP, on the other hand, occurs at every token individually, with no communication between tokens. Attention is the reduce, and the MLP is the map. The Transformer is a repeated application of map-reduce operations. The blocks refine the representation within the residual stream. This is our block implementation, slightly modified from the original. Let's look at the MLP block, implemented with two linear projections and a GELU nonlinearity. The GELU is approximated using tanh in GPT-2. In the PyTorch documentation, there are two versions of GELU: the original and the approximate. The original GELU is more like a ReLU with a smooth curve, it is described mathematically in the original paper. The approximate version was developed at the time because it was faster to compute in TensorFlow, even if it is less accurate. Today, there is no real reason to use the approximate version, but we want to reproduce GPT-2, so we stick with the approximate GELU. Using GELU over ReLU helps avoid the dead neuron problem, where ReLU neurons may have no gradient, as the GELU always has a local gradient, allowing change and adaptation. GELU is also empirically better. More modern networks use other variants, but for GPT-2, the approximate GELU is used. Finally, there is the attention operation. This is a multi-headed attention operation where, in the previous video, we showed that these heads function in parallel. Each head output is concatenated, creating the output of multi-headed attention. Here, instead of separate modules, we will put everything into a single self-attention module. We use a series of transpose and split tensor operations for efficiency, but the underlying algorithmic process is the same. Each token emits a query, key, and value vector. The queries and keys interact to give an attention weight. An autoaggressive mask prevents tokens from attending to tokens in the future. Then, a softmax normalizes these attention weights. The weighted sum of the value vectors generates a weighted representation of tokens found relevant. Then, another series of transpose/view operations reassembles all of the separate heads into a single output, effectively concatenating them. While this is more efficient in PyTorch, it is mathematically equivalent to our previous implementation. I am being careful with the variable names, such as 'c_attn', which should match 'attn'. These keys should align with the hugging face transformer code, making it easy to load weights. At this point, we have completed the GPT-2 implementation. This code is much shorter than the hugging face code. We can load the weights, set them, and then try generation. Let's see what that looks like. I've also changed the GPT config so that the numbers agree with the GPT-2 124M model. The block size is 1024. There are 50,257 tokens, including the merges and a special end-of-text token, 12 layers, 12 attention heads, and a 768 dimension for the transformer. Here is how we can load parameters from Hugging Face into our code and initialize the GPT class with these parameters. I am copying and pasting some code, which is not particularly interesting, but it loads the weights. There are four models in the miniseries. This is the Jupyter code from earlier. These are the hyper parameters of the GPT-2 models. We are creating a config object, creating our model, and then we're creating a state dictionary, for both our model and for the hugging face model. Then, we are looping over the hugging face model keys and copying those tensors, ignoring some of the buffers like attention biases, which are used for the auto regressive mask. One additional issue is that some of the weights are transposed, from the TensorFlow format, so I have manually hardcoded which weights to transpose. We return this model. The `from_pretrained` is a class method that returns a GPT object when given a model type, which in our case is "gpt2". This code loads the weights and biases into our `nn.Module`. Let’s test if this is working by generating from this model. Before generating, we need to forward the model, which we haven’t implemented yet. Here is the forward function: The input to the forward function is going to be the indices which represent our tokens. They are always of the shape 'B x T'. Where B is the batch dimension and T is the time dimension. T cannot be more than the block size (maximum sequence length). The indices are a two dimensional layout. Each row is a sequence of length up to the block size, and we have B independent sequences in a batch.

Here is the cleaned-up transcript:

We are forwarding the position embeddings and the token embeddings. This code should be recognizable from the previous lecture. We use a range, which is similar to the range function but for PyTorch. We iterate from zero to T, creating these position indices. We ensure they are on the same device as the input indices because we will be training on a GPU, not just a CPU. This is more efficient. The position embeddings and the token embeddings are added together. The position embeddings are identical for every row of input, so there's broadcasting happening in this addition. We create an additional dimension so these two can add up because the same position embeddings apply to every row of our examples in a batch. We forward the Transformer blocks, and then the last layer normalization and the LM head. What comes out of the forward pass are the logits. If the input was B by T indices, then at every position, B by T, we calculate the logits for what token comes next in the sequence. So, what is the token at B, T+1, the one to the right of this token? B times vocab size is the number of possible tokens. This is the tensor we will obtain, and these logits are a softmax away from becoming probabilities. This is the forward pass of the network, and we can generate from the model.

Now we're going to set up the identical thing on the left here that matches Hugging Face on the right. We sampled from the pipeline five times, up to 30 tokens, with the prefix "hello I'm a language model," and these are the completions we achieved. We'll replicate that on the left. The number of sequences is five, and the max length is 30. First, we initialize our model, and put it into evaluation mode. This is good practice to put the model into eval mode when you're not going to be training it. I’m not sure if this is doing anything right now because our model contains no modules or layers that have different behavior at training or evaluation time. For example, dropout, batch norm, and other layers have this behavior, but the layers we've used should be identical in both training and evaluation. So, `model.eval()` might not do anything, but maybe PyTorch internals do some clever things depending on the evaluation mode. Next, we're moving the entire model to CUDA, so all the tensors go to the GPU. I am on a cloud box with several GPUs. Here, I am moving the entire model, its members, its tensors; everything is shipped off to a separate computer, the GPU, which is connected to the CPU. It's really well suited to parallel processing tasks like running neural networks. Doing this makes our code a lot more efficient, because all of this runs faster on GPUs. So, the model lives on the GPU, a separate computer.

Next, we want to use "hello I'm a language model" as the prefix when we generate. Let's create those prefix tokens. We import the Tiktoken library from OpenAI and get the GPT-2 encoding, which is the tokenizer for GPT-2. Then we encode this string and get a list of integers, which are the tokens. We can copy and paste the string and inspect it in Tiktoken. These are the tokens that will come out, and, as you may recall, all tokens are just little string chunks. This is the chunking of the string into GPT-2 tokens. Once we have the tokens as a list of integers, we create a PyTorch tensor out of it. In this case, there are eight tokens. Then we replicate these eight tokens five times to get five rows of eight tokens. This is our initial input X, and it also lives on the GPU. X is now the input that we can use in the forward pass to get our logits.

We are ready to generate. In this code block, we have X, which is of size B by T, so batch by time. In every iteration of this loop, we will add a column of new indices into each of these rows. These are the new indices, and we are appending them to the sequence. With each loop iteration, we get one more column into X. All of the operations happen in the context manager of `torch.no_grad()`. This tells PyTorch that we are not going to call backward, so it doesn’t have to cache the intermediate tensors or prepare for a potential backward pass. This saves space and time. We get our logits. Then, we get the logits at only the last location. We throw away the other logits because we only care about the last column. This is wasteful and an inefficient implementation of sampling, but it's correct. We pass the last column of logits through softmax to get our probabilities. Then I am doing top-k sampling of 50, which is the Hugging Face default. Looking at the Hugging Face documentation, there are a bunch of arguments that go into the pipeline, but the important one is that they use top-k, which is 50, by default. This takes our probabilities, and only keeps the top 50. Anything lower than the 50th probability is clamped to zero and renormalized. We are never sampling rare tokens; we are only sampling the top 50 most likely tokens. This helps keep the model on track and prevents it from getting lost or going off the rails as easily. It sticks to likely tokens.

This is how to do it in PyTorch. Roughly speaking, we get a new column of tokens, we append them to X, and the columns of X grow until the loop terminates. Finally, we have an entire X of size 5 by 30 in this example, and we can print all of the individual rows. I am getting all the rows, all the tokens that were sampled, and using the decode function from Tiktoken to get the string, which we can print. These are the generations that we're getting. The model will just keep going. These generations are not the same as the Hugging Face generations. I cannot find the discrepancy, and I didn't go through all these options, but there's probably something else, in addition to the top-k, that's different. I am not able to match them up. For correctness, I replicated the code below in the Jupyter notebook using the Hugging Face model, and I am getting the same results, so the model internals are correct. I'm just not 100% sure what the pipeline does in Hugging Face. Otherwise, the code is correct, we loaded the tensors correctly, and we initialized the model correctly. This works. We've ported all the weights, initialized the GPT-2 model, and it can generate sensible sequences. Here, we are initializing with GPT-2 model weights. Now, we want to initialize from scratch using random numbers and train a model that gives us sequences as good as, or better than, these ones in quality. We'll turn to that next.

Using a random model is fairly straightforward because PyTorch initializes our model randomly by default. When we create the GPT model in the constructor, these layers and modules have random initializers that are there by default. When the linear layers are created, they use default constructors, like the Xavier initialization, to construct the weights of these layers. Creating a random model instead of a GPT-2 model is fairly straightforward. Instead, we would create `model = GPT()` with the default `GPTConfig`, and the default config uses 124 million parameters. This is the random model initialization, and we can run it. The results will be garbage because this is a random model, and we are getting random token string pieces chunked up randomly.

One more thing I want to point out is that if you do not have CUDA available because you do not have a GPU, you can still follow along with what we're doing, to some extent. Maybe not to the very end, because eventually we will be using multiple GPUs, but for now you can follow along. In PyTorch, I like to auto-detect the device that is available. By default, we start with the CPU, which is available everywhere. Then we try to detect if a GPU exists, by trying to use CUDA. If not, we check for MPS, the backend for Apple Silicon. If you have a fairly new MacBook, you probably have Apple silicon on the inside, which has a capable GPU. You can use MPS, which will be potentially faster than the CPU. We can print the device. Once we have the device, we can use it in place of CUDA. When we call model on X, if X is on the CPU instead of the GPU, it will work fine because in the forward pass, when we create the position indices, we are careful to use the device of the input indices. There will not be a mismatch where one tensor is on the CPU and one is on the GPU. We are carefully initializing on the correct device. This will auto-detect the device. It will be the GPU, or CUDA. You can also run with another device. It will not be too much slower. If I override `device=CPU`, then we're using the CPU. We're not using torch compile, which will speed things up. You can follow along even on a CPU.

I want to loop around eventually into what it means to have different devices in PyTorch, what PyTorch does in the background when you do something like `module.to(device)` or `torch.tensor.to(device)`, but for now, I'd like to get to training. Let's say that the device makes code go fast. Let's go into how we can train the model. To train the model, we will need a dataset. The best debugging, simplest dataset that I like to use is the tiny Shakespeare dataset. It is available at this URL. You can use Wget, or you can just search for "tiny Shakespeare dataset". I have it as `input.txt`. I downloaded it. Here, I am reading the dataset, getting the first 1000 characters, and printing the first 100. GPT-2 has roughly a compression ratio of three to one with its tokenizer, so 1000 characters is roughly 300 tokens. This is the first few characters. If you want more statistics, we can use `wc input.txt`. This is 40,000 lines, about 200,000 words, and about 1 million bytes. Because this file is only ASCII characters, every ASCII character is encoded with one byte. So, there are roughly 1 million characters. That's the dataset size, small for debugging.

To tokenize the dataset, we'll get the Tiktoken encoding for GPT-2, encode the data, the first 1000 characters, and then print the first 24 tokens. These are the tokens as a list of integers. If you can read GPT-2 tokens, you will recognize 198 as the slashing character, which is a new line. Here we have two new lines, so that's 198 twice. This is the tokenization of the first 24 tokens. We want to process these token sequences and feed them into a Transformer. We want to rearrange these tokens into this input variable, or `idx`. We do not want a single long, one-dimensional sequence. We want an entire batch where each sequence is up to T tokens, and T is not larger than the maximum sequence length. We want these T-long sequences of tokens, and B independent examples of sequences. How can we create a B by T tensor to feed into the forward pass out of these one-dimensional sequences?

My favorite way to do this is to create a tensor from the list of integers. Take the first 24 tokens. Then do a `.view()` of 4x6, which multiplies to 24. It is a two-dimensional rearrangement of these tokens. When you view this one-dimensional sequence as two-dimensional, the first six tokens end up as the first row, the next six tokens are the second row, and so on. It will stack up every six tokens as independent rows, creating a batch of tokens. For example, if token 25 is in the Transformer, and this becomes the input, this token will see these three tokens and predict that 198 comes next. In this way, we can create this two-dimensional batch. This is quite nice. How do we get the labels for the target to calculate the loss? We could write some code inside the forward pass, because we know the next token in the sequence is the label, which is just to the right of us. For the token at the very end, 13, we don't have the next correct token, because we didn't load it.

I'll show you how I get these batches. I like to have the input to the Transformer, which I like to call X, and I also like to create the `targets` tensor, which is the same size as X but contains the targets at every single position. Here's how I like to do this. I fetch one additional token because we need the ground truth for the last token. When creating the input, we take everything up to the last token and view it as 4x6. When creating the targets, we use the same buffer but starting at index one, not index zero. We skip the first element and view it in the exact same size. When I print this, we see that for token 25, its target was 198, and this is now stored in the exact same position in the targets tensor. Also, the last token, 13, now has its label, which is 198, because we loaded the plus one token. We take long sequences, view them in two dimensions to get batch of time, and we load one additional token so that we have a buffer of B by T+1 tokens. Then, we offset things and view them, and we have two tensors. One is the input to the Transformer, and the other is the labels. Let's reorganize the code, and create a simple data loader that loads these tokens and feeds them to the Transformer to calculate the loss.

I reshuffled the code. I am overwriting the device to use a CPU for now. We are importing Tiktoken. This should look familiar. We're loading 1000 characters. I’m setting batch size to four and time to 32. We want to have a single batch that's small. This follows what we did previously. Here, we create the model and get the logits. We have a batch of 4x32, so our logits are now of size 4 by 32 by 50257. Those are the logits for what comes next at every position. We have the labels in Y. Now is the time to calculate the loss, do the backward pass, and the optimization. Let’s first calculate the loss. To calculate the loss, we’re going to adjust the forward function of this `nn.Module` in the model. We will return not only the logits, but also the loss. We'll also pass in the targets in Y, not just the input indices. We will print the loss function and then call `sys.exit(0)` to skip the sampling logic. In the forward function, we now have optional targets. When we get targets, we can calculate the loss. We're going to return logits, the loss, and by default the loss is none. If the target is not none, then we want to calculate the loss. Co-pilot is calculating the correct loss, using cross entropy loss, as documented here. In PyTorch, this is under the `functional` module.

What is happening here? Cross-entropy does not like multidimensional inputs. It can’t take B by T by vocab size tensors. We're flattening out this three-dimensional tensor into two dimensions. The first dimension will be B*T, and the last dimension is the vocab size. We are flattening out the logits to be two dimensional, with B*T examples and the vocab size in terms of the length of each row. We flatten out the targets so they are a single tensor of B*T. This can then pass into cross entropy, and we return the loss. This should work. We should be printing the loss. Here, we printed 11. This is a tensor with a single element, the number 11.

We also want to calculate a reasonable starting point for a randomized network. Our vocabulary size is 50257. At initialization, we would hope that every vocabulary element is getting roughly uniform probability. We're not favoring, at initialization, any token too much. We’re not confidently wrong. We're hoping the probability of any token is roughly one divided by the vocabulary size. We can sanity check the loss. The cross-entropy loss is just the negative log-likelihood. If we take this probability, and take it through the natural logarithm, and then negate it, that is the loss we expect at initialization. We expect something around 10.82. We are seeing something around 11, so it's not way off. That tells me our probability distribution at initialization is roughly diffused, which is a good starting point. Now, we can perform the optimization and tell the network which elements should follow correctly in what order.

We can now do a loss, backward step, calculate the gradients, and do an optimization. Let's do the optimization. We have the loss. Now, we want a for loop. Let's do 50 steps. We’ll create an optimizer object in PyTorch. We are using the Adam optimizer, which is an alternative to stochastic gradient descent (SGD). SGD is simpler, and Adam is a bit more involved. I specifically like the AdamW variation because, in my opinion, it fixes a bug. AdamW is a bug fix of Adam. Looking at the AdamW documentation, it takes a bunch of hyperparameters. It's more complicated than the SGD we looked at before. It updates parameters with the gradient scaled by the learning rate, and keeps buffers around. It keeps two buffers, M and V, which are the first and second moments, like momentum and RMSProp, if you're familiar with them. You don't have to be. It's a normalization that happens on each gradient element, which speeds up the optimization, especially for language models. We will treat it as a black box. It optimizes the objective faster than SGD. Let's create the optimizer object and go through the optimization.

First, we zero the gradients. You have to start with a zero gradient. When you get your loss and do a backward pass, the gradients are added to. It always does a plus equals on whatever the gradients are, so you must set them to zero. This accumulates the gradient from this loss, and we call the step function on the optimizer to update the parameters and decrease the loss. Then, we print the step and loss. `loss.item()` is used here because the loss is a tensor with a single element. `loss.item()` will convert that tensor to a float. This float will live on the CPU. The loss is a tensor with a single element that lives on the GPU. When you call `.item()`, PyTorch will take that one-dimensional tensor, ship it back to the CPU memory, and convert it into a float, which we can print. This is the optimization, and this should just work.

Expected all tensors to be on the same device, but found at least two devices: CUDA zero and CPU. CUDA zero is my first GPU, because I have eight GPUs. When I wrote this code, I introduced a bug. We moved the model to the device, but we did not move the buffer to the device. You can't just do `buff.to(device)`. It is not stateful; it returns a pointer to a new memory on the device. You have to do `buff = buff.to(device)`. This should work. We expect to see a reasonable loss in the beginning, and then we continue optimizing the single batch. We want to see that we can overfit this single batch, and predict all the indices. We started off at roughly 10.82 or 11, and as we optimize this single batch, we are getting a very low loss. The transformer is memorizing this single batch. The learning rate is 3e-4, which is a good default for optimizations at an early debugging stage. This is our simple training loop, and we are overfitting a single batch. This looks good. Next, we don't want to overfit a single batch; we want to do a real optimization. We need to iterate these XY batches and create a little data loader that ensures that we always get a fresh batch. Let's do that next.

This is what I came up with. I wrote a little data loader. We are importing the tokenizer. We're reading the entire text file from `input.txt`, tokenizing it, and printing the number of tokens in total and the number of batches in a single epoch of iterating over this dataset. This is how many unique batches we output before we loop back to the beginning of the document and start reading it again. We start at position zero and walk the document in batches of B*T. We take chunks of B*T, and we advance by B*T. It’s important to note that we advance by B*T, but when we fetch the tokens, we are fetching from the current position to B*T+1. We need that +1 because we need the target token for the last token in the current batch, so we can do the XY exactly as we did before. If we run out of data, we’ll loop back around to zero. This is one way to write a simple data loader that goes through the file in chunks and is good enough for us. We're going to make it more complex later. Now, we would like to use our data loader. The import of Tiktoken is up there. All of this is useless now. Instead, we want a train loader for the training data. We'll use the same hyper-parameters. Batch size is 4, and time is 32. Now we need to get the XY for the current batch.

The next batch needs to have tensors moved from the CPU to the device. When I converted the tokens, they were left on the CPU, which is the default, to avoid wasting GPU memory. This is a tiny dataset and would fit on the GPU. For our purposes, it's fine to ship it to the GPU now. We keep the data loader simple on the CPU and then ship it to the GPU for computation. Let's see if `python train gpt2.py` runs. We expect to get the next batch and avoid overfitting a single batch. Therefore, the loss should come down, but not too much. I still expect the loss to come down because many tokens in the 50,257 tokens never occur in our dataset. There are some easy gains to be made in the optimization by, for example, taking the biases of the logits that never occur and driving them to negative infinity. These include crazy Unicode or different languages whose tokens never occur and their probability should be very low. The gains should be along the lines of deleting the usage of tokens that never occur, which is probably most of the loss gain we'll see at this scale. The loss shouldn't come down to zero because we are only doing 50 iterations, which is not enough for an epoch. We have 338,000 tokens, which makes sense with our 3:1 compression ratio, as there are one million characters. One epoch with the current settings of B and T will take 2,600 batches and we are only doing 50 batches of optimization here. We start off as expected, and then the loss comes down to about 6.6, so things are working okay right now. Next, I want to fix a bug in our code, which is not major, but it affects how GPT2 training should happen. We were not careful enough when loading weights from Hugging Face and missed a detail. The shape of the token embedding tensor at the bottom of the Transformer and the language modeling head at the top of the Transformer are the same. Both tensors are two-dimensional and have a shape of 50,257 x 768. The first tensor gives us our token embeddings, and the second takes the 768 channels of the Transformer and upscales them to 50,257 to get the logits for the next token. The tensors have the same shape, and if we compare their elements using `torch.all` we see that every element is identical. Also, if we look at the data pointer using `data_ptr()`, we see that the pointer is identical. These are not just two separate tensors with the same shape and elements; they are pointing to the identical tensor. This is a common weight tying scheme that comes from the original "Attention is All You Need" paper. The paper mentions that the model shares the same weight matrix between the two embedding layers and the pre-softmax linear transformation. This means that the two are shared, tied, and are the same matrix. The idea for this comes from the concept that if two tokens are semantically similar, they should be nearby in the token embedding space, and they should get the same probabilities at the output of a Transformer. Both positions, at the bottom and top of the Transformer, should have the property that similar tokens should have similar embeddings or weights. This motivates the exploration to tie the matrices, which leads to better performance. The authors also observe that output embeddings behave like word embeddings. This was adopted in the "Attention is All You Need" paper and used again in GPT2. I could not find it in the Transformers implementation, but it is in the original GPT2 code released by OpenAI. In their model's forward pass, they get the `wte` token embeddings and reuse it to do the logits. The `wte` tensor is used twice, at the bottom and top of the Transformer. In the backward pass, we'll get gradient contributions from both branches, which will add up on the `wte` tensor. We are currently not sharing `wte` in our code but want to do so. Here's one way to do it: We take the `wte.weight` and redirect it to point to the `lm_head`. This copies the data pointer and the old `wte.weight` value is orphaned and cleaned up by Python. We are left with a single tensor used twice in the forward pass. This is to my knowledge all that is required and this should train better with this exact tensor used twice. According to the paper, we expect slightly better results by doing this. This is also nice because it saves parameters. The size is 768 * 50,257, which is 40 million parameters. The model has 124 million parameters, so this is like saving 30% of the parameters. This could be why it works better with weight tying when not training long enough, as there are fewer parameters. It also provides an inductive bias that these embeddings should share similarities between tokens. Next, we need to be more careful with initialization and follow how GPT2 initializes their model. The GPT2 and GPT3 papers are not very explicit about initialization, so we need to read between the lines. The OpenAI code initializes weights with a standard deviation of 0.02 using a normal distribution, and biases are initialized to zero. The token embeddings are initialized at 0.02 and positional embeddings at 0.01. Here's some code to mirror that: At the end of the GPT module's initializer, we call the `apply` function of `nn.Module`. This iterates over all the sub-modules and applies the `init_weights` function on them. If a module is an `nn.Linear` module, we initialize the weight with a normal distribution with a standard deviation of 0.02, and the bias to zero. We use 0.02 for the embedding, and keep the positional embedding to the same instead of 0.01. The only other layer that needs initialization is the layer norm. The PyTorch default initialization sets the scale in the layer norm to one and the offset to zero, which is what we want. We are following the GPT2 source code for initialization. The standard deviation on this initialization, if following Xavier initialization, would be one over the square root of the number of incoming features, but 0.02 is consistent with that. One over the square root of 768 is 0.03. One over the square root of 1600 is 0.02. 0.02 is roughly in the vicinity of reasonable values for these initializations. We will keep it at 0.02 because that is the GPT2 initialization per their source code. We are not fully done yet on initialization. A model initialization that accounts for accumulation on the residual path with model depth is used. We scale the weight of residual layers of initialization by a factor of one over the square root of n, where n is the number of residual layers. If starting with zeros in the residual stream, each residual block contributes an amount and gets added. The variance of activations in the residual stream grows. We have a small example of starting at zero with 768 zeros in a residual stream and add random numbers from a normal distribution with zero mean and one standard deviation 100 times. At the end, the residual stream has grown to have a standard deviation of 10. The scaling factor compensates for this growth. If we scale down every contribution into the residual stream by one over the square root of n (or n^-0.5), we get a standard deviation of one, which controls the growth of activations inside the residual stream. We want to initialize the weights at the end of each block, the C layer, by scaling down the weights by one over the square root of the number of residual layers. Here's a way to implement this: In the initialization, we set a flag, `scale_init`, to one. When we iterate through the modules, if the module has this flag, we scale down the standard deviation by one over the square root of twice the number of layers, raised to the power of 0.5. The two times number of layers comes from the fact that every layer in the Transformer has two blocks that add to the residual pathway: the attention and the MLP. Also, because we are weight sharing `wte` and the `lm_head`, we're going to come around to that tensor twice. It will be initialized as an embedding and then as a linear layer with the identical initialization of 0.02. I have added some code for reproducibility, setting the seeds. Now we should be able to run `python train gpt2.py`. At this point, we have the GPT2 model, some confidence that it's correctly implemented, and it is properly initialized. We have a data loader that iterates through data batches, and we can train. Now comes the fun part; speeding up the training. We need to get our money's worth with respect to the hardware. First, you want to consider what hardware you have and if you are fully utilizing it. I have eight A100 SXM 80 GB GPUs. Lambda Labs is where I spin up these kinds of boxes and you pay per hour. When looking at the A100s, there are numbers for how many calculations you can expect from this GPU. When breaking into the code after calculating the logits and loss, we see that `lit.dtype` prints `torch.float32`. By default, PyTorch tensors are in float32, which means every number uses 32 bits. This is too much memory, and deep learning can tolerate significantly lower precisions. These GPUs support up to fp64, which is useful for scientific computing. We don't need that much precision for deep learning. Currently, we are using fp32, and we expect to get at most 19.5 Teraflops of performance. If we are willing to go down in precision to tf32, we can get an 8x improvement. If we are willing to go down to float16 or bfloat16, we can get 16x performance. Nvidia likes to cite numbers with sparsity, but we are not using that. We could have gotten more with int8, but int8 is used for inference, not training. int8 has uniform spacing, and we need a float to match the normal distributions that occur during training of neural networks. By reducing the precision, we can get more Teraflops and reduce the memory bandwidth and memory needs. It becomes easier to move the numbers around with fewer bits of representation. The memory bandwidth is a precious resource, and many deep learning workloads are memory bound. The tensor cores are often waiting because we can't load data fast enough from memory. If you are getting 60% utilization, you are doing extremely well. By coming down in precision for all the floats, weights and activations require less memory, so we can store and access it faster, which speeds everything up. First, let's look at the tensor float32 format. Tensor cores are just an instruction in the A100 architecture that does 4x4 matrix multiplication. There are multiple configurations for precision. Operations that require matrix multiplication get broken up into this 4x4 multiply instruction. Most of the computational work is matrix multiplication in the linear layers. The biggest matrix multiplication is the classifier layer, going from 768 to 50,257. Matrix multiplication is accelerated through tensor cores. The best reference for tensor cores is the A100 architecture white paper. Figure 9 shows tensor float 32. The input operands, accumulator, and the internal representation in the instruction have multiple configuration options. Normally, fp32 has 32 bits. tf32 has the same bits, except the mantissa bits get cropped in the float, ending up with just 19 bits. This allows it to calculate the little matrix multiply significantly faster, 8x faster. Our code will not change; it's internal to the operation. The results are approximate, but empirically, you can't tell the difference. tf32 is a good option; if you can tolerate a little bit of a precision fudge, you can go 8x faster. Let's look at what that looks like. I've set up our code to time the iterations. I changed the hyperparameters to reflect a workload we want to run. We'll use a batch size of 16 and the GPT2 maximum sequence length of 1024 tokens. I'm using `time.time()` to get the current time. The optimization loop includes a `torch.cuda.synchronize()`. This waits for the GPU to finish all the work that was scheduled to run before taking time. I'm going to print the iteration time. I'm also using `nvidia-smi` to watch GPU usage. PyTorch uses GPU0 by default, and we see it is filled up with about 35 GB out of 80GB. With the batch size cranked up, it's only 20 batches to do a single epoch on our tiny Shakespeare dataset. We see roughly 1000 milliseconds per iteration. The first iteration is slower due to PyTorch initializations. When timing, be careful about that first iteration. Our baseline in float32 is 1000 milliseconds per iteration and will run for roughly 50 seconds. If this doesn't fit into your GPU, decrease the batch size until it does. Use numbers with powers of two, like 16, 8, 24, 32, or 48. Do not use numbers like 17, because they will run inefficiently on the GPU. I'm also calculating the tokens per second throughput, which is our objective measure, because we might change the batch size. Right now, we're processing 163,000 tokens per second. Now, let's enable tf32. Luckily, PyTorch makes this easy. To enable tf32, we use a single line. This line tells PyTorch what kind of kernels to run. By default, it uses the highest precision for `matmul`, which means everything happens in float32. If we set it to high, as we do here, then we enable TF32.

Matrix multiplications will now use Tensor Flow 32 when it's available. My GPU is an A100, so it's an Ampere series, and therefore TF32 is available. If you have an older GPU, this might not be available for you. But for my GPU, it's available, and so what I expect Pytorch to do is that every single place where we see an `nn.linear` inside, there's a matrix multiplication, and I expect that matrix multiplication now to be running on Tensor Cores utilizing the TF32. This is the single line of change that is, I believe, necessary. Let's rerun this. Now, we saw that, in terms of the throughput that is promised to us, we're supposed to be getting roughly 8x. That 8x came from here, and it also came from looking at it here, 156 TFLOPS instead of 19.5. So, what actually happened? We're seeing that our throughput is roughly 3x, not 8x. We're going from 1,000 milliseconds down to 300 milliseconds, and our throughput is now about 50,000 tokens per second. So we have a roughly 3x improvement instead of 8x. What's happening here is, again, a lot of these workloads are memory-bound. Even though the TF32 offers, in principle, a lot faster throughput, all of these numbers everywhere are still float 32s, and it's float 32 numbers that are being shipped all over the place through the memory system. It's costing us way too much time to shuttle around all this data. Even though we've made the multiply itself much faster, we are memory-bound and we're not actually seeing the full benefit that would come from this napkin math here. That said, we are getting 3x faster throughput, and this is free with a single line of code in Pytorch. All your variables are still float 32 everywhere, it just runs faster, and it's slightly more approximate, but we're not going to notice it. That's TF32. 

Let's now continue. We've exercised this row, and we saw that we can crop out some of the precision inside the operation itself, but we saw that we're still memory-bound, and we're still moving around all these floats, and we're paying that cost because of this. Let's now decrease the amount of stuff that we're going to be moving around, and we're going to do that by dropping down to BFloat16. We're only going to be maintaining 16 bits per float, and we're going to use the BFloat16. I'll explain the FP16 difference in a bit. We're going to be in this row. When we go back to the documentation here for the A100, we see here the precisions that are available. This is the original FP32. The TF32 crops out the precision, and then here in BFloat16, you see that it is very similar to TF32, but it's even more aggressive in cropping off of the precision, the mantissa of this float. The important thing with BFloat16 is that the exponent bits and the sign bit, of course, remain unchanged. If you're familiar with float numbers, the exponent sets the range that you can represent of your numbers, and the precision is how much precision you have for your numbers. The range of numbers is identical, but we have fewer possibilities within that range because we are truncating the mantissa. We have less precision in that range. What that means is that things are actually fairly nice because we have the original range of numbers that are representable in float, but we just have less precision for it. The difference with FP16 is that they actually touch and change the range. FP16 cannot represent the full range of FP32. It has a reduced range, and that's where you start to actually run into issues because now you need these gradient scalers and things like that. I'm not going to go into the detail of that in this video because that's a whole video by itself. But FP16 actually historically came first. That was available in the Volta series before Ampere. FP16 came first, and everyone started to train in FP16, but everyone had to use all these gradient scaling operations, which are kind of annoying, and it's an additional source of state and complexity. The reason for that was because the exponent range was reduced in FP16. That's the IEEE FP16 spec. Then they came out with BFloat16 and the Ampere, and they made it much simpler because we're just truncating the mantissa. We have the exact same range, and we do not need gradient scalers. Everything is much, much simpler. 

Now, when we use BFloat16, though, we are impacting the numbers that we might be seeing in our Pytorch code. This change is not just local to the operation itself. Let's see how that works. There's some documentation here. I think this is probably the best page to explain how to use mixed precision in Pytorch because there are many other tutorials and so on, even within the Pytorch documentation, that are a lot more confusing. I recommend specifically this one because there's five other copies that I would not recommend. When we come here, ignore everything about gradient scalers and only look at `torch.autocast`. Basically, this comes down to a single line of code at the end. This is the context manager that we want, and we want to use that in our network. When you click into the `torch.autocast`, it has a few more guidelines for you. It's telling you, do not call BFloat16 on any of your tensors, just use `autocast` and only surround the forward pass of the model and the loss calculation. Those are the only two things that you should be surrounding. Leave the backward and the optimizer step alone. That's the guidance that comes from the Pytorch team. We're going to follow that guidance. For us, because the loss calculation is inside of the model forward pass, we are going to be doing this. We don't want to be using `torch.float16` because if we do that, we need to start using gradient scalers as well. We are going to be using BFloat16. This is only possible to do on Ampere, but this means that the changes are extremely minimal, like basically just this one line of code. Let me first break in here before we actually run this. Right after the logits, I'd like to show you that different from the TF32 that we saw, this is actually going to impact our tensors. This `logits` tensor, if we now look at this and we look at the `dtype`, we suddenly see that this is now BFloat16. It's not float32 anymore. So our activations have been changed. The activations tensor is now BFloat16, but not everything has changed. `model.transformer.wte`, this is the weight token embedding table. It has a weight inside it, and the `dtype` of this weight, this parameter, is still `torch.float32`. Our parameters seem to still be in float32, but our activations, the logits, are now in BFloat16. Clearly, this is why we get the mixed precision. Some things Pytorch is keeping in float32, some things Pytorch is converting to lower precision. What gets converted at what point is not super clear. I remember scrolling down. Is it here? Okay, I can't find it. I thought it was here. Okay, there we go. There are a few docs on when you're using this `autocast`, what gets converted to BFloat16 and when. For example, only these matrix multiply-like operations get converted to float16, but a lot of operations remain in float32. In particular, a lot of normalizations like layer norms and things like that, not all of those layers might be converted. Only some layers selectively would be running BFloat16. But things like Softmax, layer norms, log, log softmax, and loss function calculations, a lot of those things might remain in float32 because they are more susceptible to precision changes. Matrix multiplies are fairly robust to precision changes. Some parts of the network are impacted more or less by the precision change. Basically, only some parts of the model are running in reduced precision. Let's take it for a spin and let's actually see what kind of improvement we achieve here.

We used to be at 333 milliseconds, we're now at 300. We used to be somewhere around 50,000 tokens per second, we're now at 55,000. We're definitely running faster, but maybe not a lot faster, and that's because there are still many, many bottlenecks in our GPT2. We're just getting started. But we have dropped down the precision as far as we can with my current GPU, which is an A100. We're using Pytorch `autocast`. Unfortunately, I don't exactly know what Pytorch `autocast` does. I don't actually know exactly what's in BFloat16, what's in float32. We could go in and we could start to scrutinize it, but these are the kinds of rules that Pytorch has internally, and unfortunately, they don't document them very well. We're not going to go into that in too much detail, but for now, we are training in BFloat16. We do not need a gradient scaler. The reason things are running faster is because we are able to run Tensor Cores in BFloat16 now. That means we are in this row. But we are also paying in precision for this. We expect slightly less accurate results with respect to the original FP32, but empirically, in many cases, this is a worthwhile kind of trade-off because it allows you to run faster, and you could, for example, train longer and make up for that precision decrease. That's BFloat16 for now. As we can see, we are currently at about 300 milliseconds per iteration, and we're now going to reach for some really heavy weapons in the Pytorch arsenal. In particular, we're going to introduce `torch.compile`. `torch.compile` is really quite incredible infrastructure from the Pytorch team, and it's basically a compiler for neural networks. It's almost like GCC for C/C++ code. This is just the GCC of neural nets. It came out a while ago and is extremely simple to use. The way to use `torch.compile` is to do this. It's a single line of code to compile your model and return it. Now, this line of code will cost you compilation time, but as you might guess, it's going to make the code a lot faster. Let's actually run that because this will take some time to run. Remember, we're currently at 300 milliseconds, and we'll see what happens. While this is running, I'd like to explain a little bit of what `torch.compile` does under the hood. Feel free to read this page of Pytorch, but basically, there's no real good reason for you to not use `torch.compile` in your Pytorch. I kind of feel like you should be using it almost by default if you're not, unless you're debugging and you want your code to run really fast. There's one line here in `torch.compile` that I found that actually kind of gets to why this is faster. Speed-up mainly comes from reducing Python overhead and GPU read/writes. Let me unpack that a little bit. 

Here we are. We went from 300 milliseconds, we're now running at 129 milliseconds. This is a 2.3x improvement from a single line of code in Pytorch, quite incredible. What is happening? What's happening under the hood? When you pass the model to `torch.compile`, what we have here in this `nn.Module` is really just the algorithmic description of what we'd like to happen in our network. `torch.compile` will analyze the entire thing, and it will look at what operations you'd like to use. With the benefit of knowing exactly what's going to happen, it doesn't have to run in what's called the eager mode. It doesn't have to just kind of go layer by layer like the Python interpreter normally would. It starts at the forward and the Python interpreter will go, "Okay, let's do this operation," and then, "let's do that operation," and it kind of materializes all the operations as it goes through. These calculations are dispatched and run in this order, and the Python interpreter and this code doesn't know what kind of operations are going to happen later. But `torch.compile` sees your entire code at the same time, and it's able to know what operations you intend to run, and it will optimize that process. The first thing it will do is it will take out the Python interpreter from the forward pass entirely, and it will compile this entire neural net as a single object with no Python interpreter involved. It knows exactly what's going to run, and it will just run that, and it's all going to be running in efficient code. The second thing that happens is the read/write that they mentioned very briefly. A good example of that, I think, is the GELU nonlinearity that we've been looking at. Here, we use the `nn.GELU`. Now, this here is me just breaking up the `nn.GELU`, which you remember has this formula. This here is the equivalent implementation to what's happening inside `nn.GELU`. It's identical. By default, if we were just using this instead of `nn.GELU`, what would happen without `torch.compile`? Well, the Python interpreter would make its way here, and then it would be, "Okay, well, there's an input. Let me first raise this input to the third power." It's going to dispatch a kernel that takes your input and raises it to the third power, and that kernel will run. When this kernel runs, what ends up happening is this input is stored in the memory of the GPU. Here's a helpful example of the layout of what's happening. You have your CPU. This is in every single computer. There's a few cores in there, and you have your RAM, your memory, and the CPU can talk to the memory. This is all well known. Now, we've added the GPU, and the GPU is a slightly different architecture. Of course, they can communicate, but it's different in that it's got a lot more cores than a CPU. All of those cores are individually a lot simpler too, but it also has memory. This high bandwidth memory (HBM). This is the memory, and it's very equivalent to RAM in the computer. What's happening is that input is living in the memory. When you do `input**3`, this has to travel to the GPU, to the cores, and to all the caches and registers on the actual chip of this GPU. It has to calculate all the elements to the third, and then it saves the result back to the memory. It's this travel time that actually causes a lot of issues. Here, remember, this memory bandwidth, we can communicate about 2 terabytes per second, which is a lot, but also we have to traverse this link, and it's very slow. Here, on the GPU, we're on-chip, and everything is super fast within the chip, but going to the memory is extremely expensive and takes a very long amount of time. We load the input, do the calculations, and load back the output, and this round trip takes a lot of time. Now, right after we do that, we multiply by this constant. What happens then is we dispatch another kernel, and then the result travels back. All the elements get multiplied by a constant, and then the results travel back to the memory. Then, we take the result and we add back the input. This entire thing again travels to the GPU, adds the inputs, and gets written back. We're making all these round trips from the memory to actually where the computation happens because all the tensor cores and ALUs and everything like that is all stored on the chip in the GPU. We're doing a ton of round trips, and Pytorch, without using `torch.compile`, doesn't know to optimize this because it doesn't know what kind of operations you're running later. You're just telling it to raise the power to the third, then do this, then do that, and it will just do that in that sequence. But `torch.compile` sees your entire code. It will come here, and it will realize, "Wait, all of these are element-wise operations, and actually, what I'm going to do is I'm going to do a single trip of the input to the GPU. Then, for every single element, I'm going to do all of these operations while that memory is on the GPU, or chunks of it rather, and then I'm going to write back a single time." We're not going to have these round trips. That's one example of what's called kernel fusion and is a major way in which everything is sped up. Basically, if you have your benefit of insight and you know exactly what you're going to compute, you can optimize your round trips to the memory, and you're not going to pay the memory bandwidth cost. That's fundamentally what makes some of these operations a lot faster, and what they mean by read/writes here. Let me erase this because we are not using it. We should be using `torch.compile`, and our code is now significantly faster. We're doing about 125,000 tokens per second, but we still have a long way to go. 

Before we move on, I wanted to supplement the discussion a little bit with a few more figures because this is a complicated topic, but it's worth understanding at a high level what's happening here. I could probably spend an entire video of like two hours on this, but just a preview of that basically. This chip here, the GPU, is where all the calculations happen mostly, but this chip also does have some memory in it. But most of the memory, by far, is here in the high bandwidth memory (HBM) and is connected. They're connected, but these are two separate chips basically. Here, this is a zoom-in of a cartoon diagram of a GPU, and what we're seeing here is, number one, you see this HBM. On the sides here, it says HBM, and so that's the links to the HBM. The HBM is, again, off-chip. On the chip, there are a large number of these streaming multiprocessors. Every one of these is an SM. There are 120 of them in total, and this is where a lot of the calculations happen. This is a zoom-in of a single individual SM. It has these four quadrants, and, for example, the Tensor Core is where a lot of the matrix multiply stuff happens, but there's all these other units to do all different kinds of calculations for FP64, FP32, and for integers, and so on. We have all this logic here to do the calculations, but in addition to that, on the chip, there is memory sprinkled throughout the chip. L2 cache is some amount of memory that lives on the chip, and then on the SMs themselves, there's L1 cache. This blue bar is L1. There are also registers. There is memory stored here, but the way this memory is stored is very different from the way memory is stored in HBM. This is a very different implementation using transistors and capacitors. It's a very different implementation with SRAM and what that looks like. Long story short is, there is memory inside the chip, but it's not a lot of memory. That's the critical point. This is an example diagram of a slightly different GPU just like here where it shows that, for example, typical numbers for CPU RAM memory, which is this thing here, you might have one terabyte of this, but it would be extremely expensive to access, especially for a GPU. You have to go through the CPU here. Next, we have the HBM. We have tens of gigabytes of HBM memory on a typical GPU here, but it's, as I mentioned, very expensive to access. On the chip itself, everything is extremely fast within the chip, but we only have a couple of 10 megabytes of memory collectively throughout the chip. There's just not enough space because the memory is very expensive on the chip, and so there's not a lot of it. It is lightning fast to access in relative terms. Whenever we have these kernels, the more accurate picture of what's happening here is that we take these inputs, which live by default on the global memory, and now we need to perform some calculation. We start streaming the data from the global memory to the chip. We perform the calculations on the chip, and then stream it back and store it back to the global memory. If we don't have `torch.compile`, we are streaming the data through the chip, doing the calculations, and saving to the memory, and we're doing those round trips many, many times. If it's `torch.compiled`, then we start streaming the memory as before, but then, while we're on the chip, we have a chunk of the data that we're trying to process. That chunk now lives on the chip. While it's on the chip, it's extremely fast to operate on. If we have kernel fusion, we can do all the operations right there in an element-wise fashion, and those are very cheap, and then we do a single round trip back to the global memory. Operator fusion basically allows you to keep your chunk of data on the chip and do lots of calculations on it before you write it back, and that gives huge savings, and that's why `torch.compile` ends up being a lot faster, or that's one of the major reasons. Again, just a very brief intro to the memory hierarchy and roughly what `torch.compile` does for you. 

`torch.compile` is amazing, but there are operations `torch.compile` will not find, and an amazing example of that is Flash Attention, to which we turn next. Flash Attention comes from this paper from Stanford in 2022, and it's this incredible algorithm for performing attention and running it a lot faster. Flash Attention will come here, and we will take out these four lines, and Flash Attention implements these four lines really, really quickly. How does it do that? Flash Attention is a kernel fusion operation. Here, in this diagram, they're showing Pytorch, and you have these four operations. They're including dropout, but we are not using dropout here. We just have these four lines of code here, and instead of those, we are fusing them into a single fused kernel of Flash Attention. It's a kernel fusion algorithm, but it's a kernel fusion that `torch.compile` cannot find. The reason that it cannot find it is that it requires an algorithmic rewrite of how attention is actually implemented in this case. What's remarkable about it is that Flash Attention, if you just count the number of FLOPS, Flash Attention does more FLOPS than this attention here. But Flash Attention is actually significantly faster. In fact, they cite 7.6 times faster potentially, and that's because it is very mindful of the memory hierarchy as I described it just now. It's very mindful about what's in high bandwidth memory, what's in the shared memory, and it is very careful with how it orchestrates the computation such that we have fewer reads and writes to the high bandwidth memory. Even though we're doing more FLOPS, the expensive part is the load and store into HBM, and that's what they avoid. In particular, they do not ever materialize this N by N attention matrix, this `ATT` here. Flash Attention is designed such that this matrix never gets materialized at any point, and it never gets read or written to the HBM. This is a very large matrix because this is where all the queries and keys interact, and we're sort of getting, for each head, for each batch element, we're getting a T by T matrix of attention, which is a million numbers, even for a single head at a single batch index. This is a ton of memory, and this is never materialized. The way that this is achieved is that, basically, the fundamental algorithmic rewrite here relies on this online softmax trick, which was proposed previously. The online softmax trick coming from a previous paper shows how you can incrementally evaluate a softmax without having to realize all of the inputs to the softmax to do the normalization. You do that by having these intermediate variables, M and L, and there's an update to them that allows you to evaluate the softmax in an online manner. Flash Attention actually, recently Flash Attention 2 came out as well. It has additional gains to how it calculates Flash Attention. The original paper that this is based on is this online normalizer calculation for softmax. It came out of NVIDIA really early in 2018. This paper says that they propose a way to compute the classical softmax with fewer memory accesses and hypothesize that this reduction in memory accesses should improve softmax performance on actual hardware. They are extremely correct in this hypothesis. It's really fascinating that they're from NVIDIA and that they had this realization, but they didn't actually take it to the actual Flash Attention. That had to come four years later from Stanford. I don't fully understand the historical how this happened, but they do basically propose this online update to the softmax. This is fundamentally what they reuse here to calculate the softmax in a streaming manner. They realize they can actually fuse all the other operations with the online softmax calculation into a single fused kernel, Flash Attention. That's what we are about to use. Great example, I think, of being aware of memory hierarchy, the fact that FLOPS don't matter, that the entire memory access pattern matters, and that `torch.compile` is amazing, but there are many optimizations that are still available to us that potentially `torch.compile` cannot find. Maybe one day it could, but right now, it seems like a lot to ask. Here's what we're going to do. We're going to use Flash Attention. The way to do that basically in Pytorch is we are going to comment out these four lines and we're going to replace them with a single line. Here, we are calling this compound operation in Pytorch called `scaled_dot_product_attention`, and Pytorch will call Flash Attention when you use it in this way. I'm not 100% sure why `torch.compile` doesn't realize that these four lines should just call Flash Attention in this exact way. We have to do it again for it, which in my opinion, is a little bit odd, but here we are. You have to use this compound op. Let's wait for a few moments before `torch.compile` gets around to it. Let's remember that we achieved 6.05661. That's the loss we were expecting to see, and we took 130 milliseconds before this change. We're expecting to see the exact same result by iteration 49, but we expect to see faster runtime because Flash Attention is just an algorithmic rewrite and it's a faster kernel, but it doesn't actually change any of the computation. We should have the exact same optimization. 

We're a lot faster. We're at about 95 milliseconds, and we achieved 6.58. They're basically identical up to a floating-point fudge factor. It's the identical computation, but it's significantly faster, going from 130 to roughly 90, 96. This is maybe a 27% improvement. That is Flash Attention.

We are now getting to one of my favorite optimizations, and it is simultaneously the dumbest and the most brilliant optimization, and it's always a little bit surprising to me. I mentioned a few minutes ago that there are some numbers that are nice and some numbers that are ugly. 64 is a beautiful, nice number. 128 is even nicer. 256 is beautiful. What makes these numbers beautiful is that there are many powers of two inside them. You can divide by two many times. Examples of ugly numbers are like 13 and 17, and something like that, prime numbers, numbers that are not even, and so on. Pretty much you always want to use nice numbers in all of your code that deals with neural networks or CUDA because everything in CUDA works in sort of like powers of two. Lots of kernels are written in terms of powers of two, and there are lots of blocks of sizes 16 and 64, and so on. Everything is written in those terms, and you always have special case handling for all kinds of logic when your inputs are not made of nice numbers. Let's see what that looks like. Basically, scan your code and look for ugly numbers, is roughly the heuristic. 3 times is kind of ugly. I'm not 100% sure, maybe this can be improved, but this is ugly and not ideal. 4 times is nice. That's nice. 1024 is very nice. That's a power of two. 12 is a little bit suspicious, not too many powers of two. 768 is great. 50,257 is a really, really ugly number. First of all, it's odd, and there are not too many powers of two in there. This is a very ugly number, and it's highly suspicious. When we scroll down, all these numbers are nice, except for 25. In this configuration of GPT2 XL, the number of heads is 25. That's a really ugly number. That's an odd number. This did cause a lot of headaches for us recently when we're trying to optimize some kernels to run this fast, and it required a bunch of special case handling. Basically, these numbers, we have some ugly numbers, and some of them are easier to fix than others. In particular, the vocabulary size being 50,257. That's a very ugly number, very suspicious, and we want to fix it. When you fix these things, one of the easy ways to do that is you basically increase the number until it's the nearest power of two that you like. Here's a much nicer number: 50,304. Why is that? Because 50,304 can be divided by 8, or by 16, or by 32, or 64. It can even be divided by 128. It's a very nice number. What we're going to do here is the GPT

Here is the cleaned-up lecture transcript:

The configuration shows that we initialized B cap size to 50,257. Let's override just that element to be 50,304. Everything else stays the same; we are just increasing our vocabulary size. It's almost like we're adding fake tokens so that the vocabulary size has powers of two inside it. By the way, what I'm doing here is increasing the amount of computation that our network will be doing. If you count the flops, we will be doing more flops. We still have to think through whether this breaks anything, but if I just run this, let's see what we get. Currently, this ran in maybe 96.5 milliseconds per step. I'm just kind of eyeballing it, and let's see what kind of result we're going to get. While this is compiling, let's think through whether our code actually works when we increase the vocabulary size like this. Let's look at where vocabulary size is actually used. We swing up to the `inet` and see that it's used inside the embedding table, all the way at the bottom of the Transformer, and at the classifier layer, all the way at the top of the Transformer. So, it's used in two places. We're running at 93 milliseconds instead of 96.5, so we are seeing a roughly 4% improvement here by doing more calculations. The reason for this is we fixed an ugly number and made it a nice number. I will come into the explanation for that a little bit later, but for now, let's just convince ourselves that we're not breaking anything when we do this. First of all, we've made the embedding table for the tokens larger. It's almost like we introduced more tokens at the bottom, and these tokens are never used because the gpt tokenizer only has tokens up to 50,256. We will never index into the rows that we've added, so we're wasting a little bit of space here by creating memory that's never going to be accessed or used. That's not fully correct because this `wte` weight ends up being shared and is used in the classifier here at the end. What is that doing to the classifier? We are predicting additional dimensions at the classifier now. We are predicting probabilities for tokens that will never be present in the training set. Therefore, the network has to learn that these probabilities have to be driven to zero, and so the logits that the network produces have to drive those dimensions of the output to negative infinity. That's no different from all the other tokens that are already in our data set, or rather, that are not in our data set. Shakespeare probably only uses, let's say, a thousand tokens out of 50,257 tokens. Most of the tokens are already being driven to zero probability by the optimization. We've just introduced a few more tokens now that, in a similar manner, will never be used and have to be driven to zero in probability. Functionally, though, nothing breaks. We're using a bit more extra memory, but otherwise, this is a harmless operation as far as I can tell. We're adding calculation, but it's running faster. It's running faster because, as I mentioned, in CUDA, so many kernels use block tiles, and these block tiles are usually nice numbers, powers of two. Calculations are done in chunks of 64 or chunks of 32. When your desired calculation doesn't neatly fit into those block tiles, there are all kinds of boundary kernels that can kick in to do the last part. Basically, in a lot of kernels, they will chunk up your input, do the nice part first, and then they have a whole second phase where they come back to any that remains and process the remaining part. The kernels for that could be very inefficient, so you're basically spinning up all this extra compute that is extremely inefficient. You might as well pad your inputs and make them fit nicely. Usually, that empirically ends up running faster. This is another example of a 4% improvement that we've added, and this is something that torch compile did not find for us. You would hope that torch compile, at some point, could figure out an optimization like this. But for now, this is it. I also have to point out that we're using pytorch nightly, so that's why we're only seeing 4%. If you're using pytorch 2.3.1 or earlier, you would actually see something like a 30% improvement just from this change, from changing it from 50,257 to 50,304. Again, this is one of my favorite examples of having to understand the under-the-hood and how it all works and knowing what kinds of things to tinker with to push the performance of your code. At this point, we have improved the performance by about 11x. We started at about 1,000 milliseconds per step, and now we're down to like 93 milliseconds, so that's quite good, and we are doing a much better job of utilizing our GPU resources. I'm going to now turn to more algorithmic changes and improvements to the actual optimization itself. What we would like to do is follow the hyperparameters that are mentioned in the GPT-2 or GPT-3 paper. Sadly, GPT-2 doesn't actually say too much. It's very nice of them that they released the model weights and the code, but the paper itself is extremely vague as to the optimization details. The code itself, that they released as well, is just the inference code, so there's no training code here, and very few hyperparameters, so this doesn't tell us too much either. For that, we have to turn to the GPT-3 paper. In the appendix of the GPT-3 paper, they have a lot more hyperparameters for us to use. The GPT-3 paper, in general, is a lot more detailed as to all of the small details that go into the model training. GPT-3 models were never released, so GPT-2 has the weights but no details, and GPT-3 has lots of details but no weights. Roughly speaking, GPT-2 and GPT-3 architectures are very similar. There are very few changes. The context length was expanded from 1024 to 2048, and that's kind of the major change. Some of the hyperparameters around the Transformer have changed, but otherwise, they're pretty much the same model. It's just that GPT-3 was trained for a lot longer on a bigger data set and has a lot more thorough evaluations. The GPT-3 model is 175 billion parameters instead of 1.6 billion in GPT-2. Long story short, we are going to the GPT-3 paper to follow along with some of the hyperparameters. To train all the versions of GPT-3, they used Adam with beta 1 and beta 2 of .9 and .95. Let's swing over here and make sure that the betas parameter, which you can see here defaults to 0.9 and .999, is actually set to 0.9 and .95. The epsilon parameter, you can see is the default is 1e-8, and this is also 1e-8. Let's just put it in so that works explicitly. Next up, they say they clip the global norm of the gradient at 1.0. What this is referring to is that once we calculate the gradients, right after `l.backward`, we have the gradients at all the parameter tensors. People like to clip them to have some kind of a maximum norm. In PyTorch, this is fairly easy to do. It's one line of code that we have to insert right after we calculate the gradients. This utility function calculates the global norm of the parameters. Every single gradient on all the parameters, you square it, add it all up, and take a big square root of that, and that's the norm of the parameter vector. Basically, it's the length of it. We are making sure that its length is no more than 1.0, and we're going to clip it. The reason that people like to use this is that sometimes you can get unlucky during your optimization, maybe it's a bad data batch or something like that. If you get very unlucky in the batch, you might get a really high loss, and really high loss could lead to a really high gradient. This could basically shock your model and shock the optimization. People like to use gradient norm clipping to prevent the model from getting too big of shocks in terms of the gradient magnitude and upper bounding it in this way. It's a bit of a hacky solution; it's like a patch on top of deeper issues, but people still do it fairly frequently. The `clip_grad_norm` returns the norm of the gradient, which I like to always visualize because it is useful information. Sometimes you can look at the norm of the gradient. If it's well-behaved, things are good; if it's climbing, things are bad and they're destabilizing during training. Sometimes you could get a spike in the norm, which means there is some kind of issue or an instability. The norm here will be a norm, and let's do a `4f` or something like that. I believe this is just a float, and we should be able to print that. That's global gradient clipping. Now, they go into the details of the learning rate scheduler. They don't just use a fixed learning rate like we do here, 3e-4, but there is basically a cosine decay learning rate schedule. It's got a warm-up, and it's got a cosine decay to 10% over some horizon. We're going to implement this in a second. I just like to see the norm printed here. There we go. What happened here is the norm is actually really high in the beginning, 30 or so, and you see that as we continue training, it stabilizes at values below one. This is not that crazy or uncommon for the norm to be high in the very first few stages. What's happening here is the model is completely random, so there's a ton of learning happening very early in the network, but that learning is kind of like, you know, it's mostly learning the biases of the output tokens, so it's a bit of an unstable time, but the network usually stabilizes in a very few iterations. This looks relatively reasonable to me, except usually, I would expect, this looks a little bit funky that we go from 28 to 6 to 2 and then to 10. It's not completely insane, but it's just kind of a little bit funky. Let's now get to the learning rate scheduler. The learning rate schedule that's used here in GPT-3 is what's called a cosine decay learning schedule with warm-up. The way this looks is that the learning rate starts right at around zero, linearly ramps up over some amount of time, and then comes down with this cosine form and comes down to some minimum learning rate that's up to you. Here, the minimum learning rate is zero, but here in the paper, they said that they use cosine decay for learning rate down to 10% of its value over the first 260 billion tokens, and then training continues at 10% after that. There's a linear warm-up over the first 375 million tokens. That's about the learning rate. Let's now implement this. I already implemented it here, and the way this works is, let me scroll down first. I changed our training loop a little bit. This was `for i in max_steps`, I just changed it to `step` so that we have the notion of a step as a single optimization step in the for loop. Here, I get the LR for this step of the optimization using a new function I call `get_LR`. In PyTorch, to set the learning rate, this is the way to set the learning rate; it's a little bit gnarly. You have to iterate over different parameter groups that could exist in the optimizer, even though we currently have a single parameter group only, and you have to set the LR in this `for` loop style. We have this `get_LR`, we set the learning rate, and then on the bottom, I'm also printing it. That's all the changes I made to this loop, and then, of course, the `get_LR` is my scheduler. It's worth pointing out that PyTorch actually has learning rate schedulers, and you can use them. I believe there's a cosine learning rate schedule in PyTorch. I don't really love using that code because honestly, it's like five lines of code, and I fully understand what's happening inside these lines. I don't love to use abstractions where they're kind of inscrutable and then I don't know what they're doing. It's a personal style. The max learning rate here is let's say, 3e-4, but we're going to see that in GPT-3 here, they have a table of what the maximum learning rate is for every model size. For this one, a 12-layer, 768 GPT-3, so the GPT-3 small is roughly like a GPT-2 124M. We see that here, they use a learning rate of 6e-4. We could actually go higher. In fact, we may want to try to follow that and just set the max LR here at six. That's the maximum learning rate. The minimum learning rate is 10% of that, per the description in the paper. Some number of steps that we're going to warm up over, and then the maximum steps of the optimization, which I now also use in the for loop down here. You can go over this code if you like. It's not terribly insightful. I'm just modulating based on the iteration number which learning rate there should be. This is the warm-up region, this is the region after the optimization, and this is the region in between. This is where I calculate the cosine learning rate schedule, and you can step through this in detail if you'd like. This is basically implementing this curve, and I ran this already, and this is what it looks like. When we run, we start at some very low number. Note that we don't start exactly at zero because that would not be useful to update with a learning rate of zero. That's why there's an `it+1`, so that on the zeroth iteration, we are not using exactly zero, we're using something very low. Then we linearly warm up to the maximum learning rate, which in this case was 3e-4 when I ran it, but now would be 6e-4. Then it starts to decay all the way down to 3e-5, which was, at the time, 10% of the original learning rate. One thing we are not following exactly is that they mentioned that, let me see if I can find it again, we're not exactly following what they did because they mentioned that their training horizon is 300 billion tokens, and they come down to 10% of the initial learning rate at 260 billion, and then they train after 260 with 10%. Basically, their decay time is less than the max steps time, whereas for us, they're exactly equal, so it's not exactly faithful, but it's an okay, this is okay for us and for our purposes right now. We're just going to use this ourselves. I don't think it makes too big of a difference, honestly. I should point out that what learning rate schedule you use is totally up to you. There are many different types. Cosine learning rate has been popularized a lot by GPT-2 and GPT-3, but people have come up with all kinds of other learning rate schedules. This is an active area of research as to which one is the most effective at training these networks. Next up, the paper talks about the gradual batch size increase, so there's a ramp on the batch size that is linear. You start with a very small batch size and you ramp up to a big batch size over time. We are actually going to skip this and we are not going to work with it. The reason I don't love to use it is that it complicates a lot of the arithmetic because you are changing the number of tokens that you're processing at every single step of the optimization, and I like to keep that math very simple. Also, my understanding is that this is not like a major improvement. My understanding is that this is not like an algorithmic optimization improvement, it's more of a systems and speed improvement. Roughly speaking, this is because in the early stages of the optimization, again, the model is in a very atypical setting. Mostly, what you're learning is that you're mostly learning to ignore the tokens that don't come up in your training set very often. You're learning very simple biases and that kind of thing. Every single example that you put through your network is basically just telling you use these tokens and don't use these tokens. The gradients from every single example are extremely highly correlated. They all look roughly the same in the original parts of the optimization because they're all just telling you that these tokens don't appear and these tokens do appear. Because the gradients are very similar and they're highly correlated, why are you doing batch sizes of millions when, if you do a batch size of 32k, you're basically getting the exact same gradient early on in the training. Later in the optimization, once you've learned all the simple stuff, that's where the actual work starts and that's where the gradients become more decorrelated per example, and that's where they actually offer you statistical power, in some sense. We are going to skip this because it complicates things, and we are going to go to data being sampled without replacement during training, until an epoch boundary is reached. Without replacement means that they are not sampling from some fixed pool, then take a sequence, train on it, but then also return the sequence to the pool. They are exhausting a pool. When they draw a sequence, it's gone until the next epoch of training. We're already doing that because our data loader iterates over chunks of data. There's no replacement. They don't become eligible to be drawn again until the next epoch. We're basically already doing that. All models use a weight decay of 0.1 to provide a small amount of regularization. Let's implement a weight decay. You see here that I've already made the changes. Instead of creating the optimizer right here, I am creating a new `configure_optimizers` function inside the model, and I'm passing in some of the hyperparameters instead. Let's look at the `configure_optimizers` which is supposed to return the optimizer object. It looks complicated, but it's actually really simple. We're just being very careful, and there are a few settings here to go through. The most important thing with respect to this line is that there is a weight decay parameter here, and I'm passing that into something called `optim_groups` that eventually ends up going into the AdamW optimizer. The weight decay that's by default used in AdamW here is 0.01, so it's 10 times lower than what's used in the GPT-3 paper here. The weight decay basically makes its way into the AdamW through the optimizer groups. What else is going on here in this function? The two things that are important here are that I'm splitting up the parameters into those that should be weight-decayed and those that should not be weight-decayed. It is common to not weight-decay biases and any other one-dimensional tensors. The one-dimensional tensors are in the `no_decay_prams`. These are also things like layer norm scales and biases. It doesn't make sense to weight-decay those. You mostly want to weight-decay the weights that participate in matrix multiplications, and you want to potentially weight-decay the embeddings. We've covered in a previous video why it makes sense to decay the weights because you can sort of view it as a regularization. When you're pulling down all the weights, you're forcing the optimization to use more of the weights, and you're not allowing any one of the weights individually to be way too large. You're forcing the network to distribute the work across more channels because there's a pull of gravity on the weights. That's why we're separating it in those ways. Here, we're only decaying the embeddings and the matrix multiplication weights. We're printing the number of parameters that we are decaying and not. Most of the parameters will be decayed. One more thing that we're doing here is another optimization. Previous AdamW did not have this option, but later parts of PyTorch introduced it. I'm guarding it with an `inspect.signature`, which is basically checking if this `fused` keyword is present inside AdamW. If it is present, I'm going to use it and pass it in here because some earlier versions do not have `fused=`. Here's `adamw(fused=)`, it did not used to exist, and it was added later. There are some docs here for what's happening. By default, they do not use fused because it is relatively new, and we want to give it sufficient time. By default, they don't use fused, but fused is a lot faster when it is available and when you're running on CUDA. What that does is instead of iterating in a for loop over all the parameter tensors and updating them, that would launch a lot of kernels. Fused just means that all those kernels are fused into a single kernel. You get rid of a lot of overhead, and you call a single kernel on all the parameters that updates them. It's a kernel fusion for the AdamW update instead of iterating over all the tensors. That's the `configure_optimizers` function that I like to use. We can rerun it. We're not going to see any major differences from what we saw before, but we are going to see some prints coming from here. We see that the number of decay tensors is 50, and it's most of the parameters. The number of non-decay tensors is 98, and these are the biases and layer norm parameters mostly. There are only 100,000 of those, so most of it is decayed. We are using the fused implementation of AdamW, which will be a lot faster. If you have it available, I advise you to use it. I am not 100% sure why they don't default to it. It seems fairly benign and harmless. Also, because we are using the fused implementation, I think this is why we have dropped. Notice that the running time used to be 93 milliseconds per step, and now we're down to 90 milliseconds per step because of using the fused AdamW optimizer. In a single commit here, we are introducing fused Adam, getting improvements on the time, and we're adding or changing the weight decay, but we're only weight-decaying the two-dimensional parameters, the embeddings, and the matrices that participate in the linear. That is it for this line. One more quick note before we continue. I just want to point out that the relationship between weight decay, learning rate, batch size, the Adam parameters beta 1, beta 2, epsilon, and so on, are very complicated mathematical relationships in the optimization literature. For the most part, in this video, I'm just trying to copy-paste the settings that OpenAI used. This is a complicated topic, quite deep. In this video, I just want to copy the parameters because it's a whole different video to really talk about that in detail and give it proper justice, instead of just high-level intuitions. The next thing that I want to move on to is this paragraph here. By the way, we're going to turn back around to this when we improve our data loader. For now, I want to swing back around to this table, where you will notice that, for different models, we have different hyperparameters for the Transformer that dictate the size of the Transformer network. We also have a different learning rate. We see the pattern that the bigger networks are trained with slightly lower learning rates. We also see this batch size, where, in the small networks, they use a smaller batch size, and in the bigger networks, they use a bigger batch size. The problem for us is we can't just use a 0.5 million batch size. If I just try to come in here and I try to set, where is my `b`?, where do I call the data?  `b=16`. If I try to set, we have to be careful, it's not 0.5 million, because this is the batch size in the number of tokens. Every single one of our rows is 1024 tokens, so 0.5e6 / 1024 would need about a 488 batch size. I can't come in here and set this to 488 because my GPU would explode. This would not fit for sure. We still want to use this batch size because, as I mentioned, the batch size is correlated with all the other optimization hyperparameters and the learning rates and so on. We want to have a faithful representation of all the hyperparameters, and therefore we need to use a batch size of 0.5 million roughly. The question is how do we use 0.5 million if we only have a small GPU? For that, we need to use what's called gradient accumulation. We're going to turn to that next. It allows us to simulate in a serial way any arbitrary batch size that we set. We can do a batch size of 0.5 million, we just have to run longer, and we have to process multiple sequences, and basically, add up all the gradients from them to simulate a batch size of 0.5 million. Let's turn to that next. I started the implementation right here just by adding these lines of code. I set the total batch size that we desire. This is exactly 0.5 million. I used a nice number, a power of two, because 2 to the 19 is 524,288, so it's roughly 0.5 million, and it's a nice number. Our micro batch size is 16. This is still the `b` that goes into the Transformer and does forward and backward, but we're not going to do an update. We are going to do many forward-backwards. Those gradients are going to plus equal on the parameter gradients; they are all going to add up. We are going to do forward-backward `grad_accum_steps` number of times, and then we are going to do a single update once all that is accumulated. Our micro batch size is controlling how many tokens, how many rows, we're processing in a single go over a forward-backward. Here, we are doing 16 times 1024; we are doing 16,384 tokens per forward-backward. We are supposed to be doing 2 to the 19 in total, so `grad_accum` will be 32. `grad_accum` here will work out to 32. We have to do 32 forward-backwards, and then a single update. We see that we have about 100 milliseconds for a single forward-backward, so doing 32 of them will make every step roughly 3 seconds, just napkin math. That's `grad_accum_steps`, but now we have to implement that. We are going to swing over to our training loop because now this part here and this part here, the forward and the backward, we have to repeat this 32 times before we do everything else that follows. Let's see how we can implement that. Let's come over here. We do have to load a new batch every single time, so let me move that over here. Now, this is where we have the inner loop. For micro-step in range `grad_accum_steps`, we do this. Remember that `l.backward` always deposits gradients. Inside `loss.backward`, there is always a plus equals on the gradients, so in every `l.backward`, gradients will add up on the gradient tensors. We do `loss.backward`, we get all the gradients over there, and then we normalize. Everything else should just follow. We're very close, but there's a subtle and deep issue here, and this is actually incorrect. I invite you to think about why this is not yet sufficient, and let me fix it then. I brought back the Jupyter Notebook so we can think about this carefully in a simple toy setting and see what's happening. Let's create a very simple neural net that takes a 16 vector of 16 numbers and returns a single number. Here, I am creating some random examples, `X`, and some targets, `Y`. We are using the mean squared loss here to calculate the loss. Basically, this is four individual examples, and we are just doing simple regression with the mean squared loss over those four examples. When we calculate the loss and we call `loss.backward` and look at the gradient, this is the gradient that we achieve. The loss objective here, notice that in MSE loss, the default for the loss function is `reduction='mean'`, so we are calculating the average mean loss. This is the exact loss objective. This is the average, the one over four because there are four independent examples here. We have the four examples and their mean squared error, the squared error, and this makes it the mean squared error. We calculate the squared error and then normalize it to make it the mean over the examples. There are four examples here. Now, when we come to the gradient accumulation version of it, this here is the gradient accumulation version of it, where we have `grad_accum_steps` of four, and I reset the gradient. We have `grad_accum_steps` of four and now I'm evaluating all the examples individually instead and calling `loss.backward`.

We forward our function and calculate the exact same loss, then perform a backward pass. We do this four times. When we look at the gradient, you will notice that the gradients do not match. We did a single batch of four, and then we did four gradient accumulation steps of batch size one. The gradients are not the same because the mean squared error gets lost; specifically, the one quarter in this loss is lost. The loss objective for every loop is just a mean squared error. In this case, because there's only a single example, it is just this term. That was the loss in the zeroth iteration, and the same applies for the first, second, and third. When you do the loss backward, we are accumulating gradients. The accumulation in the gradient is equivalent to doing a sum in the loss. Our loss here is without the factor of one quarter outside of it. We are missing the normalizer, and therefore our gradients are off. To fix this, we can say `loss = loss / 4`. We are scaling our loss, introducing a one quarter in front of all of these places. The individual losses are now scaled by one quarter. When we backward, all of these accumulate with a sum, but there's a one quarter inside every one of these components. Now, our losses will be equivalent. When I run this, you see that the gradients are now identical.  The reason this is not correct is that the loss we're calculating in the model is using a reduction of the mean. The cross-entropy loss has a reduction by mean at all the BYT elements. If we are just doing this gradient accumulation, we are missing that. The way to fix this is to compensate for the number of gradient accumulation steps. We can divide this loss by the gradient accumulation steps. We are scaling down the loss so that when we do loss backward, which corresponds to a sum in the objective, we are summing up the already normalized loss. When we sum the losses divided by the gradient accumulation steps, we are recovering the additional normalizer. This is equivalent to the original optimization because the gradient will come out the same. To print things nicely, we need an accumulator over the loss. We cannot just print the loss because we would only print the final loss at the final micro-step.  We initialize `loss_accum` at zero, and accumulate the loss into it, detaching the tensor from the graph. We are making these leaf nodes when we add them so that we keep track of the values. We print this accumulator instead of the loss.  We also need to account for the gradient accumulation steps inside the tokens processed. The tokens processed per step is `B * T * gradient_accumulation_steps`.  Now the optimization looks reasonable. We calculated the gradient accumulation steps to be 32, and are getting about 3 seconds. If you want to verify that your optimization and the implementation is correct, you can adjust the batch size. Because we have the total batch size and the gradient accumulation steps, our setting of `B` is purely a performance optimization. If you have a big GPU, you can increase this to 32; if you have a small GPU, you can try 8 or 4. In any case, you should be getting the exact same optimization and answers up to a floating-point error, because the gradient accumulation can handle everything serially.

We will use eight GPUs to collaborate and optimize over tokens at the same time. They will communicate so they are all collaborating on the optimization. We will use distributed data parallel from PyTorch, not the legacy data parallel.  We will launch eight processes, and each process will be assigned to a GPU. For each process, the training loop and everything else will look pretty much the same. Each GPU is working on exactly what we have built so far. There will be eight of them, all processing slightly different parts of the data. Once they all calculate their gradients, we will average the gradients. That is how they will be collaborating on the computational workload. To use all eight of them, we will run our script with a special command called `torchrun`. When `torchrun` runs our Python script, it will run eight of them in parallel, creating environment variables that each process can use to look up which process it is. `Torchrun` will set `rank`, `local_rank`, and `world_size` environment variables. If DDP is running, we must make sure that CUDA is available. The important part is `world_size`, which will be eight. `Rank` is an integer, and each process will run the exact same code but will have a different DDP rank. GPU0 will have a DDP rank of zero, GPU1 will have a rank of one, and so on. They are all running the same script; only the DDP rank will be different. That is how we will coordinate that they do not run on the same data. `Local_rank` is only used in a multi-node setting. We only have a single node with eight GPUs, so `local_rank` is the rank of the GPU on a single node, from zero to seven. For us, we are mostly running on a single box. We care about `Rank` and `World_size`. According to the `local_rank`, we set the device to be `cuda:{local_rank}` to ensure that there are no collisions on which GPU is used by which process.  We have a boolean variable to indicate that DDP rank is equal to zero, which we call the master process. The master process does the printing, logging, checkpointing, etc. The other processes are mostly compute processes that assist. The master process zero will have some additional work to do. If we are not using DDP, none of these variables are set. We revert back to single GPU training. We have rank zero, `world_size` is one, and we are the master process. We autodetect the device. So far, we have initialized DDP. When we run with `torchrun`, there will be eight copies running in parallel, each with a different rank. We must make sure that everything happens correctly afterwards. Because there are eight processes running in parallel, the code should be read while imagining that eight Python interpreters are running. The only difference is that they have a different DDP rank. They all pick the exact same seed, and make all of the same calculations, unaware of the other copies. We have to adjust these calculations to take into account the `world_size` and `rank`. The micro-batches and sequence lengths are all per GPU. There will be `num_processes` of them running in parallel. The gradient accumulation steps are now `total_B_size / (B * T * DDP_world_size)`. We also want to make sure this fits nicely into the total batch size. Each process will do `B * T`, and there are this many of them. Each GPU calculates this. If we were to print here, every process would print. To avoid this, we guard the print statement with `if master_process`, so that we only print once. Before the data loader, let us print and exit. When we run with `torchrun`, and the number of processes is eight, `torchrun` will run eight of these. First, it gets busy, with warnings from distributed, which are related to setup. The first print comes from process 5, then process 5 exits, and DDP does not like this. In an actual application, we would call `destroy_process_group` to clean up DDP. Next, we need to make the data loader aware of this multi-process setting. We do not want all the processes loading the same data; we want each process to get its own chunk.  We pass the rank and size to the data loader. The current position will not be zero. We stride out all the processes. We take `B * T` and multiply it by the process rank. Process rank 0 will start at zero, process rank one starts at `B * T`, and process rank two starts at `2 * B * T`, etc. When we advance, we do not advance by `B * T`; we advance by `B * T * num_processes`. The total number of tokens consumed is `B * T * num_processes`. They go off to a different rank. The position has to advance by the entire chunk. If `B * T * num_processes + 1` exceeds the number of tokens, then we loop. This is the simplest change for a distributed data loader. If process rank is zero and num processes is one, the whole thing will be identical to what we had before, but now we can have multiple processes running.

Once all processes initialize the data loader, they come here, and they all create a GPT model. They create eight GPT models on eight processes. Because the seeds are fixed, they all create the same identical model. They all move it to the device of their rank, and they all compile the model. The models are identical, so there are eight identical compilations happening in parallel. Now, when we construct the model, we need to wrap the model into the distributed data parallel container. This is how we do it. `Device_ids` has to be passed in, and this is supposed to be the DDP local rank. This wraps the model. In a forward pass, it behaves identically. My understanding is that nothing should be changed in the forward pass. In the backward pass, once the backward passes over each independent GPU, each has the gradient for all the parameters. What DDP does is, once the backward pass is over, it will do an average of those gradients across all the ranks and deposit that average on every single rank. Every rank will end up with the average. That's the communication. It synchronizes and averages the gradients. DDP can dispatch communications for the gradient while the backward pass is still happening. There is overlap of the communication of the gradient, the synchronization, and the backward pass. The forward is unchanged, and the backward is mostly unchanged. We are tacking on the average. In the optimization loop, when you do `loss.backward()`, it will do the backward pass and synchronize the gradients.  Because of the gradient accumulation steps loop, we don't want to do the synchronization after every single loss backward. We only want to synchronize on the last step, when the micro step becomes `grat_accum_steps - 1`. To do that, we can use the `no_sync` context manager. However, instead of using a context manager, I directly toggle the `require_backward_grad_sync` variable. Before the `loss.backward()`, if we are using DDP, we only want to synchronize when it is the final iteration. In all other iterations, we want it to be false. I toggle the variable directly. `Require_backward_grad_sync` should only turn on when the micro step is the last step. This allows me to avoid the use of context managers and code duplication.  `Loss.backward()` will not synchronize most of the steps, only the last step. Once this is over, every rank will have the average of all gradients stored on all the ranks. We have averaged the gradients, but the `loss_accum` has not been impacted. This is outside of the DDP container, and is not being averaged. When we print `loss_accum`, we will only be printing the losses from the master process. We want to print the loss over all the processes and the average of that loss. We will average the `loss_accum` by using `dist.all_reduce`. This `loss_accum` tensor exists on all ranks. When we call the `all_reduce` of average, it creates the average of those numbers and deposits that average on all the ranks. All the ranks after this call will contain `loss_accum` averaged. When we print on the master process, `loss_accum` is identical in all the other ranks. We also need to be careful because we are processing more tokens, so we multiply by the DDP `world_size`. Finally, we want to destroy the process group so we are nice to NCCL. The model needs to be wrapped into DDP. The `configure_optimizers` function is now in the `raw_model` so that has to be used.

When we compare a single GPU run with 32 gradient accumulation steps to a multi-GPU run, the numbers will not exactly match up. In the data loader, we are iterating through batches in a slightly different way. If a page of data for all the GPUs exceeds the number of tokens, we loop. The single GPU and the 8 GPU process will end up resetting in a slightly different manner, making batches slightly different. To convince yourself that this is okay, make the total batch size much smaller. I used 32,768 as the total batch size.  Then, I made sure that the single GPU would do eight gradient accumulation steps, and then the multi-GPU.  This reduces the boundary effects of the data loader, and the numbers will match. We are now going really, really fast.  The optimization is mostly consistent with GPT-2 and three hyper parameters. We have outgrown our tiny Shakespeare file and want to upgrade it.

GPT-2 used the WebText dataset, which was never released. OpenWebText is an attempt at reproducing it. They scraped all outbound links from Reddit with at least three karma. This was 45 million links, and ended up being 40 GB of text. GPT-3 uses Common Crawl, which is a random subset of the internet and is extremely noisy.  People go to great lengths to filter Common Crawl. GPT-3 also uses WebText from before, books, and Wikipedia. This data set for GPT-3 was never released. Some of the data sets that are commonly used are the Red Pajama dataset, specifically the Slim Pajama subset, which is cleaned and deduplicated. This dataset contains Common Crawl, C4, which is also Common Crawl but processed differently. It also includes GitHub books archive, Wikipedia, and Stack Exchange. The FineWeb dataset is an attempt to collect high-quality Common Crawl data, filtering to 15 trillion tokens. Hugging Face released FineWeb-Edu, which is 1.3 trillion of educational and 5.4 trillion of high educational content. They filtered Common Crawl to high-quality educational subsets.  We will use this sample 10 billion tokens sub-sample of it because it is empirically close to GPT-2 performance. Our goal will be to download, process, and make sure that our data loader can work with it.

Okay, so I introduced another file that will download the Fine Web Edu dataset from Hugging Face. It will pre-process and pre-tokenize all of the data, and save data shards to a folder on the local disk. While this is running, I wanted to briefly mention that you can look through the dataset viewer to get a sense of what's in it. It seems to be working fairly well, discussing topics like nuclear energy in France and Mexican America. The filters were applied automatically using LLaMA 370B, where the LLMs are judging which content is educational and allowing that content through.

In terms of the script, I'm not going to go through the full script since it's not as interesting or LLM-centric. When you run this, it first loads the dataset, which is all Hugging Face code. You'll need to pip install datasets. Then it tokenizes all the documents within the dataset. When tokenizing a single document, it starts the tokens with the end-of-text token. This is a special token in the GPT2 tokenizer, with the ID 50256, and it begins the document. It extends this with all of the document's tokens. Then it creates a NumPy array from that, ensuring that all tokens are between 0 and 65,535, since the GPT2 max token ID is well below that. We're using uint16 to save space. There's a bunch of multiprocessing code, which isn't that exciting. We're loading the dataset, tokenizing it, and saving everything to shards, which are NumPy files very similar to torch tensors. The first shard, 0000, is a validation shard, and all other shards are training shards, each containing 100 million tokens. This makes it easier to work with the files, as a single massive file can be hard to manage on disk.

We'll let this run, which will take about 30 minutes or so. Then we will come back to actually train on this data. This is a good dataset, we're doing a lot of tokens per second, we have 8 GPUs, and the code is ready for a serious training run.

Okay, so we're back. If we list the 'edu_fine_web' directory, we see 100 shards. Each shard is 100 million tokens, so 100 shards are 10 billion tokens in total.  Switching over to the main file, I made some adjustments to our data loader because we're not using Shakespeare anymore. Instead, we want to use the Fine Web shards. There's code here that loads these shards, loading the uint16 NumPy file and converting it to a torch.long tensor. Then we're enumerating all the shards. I also added a split to the data loader so we can load the train split and also the validation split. We also have not just the current position but also the current shard. When we run out of tokens in a single shard, we first advance the shard and loop if we need to, then we get the tokens and readjust the position. This data loader will now iterate all the shards.

Our train loader now has a train split. We're doing 2 to the 19th tokens per step and want to do roughly 10 billion tokens, which is how many unique tokens we have. If we divide 10 billion tokens by 2 to the 19th, we get 19,731 steps. The GPT-3 paper says to warm up the learning rate over 375 million tokens. Dividing 375 million by 2 to the 19th, we get 715 steps. So, warm-up steps is set to 715, which matches the warm-up schedule that GPT-3 used. 715 is very mild and could be made more aggressive, maybe even 100. However, let's leave it for now to have the exact hyperparameters of GPT-3.

I also adjusted the batch size. I believe I can fit 64 on my GPU as a micro batch size. That means 64 * 124 per GPU, and we have 8 GPUs. This would result in the full batch size without gradient accumulation. If this works, then this is basically a serious pre-training run. We're not logging or evaluating the validation split yet, but if we let this run for a while, we're going to get a pretty good model, perhaps even on par with or better than GPT2 124M.

Okay, it looks like everything is going great. We're processing 1.5 million tokens per second, doing 330 milliseconds per iteration, and have a total of 19,731 iterations to do, which will take about 1.7 hours. We don't even have to use gradient accumulation. If you don't have that luxury with your GPU, just decrease the batch size until things fit, but keep it to nice numbers.  We're currently warming up the learning rate.

Now I'd like to add evaluations on the validation split, logging, and visualizations of our losses.

I've adjusted the code to evaluate on the validation split by creating a Val loader by passing in `split = "Val"`. In the data loader, I introduced a new function `reset`, which is called at init and resets the data loader. In the main training loop, every 100th iteration, including the zeroth iteration, we put the model into evaluation mode, reset the Val loader, and accumulate the gradients over 20 steps, and then print the validation loss. This is the same logic as the training loop, but with no loss backward. It is just inference.

This will print the validation loss every 100th iteration. It will also allow us to take the GPT2 124M as OpenAI released it and see what kind of loss it achieves on this validation loss. This is not a fair comparison to GPT2, as it was trained on a different distribution. However, a validation split is important in a training run to make sure that we are not overfitting. This is especially a concern if we were to train for more epochs.

I deleted orphaned code for sampling and moved it up to the main loop. Once in a while, we validate, and once in a while, we sample, doing this every 100 steps while training on every step. The only thing I did is create a generator object in PyTorch for sampling to have direct control over the random numbers and not impact the training RNG. I'm using a special sampling RNG and making sure to seed it.

We're running a bit slower because I had to disable `torch.compile` to get sampling to work. It gives me a very scary error, which I hope to resolve later. I will be releasing all this code, with git commits to document each change.

I have the optimization running and we're on step 6,000. While this is training, I would like to introduce the HSwag eval. HSwag is a sentence completion dataset with multiple-choice questions. For each question, there's a shared context, and multiple options. The idea is that the multiple choices are constructed so one is a natural continuation and the others are not. Models that are trained well will be able to tell these apart. The sentences are sourced from ActivityNet and WikiHow. HSwag is interesting because the incorrect options are sourced adversarially from language models, which makes it harder for other language models. Humans have a 95% accuracy on it, but language models at the time had around 48%. It is an evaluation that has early signals.

The way we will evaluate is that we have a shared context. Instead of giving the model a multiple-choice question and asking for A, B, C, or D, we will give it a completion task, which is native to how these models work. So, we construct a batch of four rows, the shared context tokens, and then the four options. Only one is correct.  These options may be of different lengths, so we take the longest length and pad the others. The mask then identifies the active tokens, with zeros for the padded areas.  To get the language model to predict A, B, C, or D, we will look at the tokens and pick the option that gets the highest average probability.

Now let's implement this and incorporate it into the script.

I introduced a file called `hell_swag.py`. I'm not going to go through all of it because it's tedious. I'm downloading HSwag from GitHub and rendering its 10,000 examples into this 4xT format. The `render_example` function returns the tokens, mask, and correct label. The evaluate function loads GPT2 from Hugging Face and runs the eval. It calculates the cross-entropy loss of predicting the next token in a sequence and then picks the row with the lowest average loss. This gives the predicted option. If you run this, you'll see GPT2 124M gets 29.5% accuracy. GPT2 XL gets about 49%, so these are fairly low values for current language models. There are other numbers from Uther Harness that are slightly different.

I'm going to incorporate this eval into our main training script to track how HSwag evolves over time.

In `train_gpt2.py`, I made `torch.compile` optional, disabling it by default because it breaks evaluation and sampling. I created a log directory for a log.txt file, recording train loss, validation loss, and HSwag accuracies. A variable indicates the last step. Periodically, every 250th iteration or the last step, we evaluate the validation loss. Then, also every 250th iteration, we evaluate HSwag if not using compile, and we sample from the model. We then perform the training step. The only addition is the section for HSwag. I'm getting all GPUs to collaborate on this. We're iterating over the examples, and each process picks the examples assigned to it. We render an example, put it on the GPU, and get the logits. Then we use a helper function to predict the option with the lowest loss. We keep count of the correct predictions. Multiple processes synchronize their stats, packaging them into tensors, calling `all_reduce` and summing. The master process will then print the HSwag accuracy.

I'm currently running the optimization, and we just had a generation. We are halfway done at step 10,000 out of 20,000. The predictions are less random, and the model seems more self-aware.

Let's wait for the optimization to finish and see what kind of samples we get, as well as look at train loss, validation loss, and HSwag accuracy.

Focusing on the Jupyter Notebook, I have created a new cell that visualizes the train loss, validation loss, and HSwag score. We ran for 19,731 billion tokens, which is one epoch of the sample 10B of Fine Web Edu. In blue, we have training loss; in orange, validation loss. The red horizontal line shows the OpenAI GPT2 124M model evaluated on the validation set of Fine Web Edu. We are surpassing the validation loss of this model. The data set distributions are different, so it's not an entirely fair comparison, but it's a good cross-check. The ideal is to use a standard, withheld evaluation like HSwag. We see that our HSwag performance improved from 25%. We surpassed the GPT2 124M model, with green marking the GPT3 model at 124M. We were able to surpass the GPT2 model using only 10 billion tokens compared to GPT2's 100 billion.

There are many reasons why we could match or surpass this accuracy with fewer training tokens. First, OpenAI GPT2 was trained on a much wider data distribution, and the Fine Web Edu dataset is all English. Perhaps math, code, and multilingual data were stealing capacity from the original GPT2 model. HSwag is also fairly old, so some aspects may have made it into the training set.

Looking at the loss curve, it looks wrong, which I suspect is because the 10 billion sample of Fine Web Edu was not properly shuffled, and there is some periodicity to it. I think we are inheriting the ordering in the data set. This is not ideal, but I hope to fix it by the time this is released.

Okay, here is the cleaned-up transcript:

Some of the things in this repo will hopefully be fixed. I will release this build n GPT repo. Right now it looks a little ugly and preliminary, so hopefully, by the time you get here it's nicer. Down here, I'm going to show results and talk about some of the things that happened after the video. I expect that we will have fixed a small issue, but for now, this shows that our training is not completely wrong. It shows that we're able to surpass the accuracy with only 10x the token budget. Possibly, it could also be that the dataset has improved. The original GPT2 dataset was web text, and it is possible that not a lot of care and attention went into that data set. This was very early in LLMs. Now there is a lot more scrutiny on good practices around deduplication, filtering, quality filtering, and so on. It's possible that the data we're training on is just of higher quality per token, and that could be giving us a boost as well. There are a number of caveats to think about, but for now, we're pretty happy with this. Now the next thing I was interested in, as you see it’s morning now, so there was an overnight run, and I wanted to see how far I could push the result. To do an overnight run, I basically did instead of one epoch, which took roughly two hours, I just did a times four, so that would take eight hours while I was sleeping. We did four epochs, or roughly 40 billion tokens of training, and I was trying to see how far we could get. This was the only change and I reran the script. When I read the log file at 40 billion tokens, this is what the curve looked like. To narrate this, number one, we are seeing this issue here with the periodicity through the different epochs, and something really weird with the fine web edu data set. That is to be determined. Otherwise, we are seeing that the HSWAG accuracy actually went up by a lot and we almost made it to the GPT 324m accuracy up here, but not quite. It’s too bad that I didn’t sleep slightly longer. I think if this was a five epoch run, we may have gotten there. One thing to point out is that if you're doing multi-epoch runs, we're not actually being very careful in our data loader. This data loader goes through the data in exactly the same format and in exactly the same order. This is kind of suboptimal, and you would want to look into extensions where you actually permute the data randomly, you permute the documents around in every single shard on every single new epoch, and possibly even permute the shards. That would go a long way into decreasing the periodicity, and it's also better for the optimization, so that you're not seeing things in the identical format. You're introducing some randomness in how the documents follow each other. You have to remember that in every single row these documents follow each other and then there's the end of text token and then the next document. The documents are currently glued together in the exact same identical manner, but we actually want to break up the documents and shuffle them around, because the order of the documents shouldn't matter and they shouldn't— basically, we want to break up that dependence because it's a kind of a spurious correlation. So our data loader is not currently doing that, and that's one improvement you could think of making. The other thing to point out is we're almost matching GPT3 accuracy with only 40 billion tokens. GPT3 trained on 300 billion tokens, so again we're seeing about a 10x improvement here with respect to learning efficiency. I don’t actually know exactly what to attribute this to, other than some of the things that I already mentioned previously for the previous run. The other thing I wanted to briefly mention is the max LR here. I saw some people already play with this a little bit in a previous related repository. It turns out that you can actually almost like 3x it. It's possible that the maximum learning rate can be a lot higher, and for some reason, the GPT3 hyper-parameters that we are inheriting are actually extremely conservative. You can actually get away with a higher learning rate, and it would train faster. A lot of these hyper-parameters are quite tunable. Feel free to play with them, and they're probably not set precisely correctly. It’s possible that you can get away with doing this. If you wanted to exactly be faithful to GPT3, you would also want to make the following difference. You'd want to come here, and the sequence length of GPT3 is 2048 instead of 1024. You would come here and change this to 2048 for T. Then, if you want the exact same number of tokens, half a million per iteration or per step, you want to then decrease this to 32, so they still multiply to half a million. That would give your model a sequence length equal to that of GPT3. In that case, the models would be roughly identical as far as I am aware, because again GPT2 and GPT3 are very similar models. We can also look at some of the samples here from the model that was trained overnight. This is the optimization, and you see that here we stepped all the way to 76,290, and these are the HSWAG scores we achieved was 33.24. These are some of the samples from the model. You can see that if you read through this, and pause the video briefly, you can see that they are a lot more coherent. They're actually addressing the fact that it's a language model almost. For example, “Hello, I'm a language model and I try to be as accurate as possible. I'm a language model not a programming language, I know how to communicate, I use Python.” I don't know if you pause this and look at it, and then compare it to the model that was only trained for 10 billion, you will see that these are a lot more coherent, and you can play with this yourself. One more thing I added to the code by the way, is this chunk of code here. Basically, right after we evaluate the validation loss, if we are the master process, in addition to logging the validation loss every 5,000 steps, we're also going to save the checkpoint, which is really just the state dictionary of the model. Checkpointing is nice just because you can save the model and later you can use it in some way. If you wanted to resume the optimization, then in addition to saving the model, we have to also save the optimizer state dict. Remember that the optimizer has a few additional buffers because of Adam, so it's got the m and v and you need to also resume the optimizer properly. You have to be careful with your RNG seeds, random number generators, and so on. If you wanted to exactly be able to resume optimization, you have to think through the state of the training process, but if you just want to save the model, this is how you would do it. One nice reason why you might want to do this is because you may want to evaluate the model a lot more carefully. Here, we are only kind of like winging the HSWAG eval. You may want to use something nicer, like for example, the LUTHER evaluation harness. This is a way to also evaluate language models. It's possible that you may want to use different infrastructure to more thoroughly evaluate the models on different evaluations and compare it to the original GPT2 model on many other tasks, like for example, that involve math, code, or different languages. This is a nice functionality to have as well. The other thing I wanted to mention is that everything we've built here, this is only the pre-training step. The GPT here, it dreams documents, it just predicts the next token. You can't talk to it like you can talk to chat GPT. If you wanted to talk to the model, we have to fine-tune it into the chat format. It's not actually that complicated. If you're looking at supervised fine-tuning, or SFT, really what that means is we're just swapping out a data set into a data set that is a lot more conversational, and there's a user assistant, user assistant kind of structure. We just fine-tune on it, and then we basically fill in the user tokens, and we sample the assistant tokens. It's not a lot more deeper than that, but basically we swap out the data set and continue training. For now, we're going to stop at pre-training. One more thing that I wanted to briefly show you is that, of course, what we've built up today was building towards nanoGPT, which is this repository from earlier. There's actually another nanoGPT implementation, and it's hiding in a more recent project that I've been working on called llm.c. llm.c is a pure Cuda implementation of GPT2 or GPT3 training, and it just directly uses Cuda and is written as Cuda. The nanoGPT here acts as reference code in pytorch to the C implementation. We're trying to exactly match up the two, but we're hoping that the C Cuda is faster. Currently, that seems to be the case because it is a direct optimized implementation. train_gpt2.py in llm.c is basically the nanoGPT, and when you scroll through this file, you'll find a lot of things that very much look like things that we've built up in this lecture. When you look at train_gpt2_cuda, this is the C Cuda implementation. There’s a lot of MPI, NCCL, GPU Cuda cc++, and you have to be familiar with that. When this is built up, we can actually run the two side by side, and they're going to produce the exact same results, but llm.c actually runs faster. Let’s see that. On the left, I have pytorch, a nanoGPT looking thing. On the right, I have the llm.c call. Here, I'm going to launch the two. Both of these are going to be running on a single GPU. I'm putting the llm.c on GPU 1, and this one will grab GPU 0 by default. You can see here that llm.c compiled and then allocated space, and it's stepping. Meanwhile, pytorch is still compiling because torch compile is a bit slower here than the llm.c nvcc Cuda compile. This program has already started running, and we're still waiting here for torch compile. Of course, this is a very specific implementation to GPT2 and 3. Pytorch is a very general neural network framework, so they're not exactly comparable. If you're only interested in training GPT2 and 3, llm.c is very fast. It takes less space, it's faster to start, and it's faster per step. PyTorch started stepping here, and as you can see, we're running at about 223,000 tokens per second here, and about 185,000 tokens per second here. Quite a bit slower, but I don't have full confidence that I exactly squeezed out all the juice from the pytorch implementation. The important thing here is, notice that if I align up the steps, you will see that the losses and norms that are printed between these two are identical. On the left, we have the PyTorch, and on the right, this C implementation, and they're the same, except this one runs faster. That's kind of what I wanted to show you, also briefly, llm.c, and this is a parallel implementation, and it's also something that you may want to play with or look at. It's kind of interesting. At this point, I should probably start wrapping up the video because I think it's getting way longer than I anticipated. We did cover a lot of ground and we built everything from scratch. As a brief summary, we were looking at the GPT2 and GPT3 papers. We were looking at how you set up these training runs and all the considerations involved. We wrote everything from scratch, and then we saw that over the duration of either a two-hour training run, or an overnight run, we can actually match the 124 million parameter checkpoints of GPT2 and GPT3 to a very large extent. In principle, the code that we wrote would be able to train even bigger models if you have the patience or the computing resources. You could potentially think about training some of the bigger checkpoints as well. There are a few remaining issues to address: What's happening with the loss here, which I suspect has to do with the fine web edu data sampling? Why can't we turn on torch compile? It currently breaks generation and HSWAG. What’s up with that? In the data loader, we should probably be permuting our data when we reach boundaries. There are a few more issues like that, and I expect to be documenting some of those over time in the build n GPT repository here, which I'm going to be releasing with this video. If you have any questions or would like to talk about anything that we covered, please go to the discussions tab, so we can talk here, or please go to issues or pull requests, depending on what you'd like to contribute. Also, have a look at the Zero to Hero Discord, and I'm going to be hanging out here on N GPT. Otherwise, for now, I'm pretty happy about where we got, and I hope you enjoyed the video and I will see you later.

