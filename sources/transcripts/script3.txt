Okay, let's begin. In this lecture, we are continuing our implementation of "make more". In the last lecture, we implemented a multi-layer perceptron (MLP) for character-level language modeling, following the approach described in Benj's 2003 paper. We took in a few past characters and used an MLP to predict the next character in a sequence. Now, we want to move on to more complex and larger neural networks, like recurrent neural networks and their variations such as GRUs and LSTMs. However, before we do that, we need to stick with MLPs for a bit longer. I want us to have a good intuitive understanding of the activations in the neural net during training, especially the gradients that flow backward and how they behave. This is crucial for understanding the history of the development of these architectures. Recurrent neural networks, while very expressive and capable of implementing any algorithm, are not easily optimized with the first-order gradient-based techniques we commonly use. The key to understanding why they are not easily optimizable lies in understanding the activations and gradients and their behavior during training. Many variants of recurrent neural networks have attempted to improve this situation. That is the path we will take.

The starting code for this lecture is largely the same as before, but I have cleaned it up a bit. We are importing all the necessary PyTorch, math, and matplotlib utilities. We are reading in the words as before; these are eight example words from a total of 32,000. Here is a vocabulary of all lowercase letters and the special dot token. We are reading the dataset, processing it, and creating three splits: train, development, and test. In this MLP, which is identical to the previous one, I have removed the magic numbers. Instead, we have the dimensionality of the character embedding space and the number of hidden units in the hidden layer pulled out as variables. This avoids having to change these magic numbers all the time. We have the same neural net with 11,000 parameters, optimized over 200,000 steps with a batch size of 32. I have refactored the code a bit here, creating some extra variables and comments, but there are no functional changes. When we optimize, we see that our loss looks something like this. The train and validation loss are around 2.16. I've also refactored the code for evaluating arbitrary splits. You can pass in a string of which split you want to evaluate, and it will retrieve the correct split. Then, it performs the forward pass of the network, evaluates the loss, and prints it. This makes it nicer to work with.

You will notice that I am using a `torch.no_grad()` decorator. This decorator tells PyTorch that the function does not require any gradients. Therefore, it will not perform the bookkeeping needed to track gradients in anticipation of a backward pass. It's as if all tensors created here have `requires_grad` set to `False`, making everything more efficient. You can also use a context manager with `torch.no_grad()`. We also have the sampling function, which performs a forward pass, gets the distribution, adjusts the context window, and repeats until it gets the special end token. We are starting to get much nicer-looking words from the model, though they are still not fully name-like. This is much better than what we had with the bi-gram model. This is our starting point.

The first thing I would like to examine is the initialization. I can tell that our network is improperly configured at initialization. There are multiple things wrong, but let’s start with the first one. Look here: at the zeroth iteration, the very first iteration, we record a loss of 27, which rapidly decreases to roughly one or two. I can tell that the initialization is messed up because this value is too high. When training neural nets, you usually have a rough idea of what loss to expect at initialization. This depends on the loss function and the problem setup. In this case, I do not expect 27; I expect a much lower number. We can calculate it together. At initialization, there are 27 characters that could come next for any one training example. We have no reason to believe that any characters are much more likely than others. Therefore, we expect that the probability distribution that initially comes out is a uniform distribution, assigning equal probability to all 27 characters. The probability for any character should be roughly 1/27. The loss is the negative log probability. If we wrap this in a tensor, take the log of it, and then negate it, the expected loss would be 3.29, much lower than 27. At initialization, the neural net is creating probability distributions that are all messed up. Some characters are very confidently predicted while others are not. The network is confidently wrong, leading to a very high loss.

Here’s a smaller, four-dimensional example of the issue. Let's say we have four characters. The logits coming out of the neural net are very close to zero. When we take the softmax of all zeros, we get probabilities that are a diffused distribution. It sums to one and is exactly uniform. In this case, if the label is two, it doesn’t matter if the label is two, three, one, or zero. Because it’s a uniform distribution, we record the same loss, 1.38. This is the loss we would expect for a four-dimensional example. As we start to manipulate these logits, we will change the loss. If we were to get lucky and the correct logit was a very high number, like five, we would record a low loss because, by chance, we're assigning the correct probability to the correct label. More likely, some other dimension will have a high logit. We will then start to record a much higher loss. The logits could take on extreme values, and we can record very high loss. For example, if the logits are randomly drawn from a normal distribution, the loss that comes out is okay because the logits are near zero. But if these logits are multiplied by ten, the more extreme values make it very unlikely that we guess the correct bucket. We become confidently wrong and record high loss. If logits are even more extreme, we might get insane losses, even infinity, at initialization. This is not good, and we want the logits to be roughly zero when the network is initialized. The logits do not have to be exactly zero; they just need to be equal. For example, if all logits are one, because of the normalization inside the softmax, this will come out okay. But for symmetry, we don’t want any arbitrary positive or negative number. We want them to be all zeros and record the expected loss at initialization.

Let’s concretely see where things go wrong in our example. Let me reinitialize the neural net, and let me break after the very first iteration to see only the initial loss, which is 27. That’s way too high. The logits here, if we print the first row, take on quite extreme values. That is what creates the false confidence in incorrect answers and makes the loss very high. These logits should be much closer to zero. Let’s think about how to make the logits coming out of this neural net closer to zero. The logits are calculated as hidden states multiplied by W2 plus B2. First, we are initializing B2 as random values. Since we want roughly zero, we don't want to add a bias of random numbers. I will add `* 0` here to make B2 zero at initialization. Second, the logits are H * W2. If we want logits to be very small, we should make W2 smaller. If we scale down all the elements of W2 by 0.1, and do the very first iteration again, the loss gets much closer to what we expect. The goal is about 3.29. This is now 4.2. We can make it even smaller: 3.32. We are getting closer. You might wonder, can we set it to zero? Of course, we get exactly what we are looking for at initialization. But I am nervous, and I’ll show you why you don't want to set the weights of a neural net to exactly zero. You want them to be small numbers instead. For this specific case, it might be fine, but I’ll show you where things go wrong quickly if you do that. Let’s use 0.01. The loss is close enough, with a bit of entropy. It's not exactly zero, and this is used for symmetry breaking. The logits are now much closer to zero. If we remove the break statement and run the optimization with this new initialization, we see that we started well and then came down a bit. The plot of the loss does not have this hockey stick appearance. What was happening in the hockey stick was that the first few iterations of training were just squashing down the logits. We removed this easy part of the loss function where the weights were just being shrunk down. Therefore, we don't see these easy gains at the start. We are getting some of the harder gains from training the actual neural net. There's no hockey stick appearance, and good things are happening. First, the loss at initialization is what we expect. Second, the loss doesn't look like a hockey stick. This is true for any neural net that you might train, and it is something to watch out for. Third, the loss is improved. I believe the previous losses were 2.2 and 2.16, so we got slightly improved results. The reason is that we are spending more cycles, more time, optimizing the neural net instead of squashing down the weights. This is something to be aware of.

Let’s look at the second problem. Let me reinitialize our neural net and reintroduce the break statement so that we have a reasonable initial loss. Everything looks good on the level of the loss, but there is still a deeper problem looking inside the neural net and its initialization. The logits are now okay. The problem is with the values of H, the activations of the hidden states. If we visualize this tensor H, the problem is that many of the elements are one or negative one. Recall that the tanh function squashes numbers into a range between negative one and one. It does so smoothly. Let's look at the histogram of H to get a better idea of the distribution of the values inside this tensor. We see that H has 32 examples and 200 activations in each example. We can view it as `-1` to stretch it out into a large vector. Then, we can call `tolist()` to convert it into a Python list of floats. We can pass this into `plt.hist()` for a histogram, with 50 bins and a semicolon to suppress the output. The histogram shows that most values are close to negative one and one. This means that the tanh is very active. Let's look at why that is. We can look at the pre-activations that feed into tanh. The pre-activations have a broad distribution, with numbers between -15 and 15. Everything is being squashed by the tanh function, resulting in the range of -1 and 1. Lots of the numbers here take on very extreme values.

If you are new to neural networks, you might not see this as an issue. However, if you are well-versed in the dark arts of backpropagation, having an intuitive sense of how gradients flow through a neural net, you are sweating while looking at the distribution of tanh(h) activations. Keep in mind that during backpropagation, we are doing backward passes, starting at the loss, and flowing through the network backwards. In particular, we're going to backpropagate through this `torch.tanh()`. This layer is made up of 200 neurons for each example, and it implements an element-wise tanh. Let's look at what happens in the tanh backward pass. We can go back to our micrograd code from the first lecture and see how we implemented tanh. The input was X, and we calculate T, which is the tanh of X. T is between -1 and 1, and it is the output of tanh. In the backward pass, how do we backpropagate through tanh? We take the output gradient and multiply it by the local gradient, which takes the form of `1 - t^2`. If the outputs of your tanh are very close to -1 or 1, then plugging in one here will result in a zero, multiplying the output gradient. This kills the gradient, effectively stopping backpropagation through the tanh unit. Similarly, when T is -1, this again becomes zero. The output gradient stops. Intuitively, this makes sense because it is a tanh neuron. If its output is very close to one, we are in the tail of the tanh. Changing the input will not impact the output of the tanh because it's in a flat region. There’s no impact on the loss. The weights and biases, along with the tanh neuron, do not impact the loss because the output of the tanh unit is in a flat region. The gradient is basically zero. When T equals zero, we get one times the output gradient. When tanh takes on a value of zero, the output gradient passes through. If T is equal to zero, then the tanh unit is inactive and the gradient just passes through. The more you are in the flat tails, the more the gradient is squashed. The gradient flowing through tanh can only ever decrease, and the amount it decreases is proportional to the square. The concern here is that if all outputs H are in the flat regions of -1 and 1, then the gradients flowing through the network will be destroyed at this layer.

There is a redeeming quality here. We can get a sense of the problem. I have some code here to show you this. We want to look at H and take the absolute value. We want to see how often it is in a flat region, greater than 0.99. We get a Boolean tensor. A white pixel is true, and a black pixel is false. We see that a lot of it is white. This tells us that all the tanh neurons were very active. They are in a flat tail. In all these cases, the backward gradient is destroyed. We would be in trouble if for any of these 200 neurons, the entire column was white. We would have a dead neuron. This is a tanh neuron where the initialization of weights and biases is such that no single example activates this tanh in the active part of the tanh. If all examples land in the tail, this neuron will never learn. It is a dead neuron. We are looking for columns of white. I don’t see a single neuron that is all white. For every tanh neuron, we have some examples that activate them in the active part of the tanh. Some gradients will flow through, this neuron will learn, and it will change. Sometimes you get cases with dead neurons. The way this manifests is for a tanh neuron. No matter what inputs you plug in, the tanh neuron will always fire completely at one or completely at negative one. It will not learn because all the gradients will be zeroed out. This is true not just for tanh but also for many other nonlinearities used in neural networks. Sigmoid will have the exact same issue because it is a squashing neuron. The same will apply to sigmoid. The same will also apply to ReLU. ReLU has a flat region below zero. If you have a ReLU neuron, and the pre-activation is positive, it will pass through. If the pre-activation is negative, it will shut it off. Because the region is flat, backpropagation will set the gradient exactly to zero, instead of to a very small number depending on how positive or negative T is. You can get a dead ReLU neuron. A dead ReLU neuron is when a neuron with a ReLU nonlinearity never activates. It is always in the flat region. The weights and bias will never learn. They will never get a gradient because the neuron never activated. This can happen at initialization because weights and biases make some neurons dead by chance. It can also happen during optimization if you have a high learning rate. Sometimes neurons get too much gradient and get knocked off the data manifold. Then, no example ever activates the neuron. The neuron is dead. If your learning rate is very high, for example, and you have a neural net with ReLU neurons, you might get some loss. Then you can go through the training set and find neurons that never activate. Those are dead neurons. They will never turn on. During training, ReLU neurons are changing and moving. However, because of a high gradient somewhere, they get knocked off, and from then on, they are dead. Other nonlinearities, like Leaky ReLU, will not suffer from this issue as much because they don't have flat tails. You will almost always get gradients. ELU is also frequently used. It might suffer from this issue because it has flat parts. This is something to be aware of and be concerned about. In this case, we have too many activations that take on extreme values. Because there is no column of white, I think we will be okay. The network optimizes and gives us a pretty decent loss, but it is not optimal. We don’t want this, especially during initialization. This pre-activation that's flowing into the tanh is too extreme. It is too large, creating a distribution that is too saturated on both sides of the tanh. This means that there is less training for the neurons because they update less frequently.

How do we fix this? The pre-activation, which comes from the concatenation of the characters, are uniform Gaussians. This is multiplied by W1 plus B1. The pre-activation is too far from zero, and that’s causing the issue. We want the pre-activation to be closer to zero, similar to what we had with logits. It's okay to set the biases to a small number. We can multiply by 0.01 to get a little bit of entropy. I sometimes like to do that so there is some variation in the original initialization of these tanh neurons. I find that in practice, it can help optimization. We can also squash the weights. Let's multiply everything by 0.1. Let's rerun the first batch and look at the histogram. Now, we have a much better histogram. The pre-activations are between -1.5 and 1.5. We expect much less white. There is no white. There are no neurons saturated above 99 in either direction. This is a pretty decent place to be. Maybe we can go up a little bit, to 0.2. Something like this is a nice distribution. This is what our initialization should be. Let me erase these and run the full optimization starting with this initialization without the break. We see that the optimization finished. This is the result that we get. The previous losses were a validation loss of 2.17, which became 2.13, which became 2.10. The reason this is happening is that our initialization is better. We're spending more time doing productive training instead of non-productive training where the gradients are set to zero and we have to learn very simple things like the overconfidence of the softmax at the beginning. We are spending cycles squashing down the weight matrix. This is illustrating initialization and its impact on performance. By being aware of the internals of these neural nets, we can understand activations and gradients. We're working with a very small network. This is just a one-layer MLP. The optimization problem is quite easy and forgiving. Even though our initialization was terrible, the network still learned eventually. It just got a worse result. This is not the case in general. When working with much deeper networks that have, say, 50 layers, things can get more complicated. These problems stack up. You can get into a place where the network is not training at all if your initialization is bad enough. The deeper your network is, the more complex it is, and the less forgiving it is to these errors. This is something to definitely be aware of, scrutinize, plot, and be careful with.

Okay, this worked for us, but we have magic numbers like 0.2. Where do I get this? How am I supposed to set these with a large neural net with lots of layers? Obviously, no one does this by hand. There are principled ways of setting these scales that I would like to introduce now. Here is some code that I have prepared to motivate the discussion. We have random input X drawn from a Gaussian. There are 1,000 examples that are 10-dimensional. There's a weight layer, also initialized using a Gaussian, just like we did before. These neurons in the hidden layer look at 10 inputs. There are 200 neurons in this hidden layer. We have the multiplication X * W to get the pre-activations of these neurons. We are looking at what happens. Suppose these are uniform Gaussians and these weights are uniform Gaussians. If I do X * W, and we forget about the bias and the nonlinearity for now, what is the mean and the standard deviation of these Gaussians? The input is a normal Gaussian with mean zero and standard deviation one. The standard deviation is a measure of the spread. Once we multiply here and look at the histogram of Y, the mean stays at zero because this is a symmetric operation. The standard deviation has expanded to three. The input standard deviation was one, but now it has grown to three. This Gaussian is expanding. We don't want that. We want most of the neural net to have relatively similar activations, a unit Gaussian roughly throughout the neural net. How do we scale the W’s to preserve this distribution, to remain a Gaussian? Intuitively, if we multiply the elements of W by a larger number, like five, the Gaussian grows in standard deviation. Now we're at 15. The numbers in the output Y take on more extreme values. If we scale down by 0.2, conversely, the Gaussian is getting smaller and smaller. The standard deviation is 0.6. What do we multiply by here to exactly preserve the standard deviation to be one? The correct answer, when you work out the variance of this multiplication, is that you are supposed to divide by the square root of the fan-in. The fan-in is the number of input elements, 10 in this case. We are supposed to divide by the square root of 10. This is one way to do the square root, raising it to a power of 0.5. When you divide by the square root of 10, we see that the output Gaussian has a standard deviation of exactly one. Unsurprisingly, a number of papers have looked into how to initialize neural networks to obtain this unit variance.

Initializing neural networks, particularly deep multilayer perceptrons with nonlinearities, requires careful attention to activation behavior. Activations should remain within reasonable values, avoiding expansion to infinity or shrinking to zero. A key question is how to initialize weights to achieve this. Kingma and Ba's paper, "Delving Deep into Rectifiers," studied this problem in detail, focusing on convolutional neural networks and ReLU nonlinearities. While their analysis centered on ReLU instead of tanh, the principles are similar. The ReLU nonlinearity acts as a squashing function, clamping negative numbers to zero while passing positive numbers through. This effectively discards half of the distribution, necessitating compensation with a gain.  They found that weights should be initialized with a zero-mean Gaussian distribution, where the standard deviation is the square root of 2 over the fan-in. The fan-in is the number of inputs to a neuron. This factor of two arises because ReLU discards half the distribution. The paper also investigates backpropagation, ensuring gradients are well-behaved. While the forward pass is initialized, the backward pass is also initialized up to a constant factor relating to the size of hidden neurons in early and late layers. Empirically, this factor doesn't significantly affect performance. This initialization method is implemented in PyTorch's `torch.nn.init` module using `kaiming_normal`. This is a common approach for initializing neural networks, accepting keyword arguments. The 'mode' argument specifies whether to normalize activations or gradients, which is typically set to the default 'fan-in'. A second argument specifies the nonlinearity, which dictates the gain. For linear activations, the gain is one, resulting in a formula where standard deviation is the square root of one over fan-in. For ReLU, the gain is the square root of two. For tanh, which is a contractive function, a gain of 5 over 3 is used. Tanh squashes the output distribution, requiring boosted weights to renormalize everything to a standard unit deviation. This ensures that distributions remain within an appropriate range. Seven years ago, precise initialization of activations and gradients was crucial for training deep networks. However, modern innovations have made this less critical. These include residual connections, normalization layers such as batch, layer, and group normalization, and more sophisticated optimizers like Adam. While precise calibration is less necessary now, initializing by normalizing weights by the square root of the fan-in, similar to what was originally done, remains a good practice. For a more precise implementation using `torch.nn.init.kaiming_normal`, the standard deviation is calculated as the gain over the square root of the fan-in. In the case of a tanh nonlinearity, the gain would be 5 over 3. The standard deviation is calculated as the gain divided by the square root of the fan-in. The weights are then multiplied by this standard deviation to achieve proper initialization. After training, the validation loss is roughly the same as with a less-principled method, but this method is more scalable and provides a useful guideline.

Batch normalization, introduced in 2015, has greatly improved the reliability of training deep neural networks. Batch normalization standardizes hidden states to approximately follow a Gaussian distribution with zero mean and unit standard deviation. The idea is to normalize hidden states, making them unit Gaussian, because these values tend to be too small, rendering the tanh function ineffective, or too large, saturating the tanh function. The hidden state's mean is calculated across the zero dimension, preserving the dimension using `keepdim=True`. The standard deviation is calculated similarly, resulting in dimensions of 1x200 for 200 neurons. The hidden states are then normalized by subtracting the mean and dividing by the standard deviation, thereby making the values unit Gaussian. However, forcing the hidden states to always be unit Gaussian is undesirable, as the network may need a different distribution. The paper thus introduces a scale and shift component. Normalized inputs are scaled by a gain and offset by a bias. A batch normalization gain is initialized to ones, while a batch normalization bias is initialized to zeros, both with a dimension of 1xN_hidden. The normalized hidden states are multiplied by the batch normalization gain and then added to the bias. This ensures that, at initialization, each neuron's firing values will be unit Gaussian. These gain and bias values are trained via backpropagation to allow the network the flexibility to adjust the distribution of activations. The gain and bias parameters must be added to the model's trainable parameters. Batch normalization is also applied during testing time using the same normalization, scaling and shifting procedures.  When applying batch normalization, the validation loss can remain approximately the same, since the weights were already scaled to achieve a roughly Gaussian distribution before. However, batch normalization is essential for deeper networks, as controlling the scales of weights to get roughly Gaussian activations becomes more challenging. Batch normalization is often placed after linear or convolutional layers to control activation scales. This stabilizes training without requiring precise mathematical analysis of distributions. This stability comes at a cost, however. Batch normalization mathematically couples the examples within a batch because mean and standard deviation are calculated across all examples in a given batch. Thus, an activation for one example depends on the values in the other examples. This results in a jittering of the hidden states and logits, which is dependent on the composition of the batch. Although undesirable, this jitter acts as a regularizer, effectively padding out input examples, introducing entropy, and reducing overfitting. Batch normalization has remained in use due to its regularizing effects and its effectiveness at controlling activations, despite the coupling of examples within a batch.

Alternative techniques like layer, instance, and group normalization do not couple the examples in a batch, but batch normalization remains popular despite this drawback. When deploying a trained neural network with batch normalization, the problem arises on how to use it for single examples since the neural net expects a batch. The solution is to calibrate the batch normalization statistics by computing the mean and standard deviation across the training set once, after training, and using these fixed values during inference. The batch normalization mean and standard deviation are computed over the entire training set, and used during inference, giving an identical loss. An alternative approach, however, is to estimate the mean and standard deviation in a running manner during training. This avoids the need for a separate calibration step after training. A running mean of both the batch normalization mean and the standard deviation are updated during training. These running mean and standard deviation are not updated via backpropagation but by a simple smoothing update. The running mean is updated by a weighted average of the current running mean and the newly calculated mean. The running standard deviation is updated similarly. During training, the running mean and standard deviation values track the typical values observed during training. After training, these running values are used for inference. After the optimization process is complete, both values are closely comparable to those calculated through explicit calibration. The running mean and standard deviations are used instead of the batch mean and standard deviation during evaluation. Batch normalization is complete with two notes. First, the small epsilon term added to the denominator is used to prevent division by zero if variance becomes zero. Second, adding bias before batch normalization is redundant because the batch normalization layer will subtract the mean, effectively nullifying the bias. Therefore, when batch normalization is used the bias term of a preceding layer can be omitted.

We want to use bias, but here you don't want to add it because it's spurious. Instead, we have this batch normalization bias, and that batch normalization bias is now in charge of biasing this distribution instead of the B1 that we had originally. Basically, a batch normalization layer has its own bias, and there's no need to have a bias in the layer before it because that bias is going to be subtracted out anyway. That's another small detail to be careful with. Sometimes, it's not going to do anything catastrophic; this B1 will just be useless. It will never get any gradient, it will not learn, it will stay constant, and it's just wasteful, but it doesn't actually impact anything otherwise.

I rearranged the code a little bit with comments, and I just wanted to give a quick summary of the batch normalization layer. We are using batch normalization to control the statistics of activations in the neural net. It is common to sprinkle batch normalization layers across the neural net, and usually, we will place it after layers that have multiplications, like, for example, a linear layer or a convolutional layer, which we may cover in the future. Now, the batch normalization internally has parameters for the gain and the bias, and these are trained using backpropagation. It also has two buffers: the mean and the standard deviation. These are the running mean and the running mean of the standard deviation, and these are not trained using backpropagation. These are trained using this janky update of a kind of like a running mean update. So, these are sort of the parameters and the buffers of the batch normalization layer. Really what it's doing is it's calculating the mean and a standard deviation of the activations that are feeding into the batch normalization layer over that batch. Then, it's centering that batch to be unit Gaussian, and then it's offsetting and scaling it by the learned bias and gain. On top of that, it's keeping track of the mean and the standard deviation of the inputs, and it's maintaining this running mean and standard deviation. This will later be used at inference so that we don't have to re-estimate the mean and standard deviation all the time. In addition, that allows us to basically forward individual examples at test time. So, that's the batch normalization layer. It's a fairly complicated layer, but this is what it's doing internally.

Now, I wanted to show you a little bit of a real example, so you can search ResNet, which is a residual neural network. These are common types of neural networks used for image classification. Of course, we haven't covered ResNets in detail, so I'm not going to explain all the pieces of it. For now, just note that the image feeds into a ResNet on the top here, and there are many, many layers with repeating structure all the way to predictions of what's inside that image. This repeating structure is made up of these blocks, and these blocks are just sequentially stacked up in this deep neural network. Now, the code for this block that's used and repeated sequentially in series is called this bottleneck block. There's a lot here; this is all PyTorch. Of course, we haven't covered all of it, but I want to point out some small pieces of it. In the `__init__` is where we initialize the neural network, so this code of the block here is basically the kind of stuff we're doing here. We're initializing all the layers, and in the `forward`, we are specifying how the neural network acts once you actually have the input. So, this code here is along the lines of what we're doing here. Now, these blocks are replicated and stacked up serially, and that's what a residual network would be. So, notice what's happening here. `conv1`—these are convolution layers, and these convolution layers are basically the same thing as a linear layer, except convolutional layers don't apply. Convolutional layers are used for images, and so they have spatial structure, and basically, this linear multiplication and bias offset are done on patches instead of the full input. So, because these images have spatial structure, convolutions just basically do WX + B, but they do it on overlapping patches of the input. But otherwise, it's WX + B. Then, we have the `norm` layer, which by default here is initialized to be a batch normalization in 2D, so a two-dimensional batch normalization layer, and then we have a non-linearity, like ReLU. So, instead of using tanh, they use ReLU in this case. We are using tanh, but both are just non-linearities, and you can use them relatively interchangeably. For very deep networks, ReLU typically, empirically, works a bit better. So, see the motif that's being repeated here. We have convolution, batch normalization, ReLU, convolution, batch normalization, ReLU, etc. Then, here, this is a residual connection that we haven't covered yet, but basically that's the exact same pattern we have here. We have a weight layer, like a convolution or a linear layer, batch normalization, and then tanh, which is a non-linearity. But basically, a weight layer, a normalization layer, and a non-linearity, and that's the motif that you would be stacking up when you create these deep neural networks, exactly as it's done here.

One more thing I'd like you to notice is that here, when they are initializing the `conv` layers, like `conv1by1`, the depth for that is right here. It's initializing an `nn.Conv2d`, which is a convolution layer in PyTorch, and there are a bunch of keyword arguments here that I'm not going to explain yet. But you see how there's `bias=False`. The `bias=False` is exactly for the same reason as the bias is not used in our case. You see how I erased the use of bias? The use of bias is spurious because after this weight layer, there's a batch normalization, and the batch normalization subtracts that bias and then has its own bias. So, there's no need to introduce these spurious parameters. It wouldn't hurt performance; it's just useless. So, because they have this motif of C, Batch Norm, ReLU, they don't need a bias here because there's a bias inside here. By the way, this example here is very easy to find. Just search ResNet PyTorch, and it's this example here. So, this is kind of like the stock implementation of a residual neural network in PyTorch, and you can find that here. But of course, I haven't covered many of these parts yet.

I would also like to briefly descend into the definitions of these PyTorch layers and the parameters that they take. Now, instead of a convolutional layer, we're going to look at a linear layer because that's the one that we're using here. This is a linear layer, and I haven't covered convolutions yet, but as I mentioned, convolutions are basically linear layers except on patches. A linear layer performs a WX + B, except here, they're calling the W a transpose. So, to calculate WX + B, very much like we did here, to initialize this layer, you need to know the fan-in and the fan-out, and that's so that they can initialize this W. This is the fan-in and the fan-out, so they know how big the weight matrix should be. You also need to pass in whether or not you want a bias. If you set it to `false`, then no bias will be inside this layer, and you may want to do that, exactly like in our case, if your layer is followed by a normalization layer, such as batch normalization. This allows you to basically disable a bias.

Now, in terms of the initialization, if we swing down here, this is reporting the variables used inside this linear layer. Our linear layer here has two parameters, the weight and the bias. In the same way, they have a weight and a bias, and they're talking about how they initialize it by default. So, by default, PyTorch will initialize your weights by taking the fan-in and then doing one over fan-in square root, and then instead of a normal distribution, they are using a uniform distribution. So, it's very much the same thing, but they are using a one instead of five over three, so there's no gain being calculated here; the gain is just one. But otherwise, it's exactly one over the square root of fan-in, exactly as we have here. So, one over the square root of K is the scale of the weights, but when they are drawing the numbers, they're not using a Gaussian by default; they're using a uniform distribution by default. So, they draw uniformly from the negative square root of K to the square root of K, but it's the exact same thing and the same motivation with respect to what we've seen in this lecture. The reason they're doing this is that if you have a roughly Gaussian input, this will ensure that, out of this layer, you will have a roughly Gaussian output, and you basically achieve that by scaling the weights by one over the square root of fan-in. So, that's what this is doing.

The second thing is the batch normalization layer. So, let's look at what that looks like in PyTorch. Here we have a one-dimensional batch normalization layer, exactly as we are using here, and there are a number of keyword arguments going into it as well. So, we need to know the number of features. For us, that is 200, and that is needed so that we can initialize these parameters here: the gain, the bias, and the buffers for the running mean and standard deviation. Then, they need to know the value of epsilon here, and by default, this is 1e-5. You don't typically change this too much. Then, they need to know the momentum, and the momentum here, as they explain, is basically used for these running mean and running standard deviations. So, by default, the momentum here is 0.1. The momentum we are using here in this example is 0.001. Basically, you may want to change this sometimes. Roughly speaking, if you have a very large batch size, then typically what you'll see is that when you estimate the mean and the standard deviation for every single batch size, if it's large enough, you're going to get roughly the same result. Therefore, you can use a slightly higher momentum, like 0.1. But for a batch size as small as 32, the mean and standard deviation here might take on slightly different numbers because there are only 32 examples we are using to estimate the mean and standard deviation, so the value is changing around a lot. If your momentum is 0.1, that might not be good enough for this value to settle and converge to the actual mean and standard deviation over the entire training set. So, basically, if your batch size is very small, a momentum of 0.1 is potentially dangerous, and it might make it so that the running mean and standard deviation are thrashing too much during training and it's not actually converging properly.

`affine=True` determines whether this batch normalization layer has these learnable affine parameters: the gain and the bias. This is almost always kept to `true`. I'm not actually sure why you would want to change this to `false`. Then, `track_running_stats` is determining whether or not the batch normalization layer of PyTorch will be doing this. One reason you may want to skip the running stats is because you may want to, for example, estimate them at the end as a stage two. In that case, you don't want the batch normalization layer to be doing all this extra compute that you're not going to use. Finally, we need to know which device we're going to run this batch normalization on, a CPU or a GPU, and what the data type should be: half precision, single precision, double precision, and so on.

So, that's the batch normalization layer. Otherwise, the link to the paper is the same formula we've implemented, and everything is the same, exactly as we've done here. Okay, so that's everything that I wanted to cover for this lecture. Really what I wanted to talk about is the importance of understanding the activations and the gradients and their statistics in neural networks. This becomes increasingly important, especially as you make your neural networks bigger, larger, and deeper. We looked at the distributions basically at the output layer, and we saw that if you have two confident mispredictions because the activations are too messed up at the last layer, you can end up with these hockey stick losses. If you fix this, you get a better loss at the end of training because your training is not doing wasteful work. Then, we also saw that we need to control the activations. We don't want them to squash to zero or explode to infinity because you can run into a lot of trouble with all of these non-linearities and these neural nets. Basically, you want everything to be fairly homogeneous throughout the neural net. You want roughly Gaussian activations throughout the neural net. Then, we talked about: if we want roughly Gaussian activations, how do we scale these weight matrices and biases during initialization of the neural network so that we don't get, you know, so everything is as controlled as possible? This gave us a large boost in improvement.

Then, I talked about how that strategy is not actually possible for much, much deeper neural networks because when you have much deeper neural networks with lots of different types of layers, it becomes really, really hard to precisely set the weights and the biases in such a way that the activations are roughly uniform throughout the neural network. So then, I introduced the notion of a normalization layer. Now, there are many normalization layers that people use in practice: batch normalization, layer normalization, instance normalization, group normalization. We haven't covered most of them, but I've introduced the first one, and also the one that I believe came out first, and that's called batch normalization. We saw how batch normalization works. This is a layer that you can sprinkle throughout your deep neural net, and the basic idea is that if you want roughly Gaussian activations, then take your activations and take the mean and the standard deviation and center your data, and you can do that because the centering operation is differentiable. On top of that, we actually had to add a lot of bells and whistles, and that gave you a sense of the complexities of the batch normalization layer because now we're centering the data; that's great, but suddenly we need the gain and the bias, and now those are trainable. Then, because we are coupling all of the training examples, suddenly the question is how do you do the inference? To do the inference, we need to now estimate these mean and standard deviations over the entire training set and then use those at inference. But then, no one likes to do stage two, so instead, we fold everything into the batch normalization layer during training and try to estimate these in a running manner so that everything is a bit simpler, and that gives us the batch normalization layer. As I mentioned, no one likes this layer. It causes a huge amount of bugs. Intuitively, it's because it is coupling examples in the forward pass of a neural network, and I've shot myself in the foot with this layer over and over again in my life, and I don't want you to suffer the same. Basically, try to avoid it as much as possible. Some of the other alternatives to these layers are, for example, group normalization or layer normalization, and those have become more common in more recent deep learning. But we haven't covered those yet. But definitely batch normalization was very influential at the time when it came out in roughly 2015 because it was kind of the first time that you could train reliably much deeper neural networks, and fundamentally, the reason for that is because this layer was very effective at controlling the statistics of the activations in the neural network. So, that's the story so far, and that's all I wanted to cover. In the future lectures, hopefully, we can start going into recurrent neural networks, and recurring neural networks, as we'll see, are just very, very deep networks because you unroll the loop, and when you actually optimize these neural networks, that's where a lot of this analysis around the activation statistics and all these normalization layers will become very, very important for good performance. So, we'll see that next time. Bye.

Okay, so I lied. I would like us to do one more summary here as a bonus, and I think it's useful to have one more summary of everything I've presented in this lecture. But also, I would like us to start to Pytorch-ify our code a little bit so it looks much more like what you would encounter in PyTorch. You'll see that I will structure our code into these modules like a `Lin` module and a `BatchNorm` module, and I'm putting the code inside these modules so that we can construct neural networks very much like we would construct them in PyTorch. I will go through this in detail. So, we'll create our neural net, then we will do the optimization loop as we did before, and then one more thing that I want to do here is I want to look at the activation statistics, both in the forward pass and in the backward pass. Then here we have the evaluation and sampling just like before.

Let me rewind all the way up here and go a little bit slower. Here, I am creating a linear layer. You'll notice that `torch.nn` has lots of different types of layers, and one of those layers is the linear layer. `torch.nn.Linear` takes a number of input features, output features, whether or not we should have a bias, and then the device that we want to place this layer on and the data type. I will emit these two, but otherwise, we have the exact same thing. We have the fan-in, which is the number of inputs, fan-out, the number of outputs, and whether or not we want to use a bias. Internally, inside this layer, there's a weight and a bias if you'd like it. It is typical to initialize the weight using random numbers drawn from a Gaussian, and then here is the Kaiming initialization that we discussed already in this lecture, and that's a good default, and also the default that I believe PyTorch chooses. By default, the bias is usually initialized to zeros. Now, when you call this module, this will basically calculate W * X + B if you have a B, and then when you also call `parameters` on this module, it will return the tensors that are the parameters of this layer.

Next, we have the batch normalization layer. I've written that here, and this is very similar to PyTorch's `nn.BatchNorm1d` layer, as shown here. I'm kind of taking these three parameters here: the dimensionality, the epsilon that we will use in the division, and the momentum that we will use in keeping track of these running stats, the running mean, and the running variance. PyTorch actually takes quite a few more things, but I'm assuming some of their settings. So, for us, affine will be true, which means that we will be using a gamma and beta after the normalization. `track_running_stats` will be true, so we will be keeping track of the running mean and the running variance in the batch norm. Our device by default is the CPU, and the data type by default is float32. So, those are the defaults; otherwise, we are taking all the same parameters in this batch norm layer. First, I'm just saving them. Now, here's something new: there's a `dot_training`, which by default is true. PyTorch `nn.Module` also has this `training` attribute, and that's because many modules, and batch norm is included in that, have a different behavior whether you are training your network or whether you are running it in an evaluation mode and calculating your evaluation loss or using it for inference on some test examples. Batch norm is an example of this because when we are training, we are going to be using the mean and the variance estimated from the current batch, but during inference, we are using the running mean and running variance. Also, if we are training, we are updating the mean and variance, but if we are testing, then these are not being updated; they're kept fixed. So, this flag is necessary, and by default, it is true, just like in PyTorch. Now, the parameters of batch norm 1D are the gamma and the beta here, and then the running mean and running variance are called buffers in PyTorch nomenclature. These buffers are trained using an exponential moving average here explicitly, and they are not part of the backpropagation and stochastic gradient descent. So, they are not sort of like parameters of this layer, and that's why when we call parameters here, we only return gamma and beta. We do not return the mean and the variance. This is trained sort of like internally here every forward pass using the exponential moving average. So, that's the initialization.

Now, in a forward pass, if we are training, then we use the mean and the variance estimated by the batch. Let me pull up the paper here. We calculate the mean and the variance. Now, up above, I was estimating the standard deviation and keeping track of the standard deviation here in the running standard deviation instead of the running variance. But let's follow the paper exactly. Here, they calculate the variance, which is the standard deviation squared, and that's what gets tracked as a running variance instead of a running standard deviation, but those two would be very, very similar, I believe. If we are not training, then we use the running mean and variance. We normalize, and then here, I am calculating the output of this layer, and I'm also assigning it to an attribute called `out`. Now, `out` is something that I'm using in our modules here. This is not what you would find in PyTorch. We are slightly deviating from it. I'm creating a `dot_out` because I would like to very easily maintain all those variables so that we can create statistics of them and plot them. But PyTorch nn modules will not have a `dot_out` attribute. Finally, here we are updating the buffers using, again, as I mentioned, the exponential moving average given the provided momentum. Importantly, you'll notice that I'm using the `torch.no_grad` context manager, and I'm doing this because if we don't use this, then PyTorch will start building out an entire computational graph out of these tensors because it is expecting that we will eventually call `dot_backward`, but we are never going to be calling `dot_backward` on anything that includes the running mean and running variance. So, that's why we need to use this context manager so that we are not sort of maintaining them using all this additional memory. So, this will make it more efficient, and it's just telling PyTorch that there will be no backward. We just have a bunch of tensors, and we want to update them; that's it. Then, we return.

Okay, now scrolling down, we have the tanh layer. This is very, very similar to `torch.tanh`, and it doesn't do too much; it just calculates tanh as you might expect. So, that's `torch.tanh`, and there are no parameters in this layer, but because these are layers, it now becomes very easy to sort of like stack them up into basically just a list, and we can do all the initializations that we're used to. So, we have the initial sort of embedding matrix, we have our layers, and we can call them sequentially, and then again, with torch no grad. But there are some initializations here. So, we want to make the output softmax a bit less confident, like we saw, and in addition to that, because we are using a six-layer multilayer perceptron here, so you see how I'm stacking linear, tanh, linear, tanh, etc., I'm going to be using the gain here, and I'm going to play with this in a second, so you'll see how, when we change this, what happens to the statistics. Finally, the parameters are basically the embedding matrix and all the parameters in all the layers, and notice here, I'm using a double list comprehension, if you want to call it that. But for every layer in layers and for every parameter in each of those layers, we are just stacking up all those pieces, all those parameters. In total, we have 46,000 parameters, and I'm telling PyTorch that all of them require a gradient. Then here, we have everything here we are actually mostly used to. We are sampling a batch, we are doing a forward pass. The forward pass now is just the linear application of all the layers in order followed by the cross-entropy. Then, in the backward pass, you'll notice that for every single layer, I now iterate over all the outputs, and I'm telling PyTorch to retain the gradient of them. Then here, we are already used to all the gradient set to none, do the backward to fill in the gradients, do an update using stochastic gradient descent, and then track some statistics, and then I am going to break after a single iteration.

Now, here in this cell, in this diagram, I'm visualizing the histogram, the histograms of the forward pass activations, and I'm specifically doing it at the tanh layers. So, I'm iterating over all the layers except for the very last one, which is basically just the soft max layer. If it is a tanh layer, and I'm using a tanh layer just because they have a finite output of -1 to 1, and so it's very easy to visualize here, so you see -1 to 1, and it's a finite range and easy to work with. I take the `out` tensor from that layer into T, and then I'm calculating the mean, the standard deviation, and the percent saturation of T. The way I define the percent saturation is that `t.absolute value` is greater than .97. So, that means we are here at the tails of the tanh. Remember that when we are in the tails of the tanh, that will actually stop the gradients, so we don't want this to be too high. Now here, I'm calling `torch.histogram`, and then I am plotting this histogram. So, basically, what this is doing is that every different type of layer, and they have a different color, we are looking at how many values in these tensors take on any of the values below on this axis here. So, the first layer is fairly saturated here at 20%, so you can see that it's got tails here. But then everything sort of stabilizes, and if we had more layers here, it would actually just stabilize at around a standard deviation of about .65, and the saturation would be roughly 5%. The reason that this stabilizes and gives us a nice distribution here is because the gain is set to 5 over 3. Now, this gain, you see that by default, we initialize with 1 / the square root of fan-in, but then here during initialization, I come in and I iterate over all the layers, and if it's a linear layer, I boost that by the gain. We saw previously that if we just do not use a gain, then what happens if I redraw this, you will see that the standard deviation is shrinking, and the saturation is coming to zero, and basically, what's happening is the first layer is pretty decent, but then the further layers are just kind of like shrinking down to zero, and it's happening slowly, but it's shrinking to zero. The reason for that is when you just have a sandwich of linear layers alone, then initializing our weights in this manner, we saw previously, would have conserved the standard deviation of one. But because we have these interspersed tanh layers in there, these tanh layers are squashing functions, and so they take your distribution and they slightly squash it. Some gain is necessary to keep expanding it, to fight the squashing. It just turns out that 5 over 3 is a good value. If we have something too small, like one, we saw that things will come toward zero, but if it's something too high, let's do two, then here, we see that, well, let me do something a bit more extreme so it's a bit more visible. Let's try three. Okay, so we see here that the saturations are going to be way too large. Okay, so three would create way too saturated activations, so 5 over 3 is a good setting for a sandwich of linear layers with tanh activations, and it roughly stabilizes the standard deviation at a reasonable point.

Honestly, I have no idea where 5 over 3 came from in PyTorch. When we were looking at the Kaiming initialization, I see empirically that it stabilizes this sandwich of linear and tanh, and that the saturation is in a good range, but I don't actually know if this came out of some math formula. I tried searching briefly for where this comes from, but I wasn't able to find anything. But certainly, we see that empirically, these are very nice ranges. Our saturation is roughly 5%, which is a pretty good number, and this is a good setting of the gain in this context. Similarly, we can do the exact same thing with the gradients. So, here is a very same loop, if it's a tanh, but instead of taking a layer `dot_out`, I'm taking the `grad`, and then I'm also showing the mean and the standard deviation, and I'm plotting the histogram of these values, and so you'll see that the gradient distribution is fairly reasonable. In particular, what we're looking for is that all the different layers in this sandwich have roughly the same gradient. Things are not shrinking or exploding. So, we can, for example, come here, and we can take a look at what happens if this gain was way too small, so this was 0.5. Then you see, first of all, the activations are shrinking to zero, but also, the gradients are doing something weird. The gradients started out here, and then now they're like expanding out. Similarly, if we, for example, have a too high of a gain, like three, then we see that also the gradients have some asymmetry going on, where as you go into deeper and deeper layers, the activation distributions are changing, and so that's not what we want. In this case, we saw that without the use of batch norm, as we are going through right now, we had to very carefully set those gains to get nice activations in both the forward pass and the backward pass. Now, before we move on to batch normalization, I would also like to take a look at what happens when we have no tanh units here, so erasing all the tanh non-linearities, but keeping the gain at 5 over 3. We now have just a giant linear sandwich. So, let's see what happens to the activations. As we saw before, the correct gain here is one. That is the standard deviation preserving gain. So, 1.667 is too high, and so what's going to happen now is the following: I have to change this to be linear, because there's no more tanh.

Here's the cleaned-up transcript:

We're looking at activations, which start blue and become very diffuse by layer four. The gradient statistics are purple on the top layer and diminish deeper in the layers. There's an asymmetry in the neural network. If you have very deep neural networks, like 50 layers, this is not ideal. Before batch normalization, this was incredibly tricky to set. If the gain is too large, diffusion happens, and if it's too little, shrinking occurs. The correct gain is exactly one, as we do at initialization. Then, the statistics for the forward and backward passes are well-behaved. Before normalization layers, advanced optimizers like Adam, and residual connections, training neural networks was a balancing act. You had to precisely orchestrate everything, considering activations, gradients, and their statistics. It was difficult to train very deep networks. This is fundamentally why you had to be very careful with your initialization. You might ask why we need these tanh layers at all. If you only have linear layers, you get nice activations, but it collapses to a single linear layer. The output as a function of the input would be a linear function, no matter how many linear layers you stack. However, although the forward pass collapses to a linear layer, the optimization is not identical. You end up with interesting dynamics in the backward pass due to chain rule calculations. Optimizing a single linear layer and a sandwich of ten linear layers have different training dynamics. There are papers that analyze infinitely layered linear layers. The tanh non-linearities turn this linear sandwich into a neural network that can approximate any arbitrary function. The code is reset to use the linear tanh sandwich with a gain of 5/3. We can run a single step of optimization and look at the activation statistics. I've added a plot to look at parameters, their values, and gradients. I am iterating over all the parameters, focusing on the two-dimensional weights of the linear layers, skipping biases, gammas, and betas. The weights have different shapes. There's the embedding layer, the first linear layer, and the last linear layer. We can view the mean, standard deviation, and histogram of these parameters. Something is not ideal, even though the gradients look okay. The gradient-to-data ratio is also shown. This gives a sense of the scale of the gradient compared to the scale of the actual values. The gradient values are much smaller than the data values in most weights. This is not true of the last layer. The last layer takes on much larger values. The standard deviations are roughly 1 and 3 throughout, except for the last layer, which has a standard deviation of roughly 10 to the negative 2. The gradients on the last layer are about 10 times greater than the other weights. This is problematic because in simple stochastic gradient descent, the last layer would be trained about 10 times faster than the other layers at initialization. This kind of fixes itself with more training. After 1,000 steps, the neurons start saturating a bit, but the forward and backward passes look good and equal, with no shrinking or exploding to infinity. The weights are stabilizing, though the last layer is still a little troubling. We must track the update-to-data ratio to understand parameter updates. We're introducing a new update-to-data ratio list. I am comparing the update, learning rate times gradient, to the data. I am iterating over all the parameters, taking the standard deviation of the update divided by the standard deviation of the data. This ratio indicates how big the updates are compared to the values in tensors. I'm taking the log base 10 of this value. Then, I add it to the list and keep track of it for all the parameters. We are plotting these update ratios over time. The update ratios take on certain values during initialization and stabilize during training. There's an approximate value of roughly 10 to the negative 3 as a guide. Updates to tensors are roughly 1000th of the magnitude of those tensors. If it was much larger, like negative 1 on log plot, the values would be changing quite a lot. The final layer is an outlier because it was artificially shrunk to keep the softmax less confident during initialization. We multiplied the weight by 0.1, making the values too low, resulting in a higher update ratio, which stabilizes with training. I like to look at the update ratios and make sure they are not much above 10 to the negative 3. If it's below negative 3 on the log plot, it indicates the parameters aren’t training fast enough. If the learning rate is too low, the updates will be very small. If the learning rate is one thousandth, the size of the update is about 10,000 times smaller than the values in the tensor. This shows that training is too slow. This is another way to set the learning rate. The learning rate is a little high. It's above the black line, so the rate is around -2.5. Everything is somewhat stabilizing and looks like a decent setting of learning rates. When things are miscalibrated, you'll see it very quickly. If we forget to apply the fan-in normalization, the weights in the linear layers will be sampled from a gaussian. The activation plot will show saturation. The gradients will be messed up. There will be an asymmetry. The update ratios will be messed up too. There will be discrepancies in how fast different layers are learning. Some will learn too fast. Miscalibrations will manifest in these ways. These plots can bring those to your attention so that you can address them. With the linear tanh sandwich, we can calibrate the gains and make the activations, gradients, and parameters look decent. It is like balancing a pencil, because the gain has to be very precisely calibrated. Let's introduce batch normalization layers to fix this. I'm going to place the batch norm 1D class inside the network. The typical placement is between the linear layer and the non-linearity, but placing it after the non-linearity works too. It is fine to place it at the end before the loss function. Because the last layer is batch-normalized, we'll change the gamma to make the softmax less confident. We can train, and the activations will look very good. This is because every tanh layer is preceded by a batch normalization. The standard deviation is roughly 65 2% and equal throughout. The gradients look good, and so do the weights. The updates are reasonable, with all the parameters training at about the same rate. We are going to be less brittle with respect to the gain. We can make the gain be two, but the activations will be unaffected due to explicit normalization. The gradients and weights will also look okay. The updates will change. The backward pass of the batch norm changes the scale of the updates on the parameters. So we don’t get a free pass with the weights, but everything is more robust. We may have to retune the learning rate if we change the scale of the activations going into the batch norms. If we use batch norms, we don't need to normalize by fan-in. I am taking out the fan-in. We’ll see that it’s relatively well-behaved. The statistics in the forward pass look good. The gradients look good. The weight updates look okay. We're significantly below negative 3 so we’d have to bump up the learning rate to get to about one-third. After increasing the learning rate everything looks good. We are more robust to the gain of the linear layers. We still have to worry about the update scales and ensure that the learning rate is properly calibrated. The activations of the forward and backward pass look well-behaved. The purpose was to introduce you to batch normalization, which stabilizes very deep neural networks, and how it works and would be used. We py-torified the code and wrapped it into modules that can be stacked up into neural networks. These layers exist in PyTorch. By prepending “nn.” everything should work, as the API developed is identical to the one in PyTorch. I introduced the diagnostic tools you would use to understand whether your neural network is in a good state. We looked at statistics, histograms, forward pass activations, backward pass gradients, and the weights being updated. We looked at means, standard deviations, and the ratio of gradients to data. People look at this over time. The update to data ratio should be roughly negative 3 on the log scale. If it's too high, the learning rate is too big, and if it's too low, it’s too small. We did not try to beat our previous performance. Our performance is bottlenecked by the context length. We need to look at more powerful architectures. I also didn't give a full explanation of the activations and gradients. There was no full detail on how to pick learning rate based on gains. The purpose was to introduce the diagnostic tools. There is still work on understanding the initialization, backward pass, and how all of that interacts. We are getting to The Cutting Edge of research, people are still trying to figure out the best initialization, the best update rule, and so on. We have tools to tell us if things are on the right track for now.

