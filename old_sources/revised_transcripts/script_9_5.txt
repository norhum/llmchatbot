We forward our function and calculate the exact same loss, then perform a backward pass. We do this four times. When we look at the gradient, you will notice that the gradients do not match. We did a single batch of four, and then we did four gradient accumulation steps of batch size one. The gradients are not the same because the mean squared error gets lost; specifically, the one quarter in this loss is lost. The loss objective for every loop is just a mean squared error. In this case, because there's only a single example, it is just this term. That was the loss in the zeroth iteration, and the same applies for the first, second, and third. When you do the loss backward, we are accumulating gradients. The accumulation in the gradient is equivalent to doing a sum in the loss. Our loss here is without the factor of one quarter outside of it. We are missing the normalizer, and therefore our gradients are off. To fix this, we can say `loss = loss / 4`. We are scaling our loss, introducing a one quarter in front of all of these places. The individual losses are now scaled by one quarter. When we backward, all of these accumulate with a sum, but there's a one quarter inside every one of these components. Now, our losses will be equivalent. When I run this, you see that the gradients are now identical.  The reason this is not correct is that the loss we're calculating in the model is using a reduction of the mean. The cross-entropy loss has a reduction by mean at all the BYT elements. If we are just doing this gradient accumulation, we are missing that. The way to fix this is to compensate for the number of gradient accumulation steps. We can divide this loss by the gradient accumulation steps. We are scaling down the loss so that when we do loss backward, which corresponds to a sum in the objective, we are summing up the already normalized loss. When we sum the losses divided by the gradient accumulation steps, we are recovering the additional normalizer. This is equivalent to the original optimization because the gradient will come out the same. To print things nicely, we need an accumulator over the loss. We cannot just print the loss because we would only print the final loss at the final micro-step.  We initialize `loss_accum` at zero, and accumulate the loss into it, detaching the tensor from the graph. We are making these leaf nodes when we add them so that we keep track of the values. We print this accumulator instead of the loss.  We also need to account for the gradient accumulation steps inside the tokens processed. The tokens processed per step is `B * T * gradient_accumulation_steps`.  Now the optimization looks reasonable. We calculated the gradient accumulation steps to be 32, and are getting about 3 seconds. If you want to verify that your optimization and the implementation is correct, you can adjust the batch size. Because we have the total batch size and the gradient accumulation steps, our setting of `B` is purely a performance optimization. If you have a big GPU, you can increase this to 32; if you have a small GPU, you can try 8 or 4. In any case, you should be getting the exact same optimization and answers up to a floating-point error, because the gradient accumulation can handle everything serially.

We will use eight GPUs to collaborate and optimize over tokens at the same time. They will communicate so they are all collaborating on the optimization. We will use distributed data parallel from PyTorch, not the legacy data parallel.  We will launch eight processes, and each process will be assigned to a GPU. For each process, the training loop and everything else will look pretty much the same. Each GPU is working on exactly what we have built so far. There will be eight of them, all processing slightly different parts of the data. Once they all calculate their gradients, we will average the gradients. That is how they will be collaborating on the computational workload. To use all eight of them, we will run our script with a special command called `torchrun`. When `torchrun` runs our Python script, it will run eight of them in parallel, creating environment variables that each process can use to look up which process it is. `Torchrun` will set `rank`, `local_rank`, and `world_size` environment variables. If DDP is running, we must make sure that CUDA is available. The important part is `world_size`, which will be eight. `Rank` is an integer, and each process will run the exact same code but will have a different DDP rank. GPU0 will have a DDP rank of zero, GPU1 will have a rank of one, and so on. They are all running the same script; only the DDP rank will be different. That is how we will coordinate that they do not run on the same data. `Local_rank` is only used in a multi-node setting. We only have a single node with eight GPUs, so `local_rank` is the rank of the GPU on a single node, from zero to seven. For us, we are mostly running on a single box. We care about `Rank` and `World_size`. According to the `local_rank`, we set the device to be `cuda:{local_rank}` to ensure that there are no collisions on which GPU is used by which process.  We have a boolean variable to indicate that DDP rank is equal to zero, which we call the master process. The master process does the printing, logging, checkpointing, etc. The other processes are mostly compute processes that assist. The master process zero will have some additional work to do. If we are not using DDP, none of these variables are set. We revert back to single GPU training. We have rank zero, `world_size` is one, and we are the master process. We autodetect the device. So far, we have initialized DDP. When we run with `torchrun`, there will be eight copies running in parallel, each with a different rank. We must make sure that everything happens correctly afterwards. Because there are eight processes running in parallel, the code should be read while imagining that eight Python interpreters are running. The only difference is that they have a different DDP rank. They all pick the exact same seed, and make all of the same calculations, unaware of the other copies. We have to adjust these calculations to take into account the `world_size` and `rank`. The micro-batches and sequence lengths are all per GPU. There will be `num_processes` of them running in parallel. The gradient accumulation steps are now `total_B_size / (B * T * DDP_world_size)`. We also want to make sure this fits nicely into the total batch size. Each process will do `B * T`, and there are this many of them. Each GPU calculates this. If we were to print here, every process would print. To avoid this, we guard the print statement with `if master_process`, so that we only print once. Before the data loader, let us print and exit. When we run with `torchrun`, and the number of processes is eight, `torchrun` will run eight of these. First, it gets busy, with warnings from distributed, which are related to setup. The first print comes from process 5, then process 5 exits, and DDP does not like this. In an actual application, we would call `destroy_process_group` to clean up DDP. Next, we need to make the data loader aware of this multi-process setting. We do not want all the processes loading the same data; we want each process to get its own chunk.  We pass the rank and size to the data loader. The current position will not be zero. We stride out all the processes. We take `B * T` and multiply it by the process rank. Process rank 0 will start at zero, process rank one starts at `B * T`, and process rank two starts at `2 * B * T`, etc. When we advance, we do not advance by `B * T`; we advance by `B * T * num_processes`. The total number of tokens consumed is `B * T * num_processes`. They go off to a different rank. The position has to advance by the entire chunk. If `B * T * num_processes + 1` exceeds the number of tokens, then we loop. This is the simplest change for a distributed data loader. If process rank is zero and num processes is one, the whole thing will be identical to what we had before, but now we can have multiple processes running.

Once all processes initialize the data loader, they come here, and they all create a GPT model. They create eight GPT models on eight processes. Because the seeds are fixed, they all create the same identical model. They all move it to the device of their rank, and they all compile the model. The models are identical, so there are eight identical compilations happening in parallel. Now, when we construct the model, we need to wrap the model into the distributed data parallel container. This is how we do it. `Device_ids` has to be passed in, and this is supposed to be the DDP local rank. This wraps the model. In a forward pass, it behaves identically. My understanding is that nothing should be changed in the forward pass. In the backward pass, once the backward passes over each independent GPU, each has the gradient for all the parameters. What DDP does is, once the backward pass is over, it will do an average of those gradients across all the ranks and deposit that average on every single rank. Every rank will end up with the average. That's the communication. It synchronizes and averages the gradients. DDP can dispatch communications for the gradient while the backward pass is still happening. There is overlap of the communication of the gradient, the synchronization, and the backward pass. The forward is unchanged, and the backward is mostly unchanged. We are tacking on the average. In the optimization loop, when you do `loss.backward()`, it will do the backward pass and synchronize the gradients.  Because of the gradient accumulation steps loop, we don't want to do the synchronization after every single loss backward. We only want to synchronize on the last step, when the micro step becomes `grat_accum_steps - 1`. To do that, we can use the `no_sync` context manager. However, instead of using a context manager, I directly toggle the `require_backward_grad_sync` variable. Before the `loss.backward()`, if we are using DDP, we only want to synchronize when it is the final iteration. In all other iterations, we want it to be false. I toggle the variable directly. `Require_backward_grad_sync` should only turn on when the micro step is the last step. This allows me to avoid the use of context managers and code duplication.  `Loss.backward()` will not synchronize most of the steps, only the last step. Once this is over, every rank will have the average of all gradients stored on all the ranks. We have averaged the gradients, but the `loss_accum` has not been impacted. This is outside of the DDP container, and is not being averaged. When we print `loss_accum`, we will only be printing the losses from the master process. We want to print the loss over all the processes and the average of that loss. We will average the `loss_accum` by using `dist.all_reduce`. This `loss_accum` tensor exists on all ranks. When we call the `all_reduce` of average, it creates the average of those numbers and deposits that average on all the ranks. All the ranks after this call will contain `loss_accum` averaged. When we print on the master process, `loss_accum` is identical in all the other ranks. We also need to be careful because we are processing more tokens, so we multiply by the DDP `world_size`. Finally, we want to destroy the process group so we are nice to NCCL. The model needs to be wrapped into DDP. The `configure_optimizers` function is now in the `raw_model` so that has to be used.

When we compare a single GPU run with 32 gradient accumulation steps to a multi-GPU run, the numbers will not exactly match up. In the data loader, we are iterating through batches in a slightly different way. If a page of data for all the GPUs exceeds the number of tokens, we loop. The single GPU and the 8 GPU process will end up resetting in a slightly different manner, making batches slightly different. To convince yourself that this is okay, make the total batch size much smaller. I used 32,768 as the total batch size.  Then, I made sure that the single GPU would do eight gradient accumulation steps, and then the multi-GPU.  This reduces the boundary effects of the data loader, and the numbers will match. We are now going really, really fast.  The optimization is mostly consistent with GPT-2 and three hyper parameters. We have outgrown our tiny Shakespeare file and want to upgrade it.

GPT-2 used the WebText dataset, which was never released. OpenWebText is an attempt at reproducing it. They scraped all outbound links from Reddit with at least three karma. This was 45 million links, and ended up being 40 GB of text. GPT-3 uses Common Crawl, which is a random subset of the internet and is extremely noisy.  People go to great lengths to filter Common Crawl. GPT-3 also uses WebText from before, books, and Wikipedia. This data set for GPT-3 was never released. Some of the data sets that are commonly used are the Red Pajama dataset, specifically the Slim Pajama subset, which is cleaned and deduplicated. This dataset contains Common Crawl, C4, which is also Common Crawl but processed differently. It also includes GitHub books archive, Wikipedia, and Stack Exchange. The FineWeb dataset is an attempt to collect high-quality Common Crawl data, filtering to 15 trillion tokens. Hugging Face released FineWeb-Edu, which is 1.3 trillion of educational and 5.4 trillion of high educational content. They filtered Common Crawl to high-quality educational subsets.  We will use this sample 10 billion tokens sub-sample of it because it is empirically close to GPT-2 performance. Our goal will be to download, process, and make sure that our data loader can work with it.
