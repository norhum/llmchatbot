We're going to iterate substantially on this simple neural network. We'll take multiple previous characters and feed them into increasingly complex neural nets. The output of the neural net will always be logits. These logits will go through a softmax, we'll calculate the loss function (negative log likelihood), and then do gradient-based optimization.  As we complexify the neural nets, up to transformers, this fundamental process won't change. Only the forward pass will change, where we take previous characters to calculate logits for the next character in the sequence. We’ll use the same optimization machinery. Extending the bigram approach to many input characters is not scalable because the tables would become too large. If you have the last ten characters as input, you can’t keep everything in a table. The neural network approach is more scalable and can be improved over time.

The `x_inc` vector is made up of one-hot vectors. When multiplied by the `w` matrix, it's like multiple neurons being forwarded in a fully connected manner, but actually, it's plucking out a row of `w`. If the one-hot vector has a '1' at the fifth dimension, multiplying by `w` gives the fifth row of `w`. This was also what happened in the bigram model. The first character indexed into a row of an array, giving us the probability distribution for the next character. Here, we encode the index as one-hot and multiply it by `w`, making logits the corresponding row of `w`. This is then exponentiated to create counts, which are normalized to get probabilities. `w` here is the same as the previous array, but it's log counts, not the raw counts. It is more accurate to say that `w` exponentiated `w.x` is the previous array, but that was filled by counting bigrams, while here, we initialize `w` randomly and let the loss guide it. We arrive at the same array at the end of optimization and obtain the same loss function. 

Remember the smoothing where we added fake counts to make the probability distributions uniform? Increasing the count makes the probability distribution more uniform. The gradient-based framework has an equivalent to this. If all entries of `w` are zero, the logits become zero, and after exponentiation, probabilities are uniform. Trying to make `w` near zero is equivalent to label smoothing. The more you incentivize that in the loss function, the smoother the distribution you’ll get. This is called regularization, where we augment the loss function with a regularization loss. We can take the entries of `w`, square them, and sum them (or take the mean) to get a regularization loss.  If `w` has non-zero numbers, you accumulate loss. We can add this to the loss function: `loss + regularization_strength * mean(w^2)`. This optimization now has two components: matching the probabilities indicated by the data and making all of the w’s zero. Minimizing this loss forces w to be zero. It’s like adding a spring force that pushes `w` to be zero. The regularization strength controls the smoothing. Adding more counts in the bigram model corresponds to increasing this regularization strength.

To sample from this neural network, we start with zero. We take the current `ix` row of `p`, the probability row, sample the next index, and accumulate it. The probabilities are now derived from the neural network. We take `ix`, encode it as a one-hot row, `x_inc`. `x_inc` multiplies with `w`, which plucks out the corresponding row of `w`, giving logits. We normalize these logits, exponentiate them to get counts, normalize to get the distribution, and then sample. We get the same results because this neural network is essentially the identical model as before, with `w` being the log counts we estimated. We came to this answer in a different way, with a different interpretation, but fundamentally, it's the same model. 

We've covered a lot. We introduced the bigram character-level language model, how to train, sample, and evaluate the model using negative log-likelihood loss. We trained the model in two ways that give the same result. First, we counted frequencies of bigrams and normalized. Then, we used negative log-likelihood loss to guide the optimization of the counts array using a gradient-based framework. Both approaches resulted in the same model. The gradient-based framework is more flexible. Right now, our neural network is simple, taking a single previous character through a linear layer. This will become more complex. We will take more previous characters and feed them into a neural net. This neural net will still output logits, normalized in the same way, and the loss and gradient-based framework will remain identical. The neural net will evolve to transformers.
