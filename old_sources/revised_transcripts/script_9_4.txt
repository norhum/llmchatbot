Here is the cleaned-up lecture transcript:

The configuration shows that we initialized B cap size to 50,257. Let's override just that element to be 50,304. Everything else stays the same; we are just increasing our vocabulary size. It's almost like we're adding fake tokens so that the vocabulary size has powers of two inside it. By the way, what I'm doing here is increasing the amount of computation that our network will be doing. If you count the flops, we will be doing more flops. We still have to think through whether this breaks anything, but if I just run this, let's see what we get. Currently, this ran in maybe 96.5 milliseconds per step. I'm just kind of eyeballing it, and let's see what kind of result we're going to get. While this is compiling, let's think through whether our code actually works when we increase the vocabulary size like this. Let's look at where vocabulary size is actually used. We swing up to the `inet` and see that it's used inside the embedding table, all the way at the bottom of the Transformer, and at the classifier layer, all the way at the top of the Transformer. So, it's used in two places. We're running at 93 milliseconds instead of 96.5, so we are seeing a roughly 4% improvement here by doing more calculations. The reason for this is we fixed an ugly number and made it a nice number. I will come into the explanation for that a little bit later, but for now, let's just convince ourselves that we're not breaking anything when we do this. First of all, we've made the embedding table for the tokens larger. It's almost like we introduced more tokens at the bottom, and these tokens are never used because the gpt tokenizer only has tokens up to 50,256. We will never index into the rows that we've added, so we're wasting a little bit of space here by creating memory that's never going to be accessed or used. That's not fully correct because this `wte` weight ends up being shared and is used in the classifier here at the end. What is that doing to the classifier? We are predicting additional dimensions at the classifier now. We are predicting probabilities for tokens that will never be present in the training set. Therefore, the network has to learn that these probabilities have to be driven to zero, and so the logits that the network produces have to drive those dimensions of the output to negative infinity. That's no different from all the other tokens that are already in our data set, or rather, that are not in our data set. Shakespeare probably only uses, let's say, a thousand tokens out of 50,257 tokens. Most of the tokens are already being driven to zero probability by the optimization. We've just introduced a few more tokens now that, in a similar manner, will never be used and have to be driven to zero in probability. Functionally, though, nothing breaks. We're using a bit more extra memory, but otherwise, this is a harmless operation as far as I can tell. We're adding calculation, but it's running faster. It's running faster because, as I mentioned, in CUDA, so many kernels use block tiles, and these block tiles are usually nice numbers, powers of two. Calculations are done in chunks of 64 or chunks of 32. When your desired calculation doesn't neatly fit into those block tiles, there are all kinds of boundary kernels that can kick in to do the last part. Basically, in a lot of kernels, they will chunk up your input, do the nice part first, and then they have a whole second phase where they come back to any that remains and process the remaining part. The kernels for that could be very inefficient, so you're basically spinning up all this extra compute that is extremely inefficient. You might as well pad your inputs and make them fit nicely. Usually, that empirically ends up running faster. This is another example of a 4% improvement that we've added, and this is something that torch compile did not find for us. You would hope that torch compile, at some point, could figure out an optimization like this. But for now, this is it. I also have to point out that we're using pytorch nightly, so that's why we're only seeing 4%. If you're using pytorch 2.3.1 or earlier, you would actually see something like a 30% improvement just from this change, from changing it from 50,257 to 50,304. Again, this is one of my favorite examples of having to understand the under-the-hood and how it all works and knowing what kinds of things to tinker with to push the performance of your code. At this point, we have improved the performance by about 11x. We started at about 1,000 milliseconds per step, and now we're down to like 93 milliseconds, so that's quite good, and we are doing a much better job of utilizing our GPU resources. I'm going to now turn to more algorithmic changes and improvements to the actual optimization itself. What we would like to do is follow the hyperparameters that are mentioned in the GPT-2 or GPT-3 paper. Sadly, GPT-2 doesn't actually say too much. It's very nice of them that they released the model weights and the code, but the paper itself is extremely vague as to the optimization details. The code itself, that they released as well, is just the inference code, so there's no training code here, and very few hyperparameters, so this doesn't tell us too much either. For that, we have to turn to the GPT-3 paper. In the appendix of the GPT-3 paper, they have a lot more hyperparameters for us to use. The GPT-3 paper, in general, is a lot more detailed as to all of the small details that go into the model training. GPT-3 models were never released, so GPT-2 has the weights but no details, and GPT-3 has lots of details but no weights. Roughly speaking, GPT-2 and GPT-3 architectures are very similar. There are very few changes. The context length was expanded from 1024 to 2048, and that's kind of the major change. Some of the hyperparameters around the Transformer have changed, but otherwise, they're pretty much the same model. It's just that GPT-3 was trained for a lot longer on a bigger data set and has a lot more thorough evaluations. The GPT-3 model is 175 billion parameters instead of 1.6 billion in GPT-2. Long story short, we are going to the GPT-3 paper to follow along with some of the hyperparameters. To train all the versions of GPT-3, they used Adam with beta 1 and beta 2 of .9 and .95. Let's swing over here and make sure that the betas parameter, which you can see here defaults to 0.9 and .999, is actually set to 0.9 and .95. The epsilon parameter, you can see is the default is 1e-8, and this is also 1e-8. Let's just put it in so that works explicitly. Next up, they say they clip the global norm of the gradient at 1.0. What this is referring to is that once we calculate the gradients, right after `l.backward`, we have the gradients at all the parameter tensors. People like to clip them to have some kind of a maximum norm. In PyTorch, this is fairly easy to do. It's one line of code that we have to insert right after we calculate the gradients. This utility function calculates the global norm of the parameters. Every single gradient on all the parameters, you square it, add it all up, and take a big square root of that, and that's the norm of the parameter vector. Basically, it's the length of it. We are making sure that its length is no more than 1.0, and we're going to clip it. The reason that people like to use this is that sometimes you can get unlucky during your optimization, maybe it's a bad data batch or something like that. If you get very unlucky in the batch, you might get a really high loss, and really high loss could lead to a really high gradient. This could basically shock your model and shock the optimization. People like to use gradient norm clipping to prevent the model from getting too big of shocks in terms of the gradient magnitude and upper bounding it in this way. It's a bit of a hacky solution; it's like a patch on top of deeper issues, but people still do it fairly frequently. The `clip_grad_norm` returns the norm of the gradient, which I like to always visualize because it is useful information. Sometimes you can look at the norm of the gradient. If it's well-behaved, things are good; if it's climbing, things are bad and they're destabilizing during training. Sometimes you could get a spike in the norm, which means there is some kind of issue or an instability. The norm here will be a norm, and let's do a `4f` or something like that. I believe this is just a float, and we should be able to print that. That's global gradient clipping. Now, they go into the details of the learning rate scheduler. They don't just use a fixed learning rate like we do here, 3e-4, but there is basically a cosine decay learning rate schedule. It's got a warm-up, and it's got a cosine decay to 10% over some horizon. We're going to implement this in a second. I just like to see the norm printed here. There we go. What happened here is the norm is actually really high in the beginning, 30 or so, and you see that as we continue training, it stabilizes at values below one. This is not that crazy or uncommon for the norm to be high in the very first few stages. What's happening here is the model is completely random, so there's a ton of learning happening very early in the network, but that learning is kind of like, you know, it's mostly learning the biases of the output tokens, so it's a bit of an unstable time, but the network usually stabilizes in a very few iterations. This looks relatively reasonable to me, except usually, I would expect, this looks a little bit funky that we go from 28 to 6 to 2 and then to 10. It's not completely insane, but it's just kind of a little bit funky. Let's now get to the learning rate scheduler. The learning rate schedule that's used here in GPT-3 is what's called a cosine decay learning schedule with warm-up. The way this looks is that the learning rate starts right at around zero, linearly ramps up over some amount of time, and then comes down with this cosine form and comes down to some minimum learning rate that's up to you. Here, the minimum learning rate is zero, but here in the paper, they said that they use cosine decay for learning rate down to 10% of its value over the first 260 billion tokens, and then training continues at 10% after that. There's a linear warm-up over the first 375 million tokens. That's about the learning rate. Let's now implement this. I already implemented it here, and the way this works is, let me scroll down first. I changed our training loop a little bit. This was `for i in max_steps`, I just changed it to `step` so that we have the notion of a step as a single optimization step in the for loop. Here, I get the LR for this step of the optimization using a new function I call `get_LR`. In PyTorch, to set the learning rate, this is the way to set the learning rate; it's a little bit gnarly. You have to iterate over different parameter groups that could exist in the optimizer, even though we currently have a single parameter group only, and you have to set the LR in this `for` loop style. We have this `get_LR`, we set the learning rate, and then on the bottom, I'm also printing it. That's all the changes I made to this loop, and then, of course, the `get_LR` is my scheduler. It's worth pointing out that PyTorch actually has learning rate schedulers, and you can use them. I believe there's a cosine learning rate schedule in PyTorch. I don't really love using that code because honestly, it's like five lines of code, and I fully understand what's happening inside these lines. I don't love to use abstractions where they're kind of inscrutable and then I don't know what they're doing. It's a personal style. The max learning rate here is let's say, 3e-4, but we're going to see that in GPT-3 here, they have a table of what the maximum learning rate is for every model size. For this one, a 12-layer, 768 GPT-3, so the GPT-3 small is roughly like a GPT-2 124M. We see that here, they use a learning rate of 6e-4. We could actually go higher. In fact, we may want to try to follow that and just set the max LR here at six. That's the maximum learning rate. The minimum learning rate is 10% of that, per the description in the paper. Some number of steps that we're going to warm up over, and then the maximum steps of the optimization, which I now also use in the for loop down here. You can go over this code if you like. It's not terribly insightful. I'm just modulating based on the iteration number which learning rate there should be. This is the warm-up region, this is the region after the optimization, and this is the region in between. This is where I calculate the cosine learning rate schedule, and you can step through this in detail if you'd like. This is basically implementing this curve, and I ran this already, and this is what it looks like. When we run, we start at some very low number. Note that we don't start exactly at zero because that would not be useful to update with a learning rate of zero. That's why there's an `it+1`, so that on the zeroth iteration, we are not using exactly zero, we're using something very low. Then we linearly warm up to the maximum learning rate, which in this case was 3e-4 when I ran it, but now would be 6e-4. Then it starts to decay all the way down to 3e-5, which was, at the time, 10% of the original learning rate. One thing we are not following exactly is that they mentioned that, let me see if I can find it again, we're not exactly following what they did because they mentioned that their training horizon is 300 billion tokens, and they come down to 10% of the initial learning rate at 260 billion, and then they train after 260 with 10%. Basically, their decay time is less than the max steps time, whereas for us, they're exactly equal, so it's not exactly faithful, but it's an okay, this is okay for us and for our purposes right now. We're just going to use this ourselves. I don't think it makes too big of a difference, honestly. I should point out that what learning rate schedule you use is totally up to you. There are many different types. Cosine learning rate has been popularized a lot by GPT-2 and GPT-3, but people have come up with all kinds of other learning rate schedules. This is an active area of research as to which one is the most effective at training these networks. Next up, the paper talks about the gradual batch size increase, so there's a ramp on the batch size that is linear. You start with a very small batch size and you ramp up to a big batch size over time. We are actually going to skip this and we are not going to work with it. The reason I don't love to use it is that it complicates a lot of the arithmetic because you are changing the number of tokens that you're processing at every single step of the optimization, and I like to keep that math very simple. Also, my understanding is that this is not like a major improvement. My understanding is that this is not like an algorithmic optimization improvement, it's more of a systems and speed improvement. Roughly speaking, this is because in the early stages of the optimization, again, the model is in a very atypical setting. Mostly, what you're learning is that you're mostly learning to ignore the tokens that don't come up in your training set very often. You're learning very simple biases and that kind of thing. Every single example that you put through your network is basically just telling you use these tokens and don't use these tokens. The gradients from every single example are extremely highly correlated. They all look roughly the same in the original parts of the optimization because they're all just telling you that these tokens don't appear and these tokens do appear. Because the gradients are very similar and they're highly correlated, why are you doing batch sizes of millions when, if you do a batch size of 32k, you're basically getting the exact same gradient early on in the training. Later in the optimization, once you've learned all the simple stuff, that's where the actual work starts and that's where the gradients become more decorrelated per example, and that's where they actually offer you statistical power, in some sense. We are going to skip this because it complicates things, and we are going to go to data being sampled without replacement during training, until an epoch boundary is reached. Without replacement means that they are not sampling from some fixed pool, then take a sequence, train on it, but then also return the sequence to the pool. They are exhausting a pool. When they draw a sequence, it's gone until the next epoch of training. We're already doing that because our data loader iterates over chunks of data. There's no replacement. They don't become eligible to be drawn again until the next epoch. We're basically already doing that. All models use a weight decay of 0.1 to provide a small amount of regularization. Let's implement a weight decay. You see here that I've already made the changes. Instead of creating the optimizer right here, I am creating a new `configure_optimizers` function inside the model, and I'm passing in some of the hyperparameters instead. Let's look at the `configure_optimizers` which is supposed to return the optimizer object. It looks complicated, but it's actually really simple. We're just being very careful, and there are a few settings here to go through. The most important thing with respect to this line is that there is a weight decay parameter here, and I'm passing that into something called `optim_groups` that eventually ends up going into the AdamW optimizer. The weight decay that's by default used in AdamW here is 0.01, so it's 10 times lower than what's used in the GPT-3 paper here. The weight decay basically makes its way into the AdamW through the optimizer groups. What else is going on here in this function? The two things that are important here are that I'm splitting up the parameters into those that should be weight-decayed and those that should not be weight-decayed. It is common to not weight-decay biases and any other one-dimensional tensors. The one-dimensional tensors are in the `no_decay_prams`. These are also things like layer norm scales and biases. It doesn't make sense to weight-decay those. You mostly want to weight-decay the weights that participate in matrix multiplications, and you want to potentially weight-decay the embeddings. We've covered in a previous video why it makes sense to decay the weights because you can sort of view it as a regularization. When you're pulling down all the weights, you're forcing the optimization to use more of the weights, and you're not allowing any one of the weights individually to be way too large. You're forcing the network to distribute the work across more channels because there's a pull of gravity on the weights. That's why we're separating it in those ways. Here, we're only decaying the embeddings and the matrix multiplication weights. We're printing the number of parameters that we are decaying and not. Most of the parameters will be decayed. One more thing that we're doing here is another optimization. Previous AdamW did not have this option, but later parts of PyTorch introduced it. I'm guarding it with an `inspect.signature`, which is basically checking if this `fused` keyword is present inside AdamW. If it is present, I'm going to use it and pass it in here because some earlier versions do not have `fused=`. Here's `adamw(fused=)`, it did not used to exist, and it was added later. There are some docs here for what's happening. By default, they do not use fused because it is relatively new, and we want to give it sufficient time. By default, they don't use fused, but fused is a lot faster when it is available and when you're running on CUDA. What that does is instead of iterating in a for loop over all the parameter tensors and updating them, that would launch a lot of kernels. Fused just means that all those kernels are fused into a single kernel. You get rid of a lot of overhead, and you call a single kernel on all the parameters that updates them. It's a kernel fusion for the AdamW update instead of iterating over all the tensors. That's the `configure_optimizers` function that I like to use. We can rerun it. We're not going to see any major differences from what we saw before, but we are going to see some prints coming from here. We see that the number of decay tensors is 50, and it's most of the parameters. The number of non-decay tensors is 98, and these are the biases and layer norm parameters mostly. There are only 100,000 of those, so most of it is decayed. We are using the fused implementation of AdamW, which will be a lot faster. If you have it available, I advise you to use it. I am not 100% sure why they don't default to it. It seems fairly benign and harmless. Also, because we are using the fused implementation, I think this is why we have dropped. Notice that the running time used to be 93 milliseconds per step, and now we're down to 90 milliseconds per step because of using the fused AdamW optimizer. In a single commit here, we are introducing fused Adam, getting improvements on the time, and we're adding or changing the weight decay, but we're only weight-decaying the two-dimensional parameters, the embeddings, and the matrices that participate in the linear. That is it for this line. One more quick note before we continue. I just want to point out that the relationship between weight decay, learning rate, batch size, the Adam parameters beta 1, beta 2, epsilon, and so on, are very complicated mathematical relationships in the optimization literature. For the most part, in this video, I'm just trying to copy-paste the settings that OpenAI used. This is a complicated topic, quite deep. In this video, I just want to copy the parameters because it's a whole different video to really talk about that in detail and give it proper justice, instead of just high-level intuitions. The next thing that I want to move on to is this paragraph here. By the way, we're going to turn back around to this when we improve our data loader. For now, I want to swing back around to this table, where you will notice that, for different models, we have different hyperparameters for the Transformer that dictate the size of the Transformer network. We also have a different learning rate. We see the pattern that the bigger networks are trained with slightly lower learning rates. We also see this batch size, where, in the small networks, they use a smaller batch size, and in the bigger networks, they use a bigger batch size. The problem for us is we can't just use a 0.5 million batch size. If I just try to come in here and I try to set, where is my `b`?, where do I call the data?  `b=16`. If I try to set, we have to be careful, it's not 0.5 million, because this is the batch size in the number of tokens. Every single one of our rows is 1024 tokens, so 0.5e6 / 1024 would need about a 488 batch size. I can't come in here and set this to 488 because my GPU would explode. This would not fit for sure. We still want to use this batch size because, as I mentioned, the batch size is correlated with all the other optimization hyperparameters and the learning rates and so on. We want to have a faithful representation of all the hyperparameters, and therefore we need to use a batch size of 0.5 million roughly. The question is how do we use 0.5 million if we only have a small GPU? For that, we need to use what's called gradient accumulation. We're going to turn to that next. It allows us to simulate in a serial way any arbitrary batch size that we set. We can do a batch size of 0.5 million, we just have to run longer, and we have to process multiple sequences, and basically, add up all the gradients from them to simulate a batch size of 0.5 million. Let's turn to that next. I started the implementation right here just by adding these lines of code. I set the total batch size that we desire. This is exactly 0.5 million. I used a nice number, a power of two, because 2 to the 19 is 524,288, so it's roughly 0.5 million, and it's a nice number. Our micro batch size is 16. This is still the `b` that goes into the Transformer and does forward and backward, but we're not going to do an update. We are going to do many forward-backwards. Those gradients are going to plus equal on the parameter gradients; they are all going to add up. We are going to do forward-backward `grad_accum_steps` number of times, and then we are going to do a single update once all that is accumulated. Our micro batch size is controlling how many tokens, how many rows, we're processing in a single go over a forward-backward. Here, we are doing 16 times 1024; we are doing 16,384 tokens per forward-backward. We are supposed to be doing 2 to the 19 in total, so `grad_accum` will be 32. `grad_accum` here will work out to 32. We have to do 32 forward-backwards, and then a single update. We see that we have about 100 milliseconds for a single forward-backward, so doing 32 of them will make every step roughly 3 seconds, just napkin math. That's `grad_accum_steps`, but now we have to implement that. We are going to swing over to our training loop because now this part here and this part here, the forward and the backward, we have to repeat this 32 times before we do everything else that follows. Let's see how we can implement that. Let's come over here. We do have to load a new batch every single time, so let me move that over here. Now, this is where we have the inner loop. For micro-step in range `grad_accum_steps`, we do this. Remember that `l.backward` always deposits gradients. Inside `loss.backward`, there is always a plus equals on the gradients, so in every `l.backward`, gradients will add up on the gradient tensors. We do `loss.backward`, we get all the gradients over there, and then we normalize. Everything else should just follow. We're very close, but there's a subtle and deep issue here, and this is actually incorrect. I invite you to think about why this is not yet sufficient, and let me fix it then. I brought back the Jupyter Notebook so we can think about this carefully in a simple toy setting and see what's happening. Let's create a very simple neural net that takes a 16 vector of 16 numbers and returns a single number. Here, I am creating some random examples, `X`, and some targets, `Y`. We are using the mean squared loss here to calculate the loss. Basically, this is four individual examples, and we are just doing simple regression with the mean squared loss over those four examples. When we calculate the loss and we call `loss.backward` and look at the gradient, this is the gradient that we achieve. The loss objective here, notice that in MSE loss, the default for the loss function is `reduction='mean'`, so we are calculating the average mean loss. This is the exact loss objective. This is the average, the one over four because there are four independent examples here. We have the four examples and their mean squared error, the squared error, and this makes it the mean squared error. We calculate the squared error and then normalize it to make it the mean over the examples. There are four examples here. Now, when we come to the gradient accumulation version of it, this here is the gradient accumulation version of it, where we have `grad_accum_steps` of four, and I reset the gradient. We have `grad_accum_steps` of four and now I'm evaluating all the examples individually instead and calling `loss.backward`.
