Okay, everyone, by now you have probably heard of ChatGPT. It has taken the world and the AI community by storm. It is a system that allows you to interact with an AI and give it text-based tasks. For example, we can ask ChatGPT to write a short haiku about the importance of people understanding AI so they can use it to improve the world and make it more prosperous. When we run this, we get, "AI knowledge brings prosperity for all to see. Embrace its power." Not bad. You can see that ChatGPT generated these words sequentially, from left to right.

I asked it the exact same prompt a little earlier, and it generated a slightly different outcome: "AI's power to grow; Ignorance holds us back, learn. Prosperity waits." Both are pretty good and slightly different. You can see that ChatGPT is a probabilistic system. For any one prompt, it can give us multiple answers.

This is just one example. People have created many, many examples. There are entire websites that index interactions with ChatGPT. Many of them are quite humorous, such as "Explain HTML to me like I'm a dog," "Write release notes for chess 2," and "Write a note about Elon Musk buying Twitter."

As another example, please write a breaking news article about a leaf falling from a tree: "In a shocking turn of events, a leaf has fallen from a tree in the local park. Witnesses report that the leaf, which was previously attached to a branch, detached itself and fell to the ground. Very dramatic." You can see that this is a pretty remarkable system.

It is what we call a language model because it models the sequence of words, characters, or more generally, tokens. It knows how words follow each other in the English language. From its perspective, it is completing a sequence. I give it the start of a sequence, and it completes the sequence with the outcome. It is a language model in that sense.

I would like to focus on the under-the-hood components of what makes ChatGPT work. What is the neural network under the hood that models the sequence of these words? It comes from a paper called "Attention is All You Need," published in 2017. This landmark paper in AI proposed the Transformer architecture. GPT is short for Generatively Pre-trained Transformer. The Transformer is the neural network that does all the heavy lifting under the hood. It comes from the 2017 paper.

The paper reads like a pretty random machine translation paper. I think the authors didn't fully anticipate the impact the Transformer would have on the field. This architecture, produced in the context of machine translation, ended up taking over the rest of AI in the next five years. This architecture, with minor changes, was copy-pasted into a huge amount of applications in AI in recent years. It is at the core of ChatGPT.

We are not going to reproduce ChatGPT. It is a serious, production-grade system. It is trained on a good chunk of the internet, with a lot of pre-training and fine-tuning stages. It is very complicated. I would like to focus on training a Transformer-based language model. In our case, it will be a character-level language model. I think that is very educational regarding how these systems work. We will not train on a chunk of the internet. We need a smaller data set. I propose that we work with my favorite toy dataset called "Tiny Shakespeare."

It is a concatenation of all the works of Shakespeare. This file is about one megabyte and is just all of Shakespeare. We are going to model how these characters follow each other. For example, given a chunk of characters like this, given some context of characters in the past, the Transformer neural network will look at the highlighted characters and predict that "g" is likely to come next in the sequence. We will train the Transformer on Shakespeare. It will try to produce character sequences that look like this. In that process, it will model all the patterns inside this data.

Once we have trained the system, we can generate infinite Shakespeare. Of course, it is fake but looks kind of like Shakespeare. Apologies for some jank that I have not been able to resolve. You can see how it goes character by character, predicting Shakespeare-like language. For instance, "verily my Lord the sites have left again the king coming with my curses with precious pale," and then Tranos says something else. This is coming out of the Transformer in a manner similar to how it would in ChatGPT. In our case, it is character by character. In ChatGPT, it is token by token. Tokens are sub-word pieces, not word-level but more like word chunk level.

I have already written all of the code to train these Transformers. It is in a GitHub repository called nanogpt. nanogpt is a repository for training Transformers on any given text. There are many ways to train Transformers, but this is a very simple implementation. It consists of two files of 300 lines of code each. One file defines the GPT model, the Transformer, and one file trains it on a given text dataset.

I am showing that if you train it on an Open Web Text dataset, which is a fairly large dataset of webpages, then I reproduce the performance of GPT-2. GPT-2 is an earlier version of OpenAI GPT from 2017, if I recall correctly. I have only reproduced the smallest 124-million-parameter model so far, but this proves that the codebase is correctly arranged. I can load the neural network weights that OpenAI released later. You can take a look at the finished code in nanogpt.

I would like to write this repository from scratch in this lecture. We will begin with an empty file and define a Transformer piece by piece. We will train it on the Tiny Shakespeare dataset. Then we will see how we can generate infinite Shakespeare. This can be copy-pasted to any arbitrary text dataset. My goal is to help you understand and appreciate how ChatGPT works under the hood. All that is required is proficiency in Python, a basic understanding of calculus and statistics, and it would help if you have seen my previous videos, in particular, my "Make More" series. Those videos define smaller and simpler neural network language models, like multi-layer perceptrons, and introduce the language modeling framework. In this video, we will focus on the Transformer neural network itself.

I created a new Google Colab Jupyter notebook. This will allow me to share the code with you. This will be in the video description. I downloaded the Tiny Shakespeare dataset from this URL. It is about a one-megabyte file. I open the "input.txt" file and read all of the text as a string. We see that we are working with roughly one million characters. The first 1,000 characters, if we print them, are what you would expect. These are the first 1,000 characters of the Tiny Shakespeare data set, roughly up to here. So far, so good.

Next, we take this text, which is a sequence of characters in Python. When I call the `set` constructor, I get the set of all characters that occur in this text. Then, I call `list` on that to create a list of those characters instead of a set, so that I have an arbitrary ordering. Then, I sort that list. We get all of the characters that occur in the data set, sorted. The number of them will be our vocabulary size. These are the possible elements of our sequences. When I print the characters, there are 65 of them. There is a space character, all kinds of special characters, and upper and lowercase letters. That is our vocabulary and the possible characters the model can see or emit.

Next, we need a strategy to tokenize the input text. When people say tokenize, they mean converting the raw text string to a sequence of integers according to a vocabulary of possible elements. As an example, we are building a character-level language model. We will translate individual characters into integers. I will show you a chunk of code that does that for us. We are building both the encoder and decoder.

When we encode an arbitrary text, like "hi there," we will receive a list of integers that represents that string, for example, 46, 47, etc. We also have the reverse mapping, so we can take this list and decode it to get back the exact same string. It is a translation to integers and back for any arbitrary string. For us, it is done on a character level.

We achieve this by iterating over all the characters and creating a lookup table from the character to the integer and vice versa. To encode a string, we translate all the characters individually. To decode it, we use the reverse mapping and concatenate everything. This is only one of many possible encodings or tokenizers. It is a very simple one, but there are many other schemas. For example, Google uses SentencePiece, which also encodes text into integers using a different schema and vocabulary. SentencePiece is a sub-word tokenizer. It encodes text into sub-word units, not entire words or individual characters. This is what is typically used in practice.

OpenAI also has a library called "tiktoken" that uses a byte pair encoding tokenizer. This is what GPT uses. You can also encode words into a list of integers, such as "hello world." Using tiktoken, I get the encoding for GPT-2. Instead of 65 possible characters or tokens, they have 50,000 tokens. When they encode "hi there," we only get a list of three integers. Those integers are not between 0 and 64 but between 0 and 50,256. You can trade off the codebook size and sequence lengths. You can have very long sequences of integers with small vocabularies or short sequences with large vocabularies. People typically use these sub-word encodings. I will keep our tokenizer simple. We are using a character-level tokenizer. That means we have very small code books and simple encode and decode functions. We get very long sequences as a result. We will stick with this level for this lecture because it is the simplest thing.

Now that we have an encoder and decoder, effectively a tokenizer, we can tokenize the entire training set of Shakespeare. Here is a chunk of code that does that. I will start to use the PyTorch library, specifically `torch.tensor`. We will take all of the text in Tiny Shakespeare, encode it, and wrap it into a `torch.tensor` to get the data tensor. Here is what the data tensor looks like when I look at the first 1,000 elements. We see a massive sequence of integers. This sequence of integers is an identical translation of the first 1,000 characters here. I believe, for example, that zero is a new line character and maybe one is a space, but I'm not 100% sure. From now on, the entire data set of text is re-represented as a single, very large sequence of integers.

Let me do one more thing before we move on. I will separate our dataset into a train and validation split. We will take the first 90% of the dataset and consider that the training data for the Transformer. We will withhold the last 10% as validation data. This will help us understand to what extent our model is overfitting. We want to hide and keep the validation data aside. We don't want a perfect memorization of Shakespeare. We want a neural network that creates Shakespeare-like text. It should be fairly likely to produce actual Shakespeare text. We will use this to get a sense of overfitting.

Now we would like to start plugging these text sequences or integer sequences into the Transformer so that it can train and learn the patterns. The important thing to realize is that we are never going to feed entire text into a Transformer all at once. That would be computationally expensive and prohibitive. When we train a Transformer on these data sets, we only work with chunks of the dataset. We sample random little chunks out of the training set and train on just chunks at a time. These chunks have a length, a maximum length. The maximum length, in my code, is typically called `block_size`. You can find it under other names, like context length. Let's start with a block size of eight. Let me look at the first training data characters, the first `block_size + 1` characters. I'll explain why plus one in a second. This is the first nine characters in the sequence in the training set.

When you sample a chunk of data like this, these nine characters out of the training set, it actually has multiple examples packed into it. All of these characters follow each other. When we plug this into a Transformer, we will train it to make a prediction at every one of these positions simultaneously. In a chunk of nine characters, there are actually eight individual examples. There's the example that when in the context of 18, 47 likely comes next. In a context of 18 and 47, 56 comes next. In a context of 18, 47, and 56, 57 can come next, and so on. There are eight individual examples.

Let me spell it out with code. X is the input to the Transformer. It will be the first `block_size` characters. Y will be the next `block_size` characters, offset by one. Y is the target for each position in the input. I am iterating over the `block_size` of eight. The context is always all of the characters in X up to T, including T. The target is always the Tth character but in the targets array Y. Let me run this, and it spells out what I said. These are the eight examples hidden in a chunk of nine characters that we sampled from the training set.

We train on all eight examples with context between one and up to `block_size`. It is not just for efficiency. It makes the Transformer Network used to seeing contexts all the way from as little as one up to the `block_size`. We like the Transformer to be used to seeing everything in between. This will be useful later during inference. While sampling, we can start the sampling generation with as little as one character of context. The Transformer knows how to predict the next character with a context of one. It can predict everything up to `block_size`. After `block_size`, we have to start truncating because the Transformer will never receive more than `block_size` inputs when it is predicting the next character.

We have looked at the time dimension of the tensors that will be fed into the Transformer. There is one more dimension to care about, and that is the batch dimension. As we sample these chunks of text, every time we feed them into a Transformer, we will have many batches of multiple chunks of text stacked up in a single tensor. That is for efficiency so that we can keep the GPUs busy. They are very good at parallel processing of data, so we want to process multiple chunks at the same time. Those chunks are processed completely independently.

Let me generalize this and introduce a batch dimension. Here is a chunk of code. I will run it and then explain what it does. Because we will sample random locations in the dataset to pull chunks from, I am setting the seed for the random number generator. The numbers I see here will be the same numbers you see if you reproduce this. The batch size is how many independent sequences we are processing every forward and backward pass of the Transformer. The `block_size`, as I explained, is the maximum context length to make the predictions.

Let's say the batch size is four and the block size is eight. Here is how we get a batch for any arbitrary split. If the split is a training split, then we look at train data. Otherwise, we look at valid data. That gives us the data array. When I generate random positions to grab a chunk out of, I generate a `batch_size` number of random offsets. Because this is four, `xs` will be four numbers randomly generated between zero and the length of the data minus the `block_size`. These are random offsets into the training set.

X's, as I explained, are the first `block_size` characters starting at I. The Y's are offset by one. Then, we will get those chunks for every integer I in `xs`. We use `torch.stack` to take all of those one-dimensional tensors and stack them up as rows. They become a row in a 4x8 tensor. When I sample a batch, `XB` and `YB`, the inputs to the Transformer are the input X, a 4x8 tensor, four rows of eight columns. Each one of these is a chunk of the training set. The targets are in the associated array, Y. They will come to the Transformer at the end to create the loss function. They will give us the correct answer for every single position inside X.

These are the four independent rows. Spelled out as we did before, this 4x8 array contains a total of 32 examples. They are completely independent. When the input is 24, the target is 43, or rather, 43 in the Y array. When the input is 24 and 43, the target is 58. When the input is 24, 43, and 58, the target is 5, etc. You can see this spelled out. These are the 32 independent examples packed into a single batch of the input X. The desired targets are in Y. This integer tensor of X will feed into the Transformer, which will simultaneously process all these examples and look up the correct integers to predict in every position of the tensor Y.

Now that we have a batch of input that we would like to feed into a Transformer, let's start feeding this into neural networks. We will start with the simplest possible neural network, which, in the case of language modeling, is the bigram language model. We have covered the bigram language model in my "Make More" series. Here, I will go faster. Let's implement a PyTorch module directly that implements the bigram language model. I am importing the PyTorch `nn` module for reproducibility. Here, I am constructing a bigram language model, which is a subclass of `nn.Module`. I call it and pass it the inputs and targets. I am just printing it now. When the inputs and targets come here, you see that I am taking the indices X, which I rename to `idx`, and passing them into this token embedding table.

Here, in the constructor, we are creating a token embedding table of size `vocab_size` by `vocab_size`. We are using an `nn.Embedding`, which is a thin wrapper around a tensor of shape `vocab_size` by `vocab_size`. When we pass `idx` here, every integer in our input refers to this embedding table. It plucks out a row of that table corresponding to its index. 24 here will go into the embedding table and pluck out the 24th row. 43 will go here and pluck out the 43rd row, etc. PyTorch will arrange all of this into a batch by time by channel tensor. In this case, the batch is four, the time is eight, and C, the channels, is `vocab_size`, or 65. We pluck out all those rows and arrange them in a B by T by C. We interpret this as the logits, which are the scores for the next character in the sequence. We are predicting what comes next based on the individual identity of a single token. Currently, the tokens are not talking to each other. They are not seeing any context except for themselves. Knowing they are token number five can make decent predictions about what comes next. Some characters follow other characters in typical scenarios. We saw this in a lot more depth in the "Make More" series. If I run this, we get the predictions, the scores, the logits, for every one of the 4x8 positions.

Now that we have made predictions about what comes next, we would like to evaluate the loss function. In the "Make More" series, we saw that a good way to measure loss is to use the negative log-likelihood loss, which is also implemented in PyTorch under the name `cross_entropy`. We would like the loss to be the cross-entropy of the predictions and the targets. This measures the quality of the logits with respect to the targets. We have the identity of the next character. How well are we predicting the next character based on the logits? Intuitively, the correct dimension of the logits, depending on whatever the target is, should have a very high number, and all the other dimensions should be very low numbers.

Unfortunately, this will not run. We get an error message. Intuitively, we want to measure this. When we go to the PyTorch `cross_entropy` documentation, we see that PyTorch expects certain input dimensions. If you have multi-dimensional input, which we do because we have a B by T by C tensor, it wants the channels to be the second dimension. It wants a B by C by T instead of a B by T by C. It is just the details of how PyTorch treats these kinds of inputs. We do not want to deal with that. We will reshape our logits. I like to take the logits' shape, which is B by T by C, unpack those numbers, and then say that logits equals `lit.view` We want it to be a B * T by C, a two-dimensional array. We take all of the positions here and stretch them out in a one-dimensional sequence and preserve the channel dimension as the second dimension. We are stretching out the array so it is two-dimensional. It will better conform to what PyTorch expects in its dimensions.

We have to do the same to targets. Currently, targets are of shape B by T, and we want it to be B * T, one-dimensional. Alternatively, you could do minus one because PyTorch will guess what this should be if you want to lay it out. Let me be explicit and say B * T. Once we have reshaped this, it will match the cross-entropy case, and we should be able to evaluate our loss. We can run that. The loss is 4.87. Because we have 65 possible vocabulary elements, we can guess what the loss should be. In particular, we covered negative log likelihood in detail. We expect the log of 1 over 65 and the negative of that. We expect the loss to be about 4.17. We are getting 4.87. That tells us that the initial predictions are not super diffuse. They have a little bit of entropy, and so we are guessing wrong. We are able to evaluate the loss.

Now that we can evaluate the quality of the model, we would also like to generate from the model. Let's generate now. I will go a little bit faster here because I covered all of this in previous videos. Here is a generate function for the model. We take an input `idx`. This is the current context of some characters in a batch, so it is B by T. The job of generate is to take this B by T and extend it to be B by T+1, +2, +3. It continues the generation in all batch dimensions in the time dimension. It does that for `max_new_tokens`. You can see that whatever is predicted is concatenated on top of the previous `idx` along the first dimension, the time dimension, to create a B by T+1. That becomes a new `idx`. The job of generate is to take a B by T and make it B by T + 1 + 2 + 3, as many as we want, `max_new_tokens`. This is the generation from the model.

Inside the generation, we take the current indices and get the predictions. Those are in the logits. The loss is ignored here because we are not using it. We have no ground truth targets that we are comparing with. Once we get the logits, we focus on the last step. Instead of a B by T by C, we pluck out the -1, the last element in the time dimension. Those are the predictions for what comes next. This gives us the logits, which we then convert to probabilities via softmax. Then we use `torch.multinomial` to sample from those probabilities, asking PyTorch to give us one sample. `idx_next` will become a B by one because, in each of the batch dimensions, we will have a single prediction for what comes next. The `num_samples=1` makes this one. We take those integers from the sampling process according to the probability distribution given here. Those integers are concatenated on top of the current running stream of integers. This gives us a B by T+1. Then we can return that.

You see how I am calling `self(idx)`, which will go to the forward function. I am not providing any targets. Currently, this would give an error because targets are not given. Targets have to be optional, so targets are none by default.
