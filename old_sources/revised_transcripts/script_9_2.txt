The next batch needs to have tensors moved from the CPU to the device. When I converted the tokens, they were left on the CPU, which is the default, to avoid wasting GPU memory. This is a tiny dataset and would fit on the GPU. For our purposes, it's fine to ship it to the GPU now. We keep the data loader simple on the CPU and then ship it to the GPU for computation. Let's see if `python train gpt2.py` runs. We expect to get the next batch and avoid overfitting a single batch. Therefore, the loss should come down, but not too much. I still expect the loss to come down because many tokens in the 50,257 tokens never occur in our dataset. There are some easy gains to be made in the optimization by, for example, taking the biases of the logits that never occur and driving them to negative infinity. These include crazy Unicode or different languages whose tokens never occur and their probability should be very low. The gains should be along the lines of deleting the usage of tokens that never occur, which is probably most of the loss gain we'll see at this scale. The loss shouldn't come down to zero because we are only doing 50 iterations, which is not enough for an epoch. We have 338,000 tokens, which makes sense with our 3:1 compression ratio, as there are one million characters. One epoch with the current settings of B and T will take 2,600 batches and we are only doing 50 batches of optimization here. We start off as expected, and then the loss comes down to about 6.6, so things are working okay right now. Next, I want to fix a bug in our code, which is not major, but it affects how GPT2 training should happen. We were not careful enough when loading weights from Hugging Face and missed a detail. The shape of the token embedding tensor at the bottom of the Transformer and the language modeling head at the top of the Transformer are the same. Both tensors are two-dimensional and have a shape of 50,257 x 768. The first tensor gives us our token embeddings, and the second takes the 768 channels of the Transformer and upscales them to 50,257 to get the logits for the next token. The tensors have the same shape, and if we compare their elements using `torch.all` we see that every element is identical. Also, if we look at the data pointer using `data_ptr()`, we see that the pointer is identical. These are not just two separate tensors with the same shape and elements; they are pointing to the identical tensor. This is a common weight tying scheme that comes from the original "Attention is All You Need" paper. The paper mentions that the model shares the same weight matrix between the two embedding layers and the pre-softmax linear transformation. This means that the two are shared, tied, and are the same matrix. The idea for this comes from the concept that if two tokens are semantically similar, they should be nearby in the token embedding space, and they should get the same probabilities at the output of a Transformer. Both positions, at the bottom and top of the Transformer, should have the property that similar tokens should have similar embeddings or weights. This motivates the exploration to tie the matrices, which leads to better performance. The authors also observe that output embeddings behave like word embeddings. This was adopted in the "Attention is All You Need" paper and used again in GPT2. I could not find it in the Transformers implementation, but it is in the original GPT2 code released by OpenAI. In their model's forward pass, they get the `wte` token embeddings and reuse it to do the logits. The `wte` tensor is used twice, at the bottom and top of the Transformer. In the backward pass, we'll get gradient contributions from both branches, which will add up on the `wte` tensor. We are currently not sharing `wte` in our code but want to do so. Here's one way to do it: We take the `wte.weight` and redirect it to point to the `lm_head`. This copies the data pointer and the old `wte.weight` value is orphaned and cleaned up by Python. We are left with a single tensor used twice in the forward pass. This is to my knowledge all that is required and this should train better with this exact tensor used twice. According to the paper, we expect slightly better results by doing this. This is also nice because it saves parameters. The size is 768 * 50,257, which is 40 million parameters. The model has 124 million parameters, so this is like saving 30% of the parameters. This could be why it works better with weight tying when not training long enough, as there are fewer parameters. It also provides an inductive bias that these embeddings should share similarities between tokens. Next, we need to be more careful with initialization and follow how GPT2 initializes their model. The GPT2 and GPT3 papers are not very explicit about initialization, so we need to read between the lines. The OpenAI code initializes weights with a standard deviation of 0.02 using a normal distribution, and biases are initialized to zero. The token embeddings are initialized at 0.02 and positional embeddings at 0.01. Here's some code to mirror that: At the end of the GPT module's initializer, we call the `apply` function of `nn.Module`. This iterates over all the sub-modules and applies the `init_weights` function on them. If a module is an `nn.Linear` module, we initialize the weight with a normal distribution with a standard deviation of 0.02, and the bias to zero. We use 0.02 for the embedding, and keep the positional embedding to the same instead of 0.01. The only other layer that needs initialization is the layer norm. The PyTorch default initialization sets the scale in the layer norm to one and the offset to zero, which is what we want. We are following the GPT2 source code for initialization. The standard deviation on this initialization, if following Xavier initialization, would be one over the square root of the number of incoming features, but 0.02 is consistent with that. One over the square root of 768 is 0.03. One over the square root of 1600 is 0.02. 0.02 is roughly in the vicinity of reasonable values for these initializations. We will keep it at 0.02 because that is the GPT2 initialization per their source code. We are not fully done yet on initialization. A model initialization that accounts for accumulation on the residual path with model depth is used. We scale the weight of residual layers of initialization by a factor of one over the square root of n, where n is the number of residual layers. If starting with zeros in the residual stream, each residual block contributes an amount and gets added. The variance of activations in the residual stream grows. We have a small example of starting at zero with 768 zeros in a residual stream and add random numbers from a normal distribution with zero mean and one standard deviation 100 times. At the end, the residual stream has grown to have a standard deviation of 10. The scaling factor compensates for this growth. If we scale down every contribution into the residual stream by one over the square root of n (or n^-0.5), we get a standard deviation of one, which controls the growth of activations inside the residual stream. We want to initialize the weights at the end of each block, the C layer, by scaling down the weights by one over the square root of the number of residual layers. Here's a way to implement this: In the initialization, we set a flag, `scale_init`, to one. When we iterate through the modules, if the module has this flag, we scale down the standard deviation by one over the square root of twice the number of layers, raised to the power of 0.5. The two times number of layers comes from the fact that every layer in the Transformer has two blocks that add to the residual pathway: the attention and the MLP. Also, because we are weight sharing `wte` and the `lm_head`, we're going to come around to that tensor twice. It will be initialized as an embedding and then as a linear layer with the identical initialization of 0.02. I have added some code for reproducibility, setting the seeds. Now we should be able to run `python train gpt2.py`. At this point, we have the GPT2 model, some confidence that it's correctly implemented, and it is properly initialized. We have a data loader that iterates through data batches, and we can train. Now comes the fun part; speeding up the training. We need to get our money's worth with respect to the hardware. First, you want to consider what hardware you have and if you are fully utilizing it. I have eight A100 SXM 80 GB GPUs. Lambda Labs is where I spin up these kinds of boxes and you pay per hour. When looking at the A100s, there are numbers for how many calculations you can expect from this GPU. When breaking into the code after calculating the logits and loss, we see that `lit.dtype` prints `torch.float32`. By default, PyTorch tensors are in float32, which means every number uses 32 bits. This is too much memory, and deep learning can tolerate significantly lower precisions. These GPUs support up to fp64, which is useful for scientific computing. We don't need that much precision for deep learning. Currently, we are using fp32, and we expect to get at most 19.5 Teraflops of performance. If we are willing to go down in precision to tf32, we can get an 8x improvement. If we are willing to go down to float16 or bfloat16, we can get 16x performance. Nvidia likes to cite numbers with sparsity, but we are not using that. We could have gotten more with int8, but int8 is used for inference, not training. int8 has uniform spacing, and we need a float to match the normal distributions that occur during training of neural networks. By reducing the precision, we can get more Teraflops and reduce the memory bandwidth and memory needs. It becomes easier to move the numbers around with fewer bits of representation. The memory bandwidth is a precious resource, and many deep learning workloads are memory bound. The tensor cores are often waiting because we can't load data fast enough from memory. If you are getting 60% utilization, you are doing extremely well. By coming down in precision for all the floats, weights and activations require less memory, so we can store and access it faster, which speeds everything up. First, let's look at the tensor float32 format. Tensor cores are just an instruction in the A100 architecture that does 4x4 matrix multiplication. There are multiple configurations for precision. Operations that require matrix multiplication get broken up into this 4x4 multiply instruction. Most of the computational work is matrix multiplication in the linear layers. The biggest matrix multiplication is the classifier layer, going from 768 to 50,257. Matrix multiplication is accelerated through tensor cores. The best reference for tensor cores is the A100 architecture white paper. Figure 9 shows tensor float 32. The input operands, accumulator, and the internal representation in the instruction have multiple configuration options. Normally, fp32 has 32 bits. tf32 has the same bits, except the mantissa bits get cropped in the float, ending up with just 19 bits. This allows it to calculate the little matrix multiply significantly faster, 8x faster. Our code will not change; it's internal to the operation. The results are approximate, but empirically, you can't tell the difference. tf32 is a good option; if you can tolerate a little bit of a precision fudge, you can go 8x faster. Let's look at what that looks like. I've set up our code to time the iterations. I changed the hyperparameters to reflect a workload we want to run. We'll use a batch size of 16 and the GPT2 maximum sequence length of 1024 tokens. I'm using `time.time()` to get the current time. The optimization loop includes a `torch.cuda.synchronize()`. This waits for the GPU to finish all the work that was scheduled to run before taking time. I'm going to print the iteration time. I'm also using `nvidia-smi` to watch GPU usage. PyTorch uses GPU0 by default, and we see it is filled up with about 35 GB out of 80GB. With the batch size cranked up, it's only 20 batches to do a single epoch on our tiny Shakespeare dataset. We see roughly 1000 milliseconds per iteration. The first iteration is slower due to PyTorch initializations. When timing, be careful about that first iteration. Our baseline in float32 is 1000 milliseconds per iteration and will run for roughly 50 seconds. If this doesn't fit into your GPU, decrease the batch size until it does. Use numbers with powers of two, like 16, 8, 24, 32, or 48. Do not use numbers like 17, because they will run inefficiently on the GPU. I'm also calculating the tokens per second throughput, which is our objective measure, because we might change the batch size. Right now, we're processing 163,000 tokens per second. Now, let's enable tf32. Luckily, PyTorch makes this easy. To enable tf32, we use a single line. This line tells PyTorch what kind of kernels to run. By default, it uses the highest precision for `matmul`, which means everything happens in float32. If we set it to high, as we do here, then we enable TF32.
