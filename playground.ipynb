{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the multi-line text file\n",
    "with open('lecture9.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Step 2: Strip whitespace from each line and join them into a single string\n",
    "single_line_text = ' '.join(line.strip() for line in text.splitlines() if line.strip())\n",
    "\n",
    "# Step 3: Save the result to a new text file\n",
    "with open('transcript9.txt', 'w') as file:\n",
    "    file.write(single_line_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Use the API key\n",
    "genai.configure(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down how `n.grad` gets updated after calling `o._backward()` in Andrej's lecture, focusing on the core logic within the `Value` class he's building.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "*   **`Value` Objects:** Andrej's custom `Value` class wraps scalar values and keeps track of the operations that created them. Crucially, each `Value` object has a `grad` attribute, which stores the gradient (derivative) of the final output with respect to that particular value.\n",
      "*   **`_backward` Functions:** Each `Value` object has a `_backward` function. This function's job is to implement the *local* part of the chain rule, passing the derivative backward through that node to its children (inputs). The `_backward` function *doesn't* do any backpropagation directly itself. Instead, when it's called, it looks up the derivative of the *current node* from its child node and then chains that derivative with the derivatives of *this current node with respect to its children*, and then the final results is *added* to the gradient to the previous value.\n",
      "*   **Topological Sort:** Andrej uses a topological sort to determine the order in which the `_backward` functions should be called. This ensures that we never attempt to backpropagate to a node before we have all its dependencies evaluated.\n",
      "*   **Chain Rule:** The essence of backpropagation! The chain rule dictates that when propagating gradients backward through a series of operations, the derivatives should be multiplied. In the local operations implemented by Andrej, he applies the derivatives of the current value with respect to each of its children and multiplies that by the derivative of the node from where the gradient was being backpropagated (the parent node).\n",
      "\n",
      "**How `n.grad` Gets Updated:**\n",
      "\n",
      "1.  **`o.grad = 1`:**\n",
      "    *   Andrej starts by setting the `grad` attribute of the output node `o` to 1.0.  This is the base case of backpropagation and represents dL/dL (the derivative of the loss with respect to itself), which is always 1.\n",
      "2.  **`o._backward()` is Called:**\n",
      "    *   This triggers the `_backward` method specific to the operation that created `o` (which is a `tanh` operation that in turn gets translated to more atomic operations in the later part of the lecture).\n",
      "    *   Inside `o._backward()`, it calculates the *local derivative* which is d(tanh(n))/dn = 1 - tanh(n)\\*\\*2, where n is the input to tanh. This value multiplied by `o.grad`, and `o.grad` is set to 1 by the previous operation. Therefore `n.grad += (1-tanh(n)**2) * o.grad`.\n",
      "    *   The derivative of the tanh node with respect to n then gets added to the `n.grad` attribute. So if `n.grad` was 0, it now has the value of the local derivative. Importantly, the chain rule multiplies the local derivative by the `grad` of the output node (`o.grad`), not the data of the output node.\n",
      "    *   **Important:** At this point, `n`'s `grad` attribute is *updated* based on the local derivative and the `grad` of its output node.\n",
      "3.  **Subsequent `_backward()` Calls:**\n",
      "    *   The topological sort ensures that `_backward()` functions of `n`'s children are now called and the updates to the gradients follow until the leaf nodes. In the later versions, `n` will be some summation of values, or a result of the multiplication of some values.\n",
      "    *   Each one of those  `_backward()` functions will do a similar operation of taking the `grad` attribute of the output, multiplying it by the local derivative of the function with respect to its input and adding that on to the input node's gradient, which may or may not have previous values from its other child nodes.\n",
      "    *  Because the operations in the values class perform *addition* onto the `grad` attributes as shown by `+=` in the code that Andrej writes, if a node was used multiple times in an expression, then all the derivatives from all the child nodes are going to be accumulated at that node.\n",
      "\n",
      "**In essence, the update of `n.grad` doesn't happen magically during `o._backward()`, but rather as part of a chain of events during the backpropagation process. The `_backward` functions don't directly change `n.grad`, instead, it takes `o.grad` *chains* it through the local derivative and then the result is added onto `n.grad`. The chain rule takes care of the \"communication\" between the nodes that enables the backward pass.**\n",
      "\n",
      "**Code Snippets (Simplified)**\n",
      "\n",
      "Here's a simplified look at the relevant parts of the `Value` class for the `tanh` operation:\n",
      "\n",
      "```python\n",
      "class Value:\n",
      "    def __init__(self, data):\n",
      "        self.data = data\n",
      "        self.grad = 0.0\n",
      "        self._backward = lambda: None  # default empty backward\n",
      "\n",
      "    def tanh(self):\n",
      "        out = Value(math.tanh(self.data))\n",
      "        def _backward():\n",
      "            # Assume local derivative is (1 - out.data**2)\n",
      "            self.grad += (1 - out.data**2) * out.grad\n",
      "        out._backward = _backward\n",
      "        return out\n",
      "```\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "*   `o._backward`'s job is not to change `n.grad` directly, but rather to contribute to it via a chain of events of calling the back prop functions of its inputs.\n",
      "*   The topological sort is essential for guaranteeing the correct order of `_backward` calls, which ensures that values are never accessed before they are set.\n",
      "*   The local derivatives are combined via the chain rule to compute the overall gradients.\n",
      "\n",
      "I hope this breakdown clarifies how `n.grad` gets updated after `o._backward()` is called. Let me know if you have more questions!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Select the Gemini Pro model (free tier)\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
    "\n",
    "lecture_transcript = transcript[0]\n",
    "\n",
    "question = '''Hi please help me\n",
    "In video I observed when andrej \n",
    "\n",
    "O.grad = 1\n",
    "O._bacward()\n",
    "\n",
    "After executing o._backward how n.grad value gets updated.i did not get that part in code.\n",
    "'''\n",
    "\n",
    "\n",
    "prompt = f\"Here is the lecture transcript: '{lecture_transcript}' Answer the following question based on the provided lecture transcript: '{question}'\"\n",
    "\n",
    "response = model.generate_content(prompt)\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "\n",
    "# image_path = r\"C:\\Users\\hyoil\\Desktop\\qnallm\\image.png\"\n",
    "# with open(image_path, \"rb\") as image_file:\n",
    "#     image_data = image_file.read()\n",
    "# image = base64.b64encode(image_data).decode(\"utf-8\")\n",
    "\n",
    "# prompt = f\"Here is the lecture transcript: '{lecture_transcript}' Answer the following question based on the provided lecture transcript: '{question}'\"\n",
    "\n",
    "# response = model.generate_content([{'mime_type':'image/jpeg', 'data': image}, prompt])\n",
    "\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
