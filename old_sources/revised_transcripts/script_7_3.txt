Okay, let's clean up the transcript into clear sentences.

Layer Norms, with their gamma and beta training parameters, may eventually create outputs that are not unit Gaussian, but the optimization will determine that. For now, this incorporates the layer norms. Let's train them. After running, we see a loss of 2.06, which is better than the previous 2.08, a slight improvement by adding layer norms. I expect they would help even more with a bigger and deeper network. I also forgot to add a layer norm at the end of the Transformer, right before the final linear layer that decodes into the vocabulary. I've added that as well. At this stage, we have a pretty complete Transformer according to the original paper. It’s a decoder-only Transformer, which I’ll explain shortly. The major pieces are now in place. We can try to scale this up and see how well we can push this number.

To scale the model, I made some cosmetic changes. I introduced the variable `n_layer` to specify how many layers of blocks we'll have, created a bunch of blocks, and also have a new variable for `number_of_heads`. I pulled out the layer norm. This part is now identical. I also added dropout. Dropout can be added right before the residual connection, before the connection back into the residual pathway, after the multi-headed attention, and after the softmax when calculating affinities. This randomly prevents some nodes from communicating. Dropout, from a 2014 paper, randomly shuts off a subset of neurons during each forward and backward pass and trains without them. Because the mask of what's being dropped out changes every forward and backward pass, it trains an ensemble of sub-networks. At test time, everything is enabled, merging all sub-networks into a single ensemble. I recommend reading the paper for full details. For now, it's a regularization technique, added because I'm about to scale up the model and was concerned about overfitting.

Looking at the hyperparameters, the batch size is now 64, and the block size is now 256, previously eight. So, we have 256 characters of context to predict the 257th character. The learning rate was brought down a bit, due to the now much larger neural network. The embedding dimension is 384, and there are six heads. Thus, each head is 64-dimensional. There will be six layers and the dropout rate is 0.2, so 20% of intermediate calculations are disabled each pass. I already trained this, with a validation loss of 1.48, which is a significant improvement from 2.07. This ran for about 15 minutes on my A100 GPU. This would not be reproducible on a CPU or MacBook, you would need to break down the number of layers and the embedding dimension. With that time we got this result.

I printed some Shakespeare, but then printed 10,000 characters to a file. The output looks more like the input text file, with someone speaking in a certain manner. The predictions take on that form, though nonsensical when read. It's a Transformer trained on a character level for 1 million Shakespeare characters. It blabbers in a Shakespeare-like manner, but doesn't make sense at this scale. I think it's still a good demonstration.

That concludes the programming section of this video. We did a good job implementing the Transformer, but the picture doesn't exactly match what we've done.  We implemented a decoder-only Transformer, with no encoder and no cross-attention block. Our block has only self-attention and the feed-forward network. We are missing the cross-attention piece. We have a decoder only because we are just generating text, unconditioned on anything. We're blabbering according to a given data set.  The triangular mask makes it a decoder, allowing us to sample from it. It can be used for language modeling. The original paper had an encoder-decoder architecture because it is a machine translation paper.  It takes tokens that encode French, for example, and decodes the translation in English. Typically, there are special tokens at the beginning. We are expected to condition on the input and start generation with a special “start” token, followed by the output, and a special “end” token to finish the generation. The decoding part is the same as what we did. However, they condition the generation on the French sentence. The encoder reads the French part, creates tokens, and puts a Transformer on it, but with no triangular mask. All tokens can talk to each other. It encodes the content of the French sentence. The outputs feed into the decoder, which does the language modeling. There's an additional connection through a cross-attention block. The queries are from X, and the keys and values are from the encoder, the keys and values feed in on the side into every single block of the decoder. The decoder is conditioned on the fully encoded French prompt. This is why we have the two Transformers, with the additional block, and why we did not do this, because we have no conditioning information. We only have a text file and want to imitate it; that's why we are using a decoder-only Transformer as is done in GPT.

I want to briefly walk through nanogPT, found on my GitHub. There are two files of interest: `train.py` and `model.py`. `train.py` is all the boilerplate code for training, the training loop from before, just more complicated with saving, loading, checkpoints, pre-trained weights, decaying learning rates, compiling the model, and using distributed training. The model.py should look very familiar. The model is almost identical. We have the causal self-attention block, which should be very recognizable. We produce queries, keys, and values, do dot products, masking, apply softmax, optionally dropout, and pull the values. In my code, the multi-headed attention was separated into individual heads, then I concatenate them. Here, it's implemented in a batched manner inside a single causal self-attention block, with a fourth "heads" dimension. It is mathematically equivalent but more efficient. We also have the multi-layer perceptron, using the GeLU nonlinearity, because OpenAI uses it and I want to be able to load their checkpoints. The blocks of the Transformer are identical, and the GPT will be as well. We have position encodings, token encodings, blocks, the layer norm at the end, and the final linear layer. This should look very recognizable. There is more here, because I'm loading checkpoints and separating parameters into those that should be weight-decayed and those that shouldn't. The generate function should be very similar. Some details differ, but you should be able to understand it.

Now let's bring things back to ChatGPT. To train it yourself, there are roughly two stages: pre-training and fine-tuning. In pre-training, you train on a large chunk of internet to get the first decoder-only Transformer to babble text, very similar to what we've done, but we've done a tiny baby pre-training. The Transformer I created had about 10 million parameters, trained on 1 million characters. Opening AI’s vocabulary is different; they use sub-word chunks of words with roughly 50,000 vocabulary elements. Their sequences are more condensed, so the Shakespeare data set would be around 300,000 tokens. We trained a 10-million-parameter model on roughly 300,000 tokens. In the GPT-3 paper, the largest Transformer has 175 billion parameters. Their number of layers, embedding dimension, heads, head size, batch size, and learning rate are shown here, while ours had a batch size of 65. They trained on 300 billion tokens, whereas ours was about 300,000, about a millionfold increase. This number isn't even that large by today's standards; they would be going up 1 trillion and above. They are training a significantly larger model on the internet. The architecture is almost identical to what we have, but it's a massive infrastructure challenge to train that, typically with thousands of GPUs.

After pre-training, you get a document completer, not an assistant. It babbles internet, creating arbitrary news articles, because it's trained to complete the sequence. It might answer questions with more questions, ignore your questions, or try to complete a news article, completely unaligned. The second, fine-tuning stage is to align it to be an assistant. This ChatGPT blog post talks about how this stage is achieved. The first step is to collect training data that looks like what an assistant would do, documents with questions at the top and answers below, likely in the thousands of examples, but not on the scale of the internet. You fine-tune the model to focus on documents that look like that, so it expects a question and then completes the answer. The models are sample-efficient during fine-tuning. The second step is to let the model respond and have raters rank the responses. This is used to train a reward model, which predicts how desirable any response would be. Then, they run policy gradient reinforcement learning to fine-tune the sampling policy, so the answers score a high reward according to the reward model. This whole aligning stage takes it from a document completer to a question answerer. This data is not available publicly, it's internal to OpenAI. This stage is harder to replicate. NanogPT focuses on the pre-training stage.

To summarize, we trained a decoder-only Transformer, a GPT, on tiny Shakespeare and got sensible results, with about 200 lines of code. I will release the code base with all git log commits and a Google Colab notebook. Hopefully, it gives you a sense of how to train models like GPT-3. The architecture is identical, but they are 10,000 to 1 million times bigger. We didn't talk about fine-tuning stages for tasks or alignment, or sentiment detection.  That requires further fine-tuning, either simple supervised learning or more complex reinforcement learning, like that used in ChatGPT to train the reward model and use policy gradient methods. We are now at the two-hour mark, so I will end it here.  I hope you enjoyed the lecture. Go forth and transform!
