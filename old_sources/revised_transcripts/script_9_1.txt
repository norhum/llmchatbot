Here is the cleaned-up transcript:

We are forwarding the position embeddings and the token embeddings. This code should be recognizable from the previous lecture. We use a range, which is similar to the range function but for PyTorch. We iterate from zero to T, creating these position indices. We ensure they are on the same device as the input indices because we will be training on a GPU, not just a CPU. This is more efficient. The position embeddings and the token embeddings are added together. The position embeddings are identical for every row of input, so there's broadcasting happening in this addition. We create an additional dimension so these two can add up because the same position embeddings apply to every row of our examples in a batch. We forward the Transformer blocks, and then the last layer normalization and the LM head. What comes out of the forward pass are the logits. If the input was B by T indices, then at every position, B by T, we calculate the logits for what token comes next in the sequence. So, what is the token at B, T+1, the one to the right of this token? B times vocab size is the number of possible tokens. This is the tensor we will obtain, and these logits are a softmax away from becoming probabilities. This is the forward pass of the network, and we can generate from the model.

Now we're going to set up the identical thing on the left here that matches Hugging Face on the right. We sampled from the pipeline five times, up to 30 tokens, with the prefix "hello I'm a language model," and these are the completions we achieved. We'll replicate that on the left. The number of sequences is five, and the max length is 30. First, we initialize our model, and put it into evaluation mode. This is good practice to put the model into eval mode when you're not going to be training it. I’m not sure if this is doing anything right now because our model contains no modules or layers that have different behavior at training or evaluation time. For example, dropout, batch norm, and other layers have this behavior, but the layers we've used should be identical in both training and evaluation. So, `model.eval()` might not do anything, but maybe PyTorch internals do some clever things depending on the evaluation mode. Next, we're moving the entire model to CUDA, so all the tensors go to the GPU. I am on a cloud box with several GPUs. Here, I am moving the entire model, its members, its tensors; everything is shipped off to a separate computer, the GPU, which is connected to the CPU. It's really well suited to parallel processing tasks like running neural networks. Doing this makes our code a lot more efficient, because all of this runs faster on GPUs. So, the model lives on the GPU, a separate computer.

Next, we want to use "hello I'm a language model" as the prefix when we generate. Let's create those prefix tokens. We import the Tiktoken library from OpenAI and get the GPT-2 encoding, which is the tokenizer for GPT-2. Then we encode this string and get a list of integers, which are the tokens. We can copy and paste the string and inspect it in Tiktoken. These are the tokens that will come out, and, as you may recall, all tokens are just little string chunks. This is the chunking of the string into GPT-2 tokens. Once we have the tokens as a list of integers, we create a PyTorch tensor out of it. In this case, there are eight tokens. Then we replicate these eight tokens five times to get five rows of eight tokens. This is our initial input X, and it also lives on the GPU. X is now the input that we can use in the forward pass to get our logits.

We are ready to generate. In this code block, we have X, which is of size B by T, so batch by time. In every iteration of this loop, we will add a column of new indices into each of these rows. These are the new indices, and we are appending them to the sequence. With each loop iteration, we get one more column into X. All of the operations happen in the context manager of `torch.no_grad()`. This tells PyTorch that we are not going to call backward, so it doesn’t have to cache the intermediate tensors or prepare for a potential backward pass. This saves space and time. We get our logits. Then, we get the logits at only the last location. We throw away the other logits because we only care about the last column. This is wasteful and an inefficient implementation of sampling, but it's correct. We pass the last column of logits through softmax to get our probabilities. Then I am doing top-k sampling of 50, which is the Hugging Face default. Looking at the Hugging Face documentation, there are a bunch of arguments that go into the pipeline, but the important one is that they use top-k, which is 50, by default. This takes our probabilities, and only keeps the top 50. Anything lower than the 50th probability is clamped to zero and renormalized. We are never sampling rare tokens; we are only sampling the top 50 most likely tokens. This helps keep the model on track and prevents it from getting lost or going off the rails as easily. It sticks to likely tokens.

This is how to do it in PyTorch. Roughly speaking, we get a new column of tokens, we append them to X, and the columns of X grow until the loop terminates. Finally, we have an entire X of size 5 by 30 in this example, and we can print all of the individual rows. I am getting all the rows, all the tokens that were sampled, and using the decode function from Tiktoken to get the string, which we can print. These are the generations that we're getting. The model will just keep going. These generations are not the same as the Hugging Face generations. I cannot find the discrepancy, and I didn't go through all these options, but there's probably something else, in addition to the top-k, that's different. I am not able to match them up. For correctness, I replicated the code below in the Jupyter notebook using the Hugging Face model, and I am getting the same results, so the model internals are correct. I'm just not 100% sure what the pipeline does in Hugging Face. Otherwise, the code is correct, we loaded the tensors correctly, and we initialized the model correctly. This works. We've ported all the weights, initialized the GPT-2 model, and it can generate sensible sequences. Here, we are initializing with GPT-2 model weights. Now, we want to initialize from scratch using random numbers and train a model that gives us sequences as good as, or better than, these ones in quality. We'll turn to that next.

Using a random model is fairly straightforward because PyTorch initializes our model randomly by default. When we create the GPT model in the constructor, these layers and modules have random initializers that are there by default. When the linear layers are created, they use default constructors, like the Xavier initialization, to construct the weights of these layers. Creating a random model instead of a GPT-2 model is fairly straightforward. Instead, we would create `model = GPT()` with the default `GPTConfig`, and the default config uses 124 million parameters. This is the random model initialization, and we can run it. The results will be garbage because this is a random model, and we are getting random token string pieces chunked up randomly.

One more thing I want to point out is that if you do not have CUDA available because you do not have a GPU, you can still follow along with what we're doing, to some extent. Maybe not to the very end, because eventually we will be using multiple GPUs, but for now you can follow along. In PyTorch, I like to auto-detect the device that is available. By default, we start with the CPU, which is available everywhere. Then we try to detect if a GPU exists, by trying to use CUDA. If not, we check for MPS, the backend for Apple Silicon. If you have a fairly new MacBook, you probably have Apple silicon on the inside, which has a capable GPU. You can use MPS, which will be potentially faster than the CPU. We can print the device. Once we have the device, we can use it in place of CUDA. When we call model on X, if X is on the CPU instead of the GPU, it will work fine because in the forward pass, when we create the position indices, we are careful to use the device of the input indices. There will not be a mismatch where one tensor is on the CPU and one is on the GPU. We are carefully initializing on the correct device. This will auto-detect the device. It will be the GPU, or CUDA. You can also run with another device. It will not be too much slower. If I override `device=CPU`, then we're using the CPU. We're not using torch compile, which will speed things up. You can follow along even on a CPU.

I want to loop around eventually into what it means to have different devices in PyTorch, what PyTorch does in the background when you do something like `module.to(device)` or `torch.tensor.to(device)`, but for now, I'd like to get to training. Let's say that the device makes code go fast. Let's go into how we can train the model. To train the model, we will need a dataset. The best debugging, simplest dataset that I like to use is the tiny Shakespeare dataset. It is available at this URL. You can use Wget, or you can just search for "tiny Shakespeare dataset". I have it as `input.txt`. I downloaded it. Here, I am reading the dataset, getting the first 1000 characters, and printing the first 100. GPT-2 has roughly a compression ratio of three to one with its tokenizer, so 1000 characters is roughly 300 tokens. This is the first few characters. If you want more statistics, we can use `wc input.txt`. This is 40,000 lines, about 200,000 words, and about 1 million bytes. Because this file is only ASCII characters, every ASCII character is encoded with one byte. So, there are roughly 1 million characters. That's the dataset size, small for debugging.

To tokenize the dataset, we'll get the Tiktoken encoding for GPT-2, encode the data, the first 1000 characters, and then print the first 24 tokens. These are the tokens as a list of integers. If you can read GPT-2 tokens, you will recognize 198 as the slashing character, which is a new line. Here we have two new lines, so that's 198 twice. This is the tokenization of the first 24 tokens. We want to process these token sequences and feed them into a Transformer. We want to rearrange these tokens into this input variable, or `idx`. We do not want a single long, one-dimensional sequence. We want an entire batch where each sequence is up to T tokens, and T is not larger than the maximum sequence length. We want these T-long sequences of tokens, and B independent examples of sequences. How can we create a B by T tensor to feed into the forward pass out of these one-dimensional sequences?

My favorite way to do this is to create a tensor from the list of integers. Take the first 24 tokens. Then do a `.view()` of 4x6, which multiplies to 24. It is a two-dimensional rearrangement of these tokens. When you view this one-dimensional sequence as two-dimensional, the first six tokens end up as the first row, the next six tokens are the second row, and so on. It will stack up every six tokens as independent rows, creating a batch of tokens. For example, if token 25 is in the Transformer, and this becomes the input, this token will see these three tokens and predict that 198 comes next. In this way, we can create this two-dimensional batch. This is quite nice. How do we get the labels for the target to calculate the loss? We could write some code inside the forward pass, because we know the next token in the sequence is the label, which is just to the right of us. For the token at the very end, 13, we don't have the next correct token, because we didn't load it.

I'll show you how I get these batches. I like to have the input to the Transformer, which I like to call X, and I also like to create the `targets` tensor, which is the same size as X but contains the targets at every single position. Here's how I like to do this. I fetch one additional token because we need the ground truth for the last token. When creating the input, we take everything up to the last token and view it as 4x6. When creating the targets, we use the same buffer but starting at index one, not index zero. We skip the first element and view it in the exact same size. When I print this, we see that for token 25, its target was 198, and this is now stored in the exact same position in the targets tensor. Also, the last token, 13, now has its label, which is 198, because we loaded the plus one token. We take long sequences, view them in two dimensions to get batch of time, and we load one additional token so that we have a buffer of B by T+1 tokens. Then, we offset things and view them, and we have two tensors. One is the input to the Transformer, and the other is the labels. Let's reorganize the code, and create a simple data loader that loads these tokens and feeds them to the Transformer to calculate the loss.

I reshuffled the code. I am overwriting the device to use a CPU for now. We are importing Tiktoken. This should look familiar. We're loading 1000 characters. I’m setting batch size to four and time to 32. We want to have a single batch that's small. This follows what we did previously. Here, we create the model and get the logits. We have a batch of 4x32, so our logits are now of size 4 by 32 by 50257. Those are the logits for what comes next at every position. We have the labels in Y. Now is the time to calculate the loss, do the backward pass, and the optimization. Let’s first calculate the loss. To calculate the loss, we’re going to adjust the forward function of this `nn.Module` in the model. We will return not only the logits, but also the loss. We'll also pass in the targets in Y, not just the input indices. We will print the loss function and then call `sys.exit(0)` to skip the sampling logic. In the forward function, we now have optional targets. When we get targets, we can calculate the loss. We're going to return logits, the loss, and by default the loss is none. If the target is not none, then we want to calculate the loss. Co-pilot is calculating the correct loss, using cross entropy loss, as documented here. In PyTorch, this is under the `functional` module.

What is happening here? Cross-entropy does not like multidimensional inputs. It can’t take B by T by vocab size tensors. We're flattening out this three-dimensional tensor into two dimensions. The first dimension will be B*T, and the last dimension is the vocab size. We are flattening out the logits to be two dimensional, with B*T examples and the vocab size in terms of the length of each row. We flatten out the targets so they are a single tensor of B*T. This can then pass into cross entropy, and we return the loss. This should work. We should be printing the loss. Here, we printed 11. This is a tensor with a single element, the number 11.

We also want to calculate a reasonable starting point for a randomized network. Our vocabulary size is 50257. At initialization, we would hope that every vocabulary element is getting roughly uniform probability. We're not favoring, at initialization, any token too much. We’re not confidently wrong. We're hoping the probability of any token is roughly one divided by the vocabulary size. We can sanity check the loss. The cross-entropy loss is just the negative log-likelihood. If we take this probability, and take it through the natural logarithm, and then negate it, that is the loss we expect at initialization. We expect something around 10.82. We are seeing something around 11, so it's not way off. That tells me our probability distribution at initialization is roughly diffused, which is a good starting point. Now, we can perform the optimization and tell the network which elements should follow correctly in what order.

We can now do a loss, backward step, calculate the gradients, and do an optimization. Let's do the optimization. We have the loss. Now, we want a for loop. Let's do 50 steps. We’ll create an optimizer object in PyTorch. We are using the Adam optimizer, which is an alternative to stochastic gradient descent (SGD). SGD is simpler, and Adam is a bit more involved. I specifically like the AdamW variation because, in my opinion, it fixes a bug. AdamW is a bug fix of Adam. Looking at the AdamW documentation, it takes a bunch of hyperparameters. It's more complicated than the SGD we looked at before. It updates parameters with the gradient scaled by the learning rate, and keeps buffers around. It keeps two buffers, M and V, which are the first and second moments, like momentum and RMSProp, if you're familiar with them. You don't have to be. It's a normalization that happens on each gradient element, which speeds up the optimization, especially for language models. We will treat it as a black box. It optimizes the objective faster than SGD. Let's create the optimizer object and go through the optimization.

First, we zero the gradients. You have to start with a zero gradient. When you get your loss and do a backward pass, the gradients are added to. It always does a plus equals on whatever the gradients are, so you must set them to zero. This accumulates the gradient from this loss, and we call the step function on the optimizer to update the parameters and decrease the loss. Then, we print the step and loss. `loss.item()` is used here because the loss is a tensor with a single element. `loss.item()` will convert that tensor to a float. This float will live on the CPU. The loss is a tensor with a single element that lives on the GPU. When you call `.item()`, PyTorch will take that one-dimensional tensor, ship it back to the CPU memory, and convert it into a float, which we can print. This is the optimization, and this should just work.

Expected all tensors to be on the same device, but found at least two devices: CUDA zero and CPU. CUDA zero is my first GPU, because I have eight GPUs. When I wrote this code, I introduced a bug. We moved the model to the device, but we did not move the buffer to the device. You can't just do `buff.to(device)`. It is not stateful; it returns a pointer to a new memory on the device. You have to do `buff = buff.to(device)`. This should work. We expect to see a reasonable loss in the beginning, and then we continue optimizing the single batch. We want to see that we can overfit this single batch, and predict all the indices. We started off at roughly 10.82 or 11, and as we optimize this single batch, we are getting a very low loss. The transformer is memorizing this single batch. The learning rate is 3e-4, which is a good default for optimizations at an early debugging stage. This is our simple training loop, and we are overfitting a single batch. This looks good. Next, we don't want to overfit a single batch; we want to do a real optimization. We need to iterate these XY batches and create a little data loader that ensures that we always get a fresh batch. Let's do that next.

This is what I came up with. I wrote a little data loader. We are importing the tokenizer. We're reading the entire text file from `input.txt`, tokenizing it, and printing the number of tokens in total and the number of batches in a single epoch of iterating over this dataset. This is how many unique batches we output before we loop back to the beginning of the document and start reading it again. We start at position zero and walk the document in batches of B*T. We take chunks of B*T, and we advance by B*T. It’s important to note that we advance by B*T, but when we fetch the tokens, we are fetching from the current position to B*T+1. We need that +1 because we need the target token for the last token in the current batch, so we can do the XY exactly as we did before. If we run out of data, we’ll loop back around to zero. This is one way to write a simple data loader that goes through the file in chunks and is good enough for us. We're going to make it more complex later. Now, we would like to use our data loader. The import of Tiktoken is up there. All of this is useless now. Instead, we want a train loader for the training data. We'll use the same hyper-parameters. Batch size is 4, and time is 32. Now we need to get the XY for the current batch.
