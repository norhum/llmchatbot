This is a staging area, and in Python, all these will be local variables to this function, so I am not changing any of the global scope here. Here, `l1` will be `l`. Copying and pasting this expression, we will add a small amount, `h`, to `a`. This will measure the derivative of `l` with respect to `a`. So, here, this will be `l2`, and then we want to print this derivative. Print `l2 - l1`, which is how much `l` changed, and then normalize it by `h`. This is the rise over run. We have to be careful because `l` is a value node, so we actually want its data so these are floats dividing by `h`. This should print the derivative of `l` with respect to `a` because `a` is the one that we bumped a little bit by `h`. The derivative of `l` with respect to `a` is six. If we change `l` by `h`, the derivative here is one. That is the base case of what we are doing here. We cannot come up here and manually set `l.grad` to one; this is our manual backpropagation. `l.grad` is one. Let's redraw, and we'll see that we filled in `grad` as 1 for `l`. We are now going to continue the backpropagation. Let's look at the derivatives of `l` with respect to `d` and `f`. Let’s do `d` first. We have that `l` is `d` times `f`, and we'd like to know what is `dl/dd`. If you know your calculus, `l` is `d` times `f`, so `dl/dd` would be `f`. We can also derive it. We go to the definition of the derivative, which is `f(x+h) - f(x)` divided by `h`, as a limit where `h` goes to zero. When we have `l` is `d` times `f`, increasing `d` by `h` would give us `d+h` times `f`, minus `d` times `f`, and then divide by `h`. Symbolically expanding, we would have `d*f + h*f - d*f` divided by `h`. The `d*f` cancels out, so you’re left with `h*f` divided by `h`, which is `f`. In the limit as `h` goes to zero, we get `f`. Symmetrically, `dl/df` will be `d`. `f.grad` is now just the value of `d`, which is 4. `d.grad` is just the value of `f`, so the value of `f` is negative two. We'll set those manually. Let me erase this markdown node, and let's redraw what we have. We seem to think that `dl/dd` is negative two, so let's double-check. Let me erase the plus `h` from before. Now, we want the derivative with respect to `f`, so when I create `f`, let's do a plus `h` here. This should print the derivative of `l` with respect to `f`. We expect to see four, and this is four, up to floating-point funkiness. `dl/dd` should be `f`, which is negative two, so `grad` is negative two. If we come here and change `d.data += h`, we've added a little `h`, and then we see how `l` changed. We expect to print negative two; there we go. We've numerically verified what we're doing here. This is like an inline gradient check. A gradient check is when we are deriving this backpropagation and getting the derivative with respect to all the intermediate results. Numerical gradient is estimating it using a small step size. Now, we're getting to the crux of backpropagation, so this will be the most important node to understand because if you understand the gradient for this node, you understand all of backpropagation and all of training of neural nets. We need to derive `dl/dc`, in other words, the derivative of `l` with respect to `c` because we've computed all these other gradients already. Now, we're coming here, and we're continuing the backpropagation manually. We want `dl/dc` and then we'll also derive `dl/de`. How do we derive `dl/dc`? We actually know the derivative of `l` with respect to `d`. We know how `l` is affected by `d`, but how is `l` sensitive to `c`? If we wiggle `c`, how does that impact `l` through `d`? We know `dl/dc`, and we also know how `c` impacts `d`. Intuitively, if you know the impact that `c` is having on `d` and the impact that `d` is having on `l`, you should be able to put that information together to figure out how `c` impacts `l`. We can do this. In particular, let's look at how what is the derivative of `d` with respect to `c`, or `dd/dc`. Here, we know that `d` is `c` times `c+e`. We're interested in `dd/dc`. If you know your calculus, differentiating `c+e` with respect to `c` gives you 1.0. We can also go back to the basics and derive this. Again, we can go to `f(x+h) - f(x)` divided by `h` as `h` goes to zero. Focusing on `c` and its effect on `d`, we can do `c + h + e` minus `c + e` divided by `h`. Expanding this out, this will be `c+h+e-c-e` divided by `h`. `c` minus `c` cancels, `e` minus `e` cancels, and we are left with `h/h`, which is 1.0. By symmetry, `dd/de` will also be 1.0. The derivative of a sum expression is simple. This is the local derivative. I call this the local derivative because we have the final output value all the way at the end of this graph, and we're now at a small node here. This is a little plus node, and it doesn't know anything about the rest of the graph. It only knows that it took `c` and `e`, added them, and created `d`. This plus node also knows the local influence of `c` on `d`, or rather, the derivative of `d` with respect to `c`. It also knows the derivative of `d` with respect to `e`, but that's not what we want. That's just a local derivative. What we actually want is `dl/dc`. `l` is one step away, but in a general case, this little plus node could be embedded in a massive graph. We know how `l` impacts `d`, and now we know how `c` and `e` impact `d`. How do we put that information together to write `dl/dc`? The answer is the chain rule in calculus. I pulled up the chain rule from Wikipedia. If a variable `z` depends on a variable `y`, which itself depends on the variable `x`, then `z` depends on `x` through the intermediate variable `y`. The chain rule is expressed as: if you want `dz/dx`, then you take `dz/dy` and multiply it by `dy/dx`. The chain rule tells you how we chain these derivatives together correctly. To differentiate through a function composition, we have to apply a multiplication of those derivatives. The chain rule says that knowing the instantaneous rate of change of `z` with respect to `y` and `y` relative to `x` allows one to calculate the instantaneous rate of change of `z` relative to `x` as a product of those two rates of change. Simply, the product of those two. If a car travels twice as fast as a bicycle, and the bicycle is four times as fast as a walking man, then the car travels two times four, eight times as fast as the man. It is clear that we need to multiply. We can take these intermediate rates of change and multiply them. That justifies the chain rule intuitively. For us, it means there is a simple recipe for deriving what we want, which is `dl/dc`. We know the impact of `d` on `l`, so we know `dl/dd`. That's negative two. Because of this local reasoning, we know `dd/dc`. How does `c` impact `d`? In particular, this is a plus node, so the local derivative is 1.0. The chain rule tells us that `dl/dc`, going through this intermediate variable, will simply be `dl/dd` times `dd/dc`. This is the chain rule. It is identical to what's happening here, except `z` is our `l`, `y` is our `d`, and `x` is our `c`. We multiply these, and because these local derivatives, like `dd/dc` are just one, we copy over `dl/dd` because this is just times one. Because `dl/dd` is negative two, what is `dl/dc`? It's the local gradient 1.0 times `dl/dd`, which is negative two. A plus node routes the gradient because the plus node's local derivatives are one, so in the chain rule, one times `dl/dd` is just `dl/dd`, and that derivative gets routed to both `c` and `e`. `c.grad` is negative two times one, or negative two. By symmetry, `e.grad` will be negative two. Let me set those, and redraw. We assigned negative two. This backpropagating signal is carrying the information of the derivative of `l` with respect to all the intermediate nodes. We can imagine it flowing backward through the graph. A plus node will distribute the derivative to all the children nodes. We have that `c.grad` is negative two. Let’s verify it. Let me remove the plus `h` from before. Instead, we're going to increment `c`, so `c.data` will be incremented by `h`. When I run this, we expect to see negative two. For `e`, `e.data += h`, and we expect to see negative two. Those are the derivatives of these internal nodes. Now, we're going to recurse our way backwards again, and we're going to apply the chain rule. Our second application of the chain rule, we will apply it all the way through the graph. We have that `dl/de`, as we have just calculated, is negative two. We know the derivative of `l` with respect to `e`. Now, we want `dl/da`. The chain rule tells us that that's just `dl/de`, negative two, times the local gradient. What is the local gradient, basically, `de/da`? We have to look at that. I'm a little times node inside a massive graph. I only know that I did `a` times `b` and produced `e`. What is `de/da` and `de/db`? That's all I know. That's my local gradient. Because `e` is `a` times `b`, we're asking what is `de/da`. If you differentiate this with respect to `a`, you will get `b`, which in this case is negative three. `dl/da`, or `a.grad`, applying the chain rule, is `dl/de`, which is negative two, times `de/da`, which is the value of `b`, negative three. Then, `b.grad` is again `dl/de`, which is negative two, times `de/db`, which is the value of `a`, which is two. These are our claimed derivatives. Let's redraw, and we see that `a.grad` is six because that is negative two times negative three, and `b.grad` is negative two times two, which is negative four. Let's verify them. We have `a` here, so `a.data += h`. The claim is that `a.grad` is six. Let's verify. Six. We have `b.data += h`, nudging `b` by `h` and looking at what happens. We claim it's negative four. It's negative four. That was the manual backpropagation all the way from here to all the leaf nodes. We did it piece by piece, and really all we did was iterate through all the nodes one by one and locally applied the chain rule. We always know what is the derivative of `l` with respect to this little output. Then, we look at how this output was produced. This output was produced through some operation. We have the pointers to the children nodes of this operation. In this little operation, we know what the local derivatives are, and we just multiply them onto the derivative. We go through and recursively multiply on the local derivatives. That's what backpropagation is, a recursive application of the chain rule backward through the computation graph. Let's see this power in action. We're going to nudge our inputs to try to make `l` go up. We want `a.data` to change. If we want `l` to go up, that means we just have to go in the direction of the gradient. `a` should increase in the direction of the gradient by some small step amount, this is the step size. We don't just want this for `a` but also for `b`, also for `c`, and also for `f`. Those are leaf nodes, which we usually have control over. If we nudge in the direction of the gradient, we expect a positive influence on `l`. We expect `l` to go up. It should become less negative, should go up to, say, negative six. We’d have to rewrite the forward pass. Let me do that here. This would be the forward pass. `f` would be unchanged. This is effectively the forward pass. Now, if we print `l.data`, we expect, because we nudged all the inputs in the rational gradient, a less negative `l`. We expect it to go up, maybe it’s negative six or so. Okay, negative seven, and this is one step of an optimization that we'll end up running. Gradient gives us some power because we know how to influence the final outcome, and this will be extremely useful for training knowledge. I would like to do one more example of manual backpropagation using a more complex and useful example. We are going to backpropagate through a neuron. We want to eventually build up neural networks. In the simplest case, these are multilayer perceptrons. This is a two-layer neural net, and it's got these hidden layers made up of neurons. These neurons are fully connected to each other. Biologically, neurons are very complicated, but we have simple mathematical models of them. This is a very simple mathematical model of a neuron. You have some inputs `x`, and then you have these synapses that have weights on them. The `w`'s are weights. The synapse interacts with the input to this neuron multiplicatively. What flows to the cell body of this neuron is `w` times `x`. There are multiple inputs, so there are many `w` times `x`'s flowing into the cell body. The cell body also has some bias. This bias can make it more or less trigger-happy regardless of the input. We are taking all the `w` times `x`'s of all the inputs, adding the bias, and then we take it through an activation function. This activation function is usually some kind of a squashing function like a sigmoid or tanh. We are going to use tanh in this example. NumPy has `np.tanh`. We can call it on a range and plot it. This is the tanh function. The inputs as they come in get squashed on the y-coordinate here. Right at zero, we're going to get exactly zero. As you go more positive, the function will only go up to one and plateau out. If you pass in very positive inputs, we're going to cap it smoothly at one. On the negative side, we're going to cap it smoothly to negative one. That's tanh, and that's the squashing or activation function. What comes out of this neuron is just the activation function applied to the dot product of the weights and the inputs. Let's write one out. Here, we have the inputs `x1` and `x2`. This is a two-dimensional neuron, so two inputs are going to come in. These are the weights of this neuron, `w1` and `w2`. These weights are the synaptic strengths for each input. This is the bias of the neuron, `b`. According to this model, we need to multiply `x1` times `w1` and `x2` times `w2`. We need to add bias on top of it. It gets messy, but all we are trying to do is `x1*w1 + x2*w2 + b`. These are multiplications here. I'm doing it in small steps so we have pointers to all these intermediate nodes. We have `x1 * w1` and `x2 * w2`. I'm also labeling them. `n` is the cell body, the raw activation without the activation function. This should be enough to plot it. Draw dot of `n` gives us `x1` times `w1`, `x2` times `w2`, being added, then the bias gets added on top of this, and this `n` is this sum. We're now going to take it through an activation function. Let’s use tanh to produce the output. The output, `o`, is `n.tanh()`. We haven't written the tanh function. The reason we need to implement another tanh function here is that tanh is a hyperbolic function. We've only implemented a plus and times. You can't make a tanh out of pluses and times. You also need exponentiation. Tanh is this formula. You see there's exponentiation involved, which we have not implemented yet for our low value node. We're not going to be able to produce tanh yet. We have to go back up and implement something like it. One option is we could implement exponentiation and return the `x` of a value instead of a `tanh` of a value. If we had `x`, then we have everything else we need. We know how to add and we know how to multiply, so we’d be able to create tanh if we knew how to exponentiate. For the purposes of this example, I wanted to show you that we don't necessarily need to have the most atomic pieces in this value object. We can create functions at arbitrary points of abstraction. They can be complicated functions, but they can also be very simple functions like a plus. The only thing that matters is that we know how to differentiate through any one function. We take some inputs and we make an output. The only thing that matters is if you know how to create the local derivative. If you know the local derivative of how the inputs impact the output, then that's all you need. We're going to cluster up all of this expression, and we're not going to break it down to its atomic pieces. We are going to directly implement tanh. Let's do that. Define `tanh`. `out` will be a value of. We need this expression here. Let me copy-paste. Let's grab `n`, which is `cell.data`, and then this is `math.exp(2*n)` minus one over `math.exp(2*n)` plus one. I can call this `x`, so it matches exactly. Now, this will be `t`, and the children of this node there’s just one child, wrapped in a tuple. The name of this operation will be `tanh`, and we're going to return that. Now, the value should be implementing `tanh`. Now, we can scroll down here and do `n.tanh()`. This will return the tanhd output of `n`. Now, we should be able to draw `o` not `n`. `n` went through `tanh` to produce this output. `tanh` is now our little micrograd-supported node as an operation. As long as we know the derivative of `tanh`, we'll be able to backpropagate through it. Let’s see this `tanh` in action. Currently, it's not squashing too much because the input to it is pretty low. If the bias was increased to, say, eight, we'll see that what's flowing into the `tanh` now is two. The `tanh` is squashing it to 0.96. We are already hitting the tail of this `tanh`. It will smoothly go up to one and plateau out. I'm going to do something slightly strange. I'm going to change this bias from 8 to 6.88, etc. I'm going to do this for specific reasons, because we're about to start backpropagation. I want to make sure that our numbers come out nice. They are not crazy numbers, they're nice numbers that we can understand in our head. Let me add a pose label. `o` is short for output. So, 0.88 flows into `tanh` and comes out 0.7. Now, we're going to do backpropagation. We're going to fill in all the gradients. What is the derivative of `o` with respect to all the inputs here? In a typical neural network setting, what we really care about is the derivative of these neurons on the weights, specifically `w2` and `w1`, because those are the weights that we're going to be changing as part of the optimization. We have only a single neuron, but in neural nets, we have many neurons, and they are connected. This is one small neuron, a piece of a much bigger puzzle. Eventually, there's a loss function that measures the accuracy of the neural net. We are backpropagating with respect to that accuracy and trying to increase it. Let’s start off with backpropagation. What is the derivative of `o` with respect to `o`? The base case is that the gradient is 1.0. Let me fill it in. Let me split out the drawing function here and clear this output. Now, when we draw `o`, we'll see that `o.grad` is one. Now, we're going to backpropagate through the tanh. To backpropagate through tanh, we need to know the local derivative of tanh. If we have that `o` is `tanh(n)`, then what is `do/dn`? You could take this expression and take the derivative, and that would work. We can scroll down Wikipedia to a section that tells us that the derivative of tanh is 1 minus tanh squared. `do/dn` is 1 minus `tanh(n)^2`. We already have `tanh(n)`, that's just `o`. So, it's one minus `o^2`. `o` is the output here. The output is this number, the data is this number, and this is saying that `do/dn` is 1 minus this squared. One minus that data squared is 0.5 conveniently. The local derivative of this tanh operation here is 0.5, and that would be `do/dn`. We can fill in that `n.grad` is 0.5. This is exactly 0.5, one half. Now, we're going to continue the backpropagation. This is 0.5, and this is a plus node. How is backprop going to work here? A plus is just a distributor of gradient. This gradient will simply flow to both of these equally. The local derivative of this operation is one for every one of its nodes. 1 times 0.5 is 0.5. Therefore, this node's grad is 0.5, and `b.grad` is also 0.5. Let's set those and redraw. So, 0.5. Continuing, another plus, 0.5 again will just distribute it. 0.5 will flow to both of these. We can set `x2w2.grad` is 0.5. Pluses are my favorite operations to backpropagate through because it's simple. It is flowing into these expressions is 0.5. Keep in mind what the derivative is telling us at every point. This is saying that if we want the output of this neuron to increase, then the influence on these expressions is positive. Both are positive contributions to the output. Backpropagating to `x2` and `w2` first, this is a times node. The local derivative is the other term. To calculate `x2.grad`, `x2.grad` will be `w2.data` times the `x2w2.grad`, and `w2.grad` will be `x2.data` times `x2w2.grad`. Let's set them and redraw. Here, the gradient on `w2` is 0 because `x2.data` was 0. `x2` will have the gradient 0.5 because `w2.data` was 1. Because the input `x2` was 0, because of the way times works, this gradient will be zero. If I wiggle `w2`, how is the output changing? It's not changing because we're multiplying by zero. Because it's not changing, there's no derivative, and zero is the correct answer. Let's do it here, 0.5 should come here and flow through times. So, we'll have that `x1.grad` is going to be the local derivative of times with respect to `x1`, which is `w1`, so `w1.data` times `x1w1.grad`, and `w1.grad` will be `x1.data` times `x1w1.grad`. Those came out to be -1.5 and 1. We've backpropagated through this expression. These are the actual final derivatives. If we want this neuron's output to increase, we know that `w2` has no gradient, `w2` doesn't actually matter to this neuron, but this weight, `w1`, should go up. If this weight goes up, then this neuron's output would have gone up proportionally because the gradient is one. Doing the backpropagation manually is ridiculous, so we are going to put an end to this suffering. We're going to see how we can implement the backward pass a bit more automatically. We're not going to be doing all of it manually. It's now obvious how these pluses and times are backpropagating gradients, so let's go to the value object. We're going to start codifying what we've seen in the examples. We're going to store a special `cell._backward`, and this will be a function which is going to do that little piece of the chain rule at each node that took inputs and produced output. We're going to store how we are going to chain the outputs gradient into the inputs gradients. By default, this will be a function that doesn't do anything. For a leaf node, there is nothing to do. When we are creating these `out` values, these `out` values are an addition of self and other. We will set `out.backward` to be the function that propagates the gradient. Let's define what should happen when we call `out.grad` in addition. Our job is to take `out.grad` and propagate it into `self.grad` and `other.grad`. We want to set `self.grad` to something, and `other.grad` to something. The way we saw how chain rule works, we want to take the local derivative times the global derivative, which is the derivative of the final output of the expression with respect to out. The local derivative of self in an addition is 1.0, so it's just 1.0 times `out.grad`. `other.grad` will be 1.0 times `out.grad`. `out.grad` will simply be copied onto `self.grad` and `other.grad` as we saw happens for an addition operation. We're going to later call this function to propagate the gradient, having done an addition. Let’s do multiplication. We're going to also define the backward and we're going to set its backward to be `backward`.
