We are converting to float and dividing every iteration of the loop, constantly renormalizing rows, which is extremely inefficient and wasteful. I want to prepare a matrix, capital P, that contains the probabilities. This matrix will be the same as the capital N matrix of counts, but each row will be normalized to 1, representing the probability distribution for the next character given the previous character, based on the row. We will calculate this matrix upfront and then use the corresponding row, `p = p[ix]`, instead of recalculating each time.

I want to do this not just for efficiency, but also to practice with n-dimensional tensors, their manipulation, and broadcasting. We need to become proficient with these tensor operations to build transformers, which involve complex array operations for efficiency.

Intuitively, we want to create a floating-point copy of N, mimicking the existing code. We will then divide all the rows so that they sum to 1. We could try `p = p / p.sum()`, but this sums all the counts in the entire matrix and gives a single number. We need to divide each row by its respective sum. 

We need to consult the documentation for `torch.sum`, where we can specify both the input array and the dimension along which to sum. We want to sum over rows. The `keepdim` argument is also important. If `keepdim` is true, the output tensor retains the same size as the input, except the dimension along which we summed, which becomes one. If `keepdim` is false, the dimension is squeezed out.

We want to use `p.sum(axis=1, keepdim=True)`.  The shape of P is 27 by 27. When we sum across axis zero, we sum across the zeroth dimension, resulting in a shape of 1 by 27. With `keepdim=True`, we get a row vector. If `keepdim` is false, it would produce a one-dimensional vector of size 27. However, summing across the first dimension, or axis 1, will give us sums along the rows. This will result in a 27 by 1 column vector.

The sums along the columns and rows happen to be identical in this case due to the nature of bigram statistics. However, in this case we want to sum across the rows, resulting in `p.sum(1, keepdim=True)`, a 27 by 1 column vector. Now, we need to divide P by this column vector. We need to know if a 27 by 27 array can be divided by a 27 by 1 array. This is determined by broadcasting rules. 

According to broadcasting semantics in PyTorch, both tensors need to have at least one dimension. When iterating over dimension sizes from right to left, the sizes must be equal, one of them must be one, or one of them does not exist. Our arrays have shapes 27x27 and 27x1. Aligning them, and iterating from right to left, we see that the last dimension is 27 and 1, which satisfies the one dimension being 1 rule. The next dimension is 27 in both arrays so they are equal. Thus the operation is broadcastable. PyTorch takes the dimension of size one in the 27 by 1 array and copies it 27 times to create a 27 by 27 array. Then, it does an element-wise division, which normalizes each row.

We can verify this by taking the sum of the first row, which should equal one.  Without normalization, it does not sum to one, but with normalization it does.

I encourage you to thoroughly read the broadcasting semantics and to not treat them casually. You must really understand them and look up tutorials and practice using them, or you can quickly run into bugs.

If we have `p.sum(1, keepdim=True)` the shape is 27 by 1. This gives us a column vector of all the row sums. If we take out `keepdim=True`, the shape of the sums becomes 27, and the dimension is squeezed out.  You might expect the division to work, but it produces garbage. Keepdim is making it work. 

The issue arises because, internally, the 27 vector becomes a 1 by 27 row vector due to broadcasting rules.  When we divide a 27x27 matrix by the 1x27 vector, the 1x27 vector is replicated vertically 27 times. Thus we are normalizing columns instead of rows.

The issue comes from the silent addition of a dimension due to broadcasting rules. We are still calculating the row sums correctly, but because `keepdim` is false, broadcasting transforms this vector into a row vector, which then leads to incorrect division. We need to use `keepdim=True` to ensure that we obtain the correct result with row-wise normalization. 

Additionally, we don't want to create a new tensor on each operation, so we use the in-place division operator, `/=`, if possible. This can be faster and avoid unnecessary memory allocation. We trained a bigram language model by counting the frequency of each pairing and normalizing them. The elements of matrix P are the parameters of the bigram model that summarize the bigram statistics. We can sample from this model by iteratively sampling and feeding the next character to get subsequent characters. 

We want to evaluate the model's quality and summarize it with a single number. We evaluate the training loss which can be done using a loss function. We'll copy code used to create diagrams, and we will calculate their probabilities. We'll print character one and two, and their probabilities, truncated to four decimal places. We are examining the probability that the model assigns to every bigram in our training set. 

If all characters are equally likely, then each probability would be roughly four percent, since there are 27 possible characters. Anything above four percent indicates that the model has learned something. We see that some probabilities are as high as 40 percent. A very good model should have probabilities near one, especially on the training set. 

We need to summarize these probabilities with a single number.  In maximum likelihood estimation, we typically use the likelihood, which is the product of all the probabilities. The product of all probabilities represents the probability of the whole dataset according to our model, and that is a measure of the model quality. The higher this product is, the better the model is. Since these probabilities are between zero and one, the product will be very tiny. For convenience, people use the log likelihood instead. 

The log of a probability is a monotonic transformation, where one maps to zero, and then as the probability goes lower, the log becomes more negative. When we plug in higher numbers, we get close to zero, and we get negative numbers for lower probabilities. The log likelihood is really the sum of the logs of the probabilities. The log likelihood starts at zero, and we can accumulate the log probabilities.  The log likelihood can go as high as zero, so the higher, the better.

We need to invert this so that low is good, creating what is known as the negative log likelihood. The negative log likelihood is the negative of the log likelihood, and it is a valid loss function. Low means good, and high means bad, which is how the loss function works. People often use the average negative log likelihood, which is obtained by normalizing by the count.  Our average negative log likelihood for this model on the training set is 2.4. The lower the number, the better the model is.

Our goal is to maximize the likelihood, which is the product of the probabilities assigned by the model, with respect to the model parameters. In our case, these parameters are the probabilities stored in the matrix P.  We are storing probabilities in a table, but later, these numbers will be calculated by a neural network.  We want to tune the neural network's parameters to maximize the likelihood. Maximizing the likelihood is equivalent to maximizing the log likelihood, because log is a monotonic function, and maximizing the log likelihood is equivalent to minimizing the negative log likelihood. This gives us a single number that summarizes the quality of the model.

Let's calculate the negative log likelihood over the whole training set, which should be about 2.45. We can evaluate the probability of any word, such as "Andre."  We can see that the word Andre has a probability which makes its average log probability roughly three. If we test "Andre" followed by "q", the model gives it a negative infinity log probability, which is because it has a zero percent chance according to our bigram model. This is because "jq" had a zero count in our model.  This is not ideal, and one common fix is model smoothing, which involves adding fake counts.  We add one count to everything. This changes the probabilities and ensures there are no zeros, so we won't get infinite loss.

By adding a count of one, we've smoothed out the model a bit, so nothing will be infinitely unlikely. We see that "jq" now has a small probability, which is not ideal, but it avoids an infinite loss. 

We've trained a bigram character-level language model by counting bigrams and normalizing, and we know how to sample from the model by iteratively generating characters. We can also evaluate the model using negative log likelihood.

Now, we will take a different approach, casting the bigram character-level language modeling problem into a neural network framework. We will end up in a very similar spot, but with a very different approach. The neural network will still be a bigram character-level language model, which will receive a character as an input and then output a probability distribution over the next characters, which is based on parameters within the model. We can evaluate the parameters of the neural network using the negative log likelihood loss function. We will use the next character in the bigram as the label, and we can adjust weights to maximize this probability. We will use gradient-based optimization to tune the weights of the neural network. 

We will first compile a training set of the bigrams. We will iterate over the bigrams and create lists for the inputs and the targets, which are the first and second characters of each bigram. Both will be represented as integers. We'll append each character to a respective list, and we will then create tensors out of these lists. For example, using only the word "emma," we will have five input/target pairs, and those pairs are summarized here. When the input to the neural net is 0, we want the output to have high probability on the integer 5 (e). When the input is 5, we want the model to place high probability on 13 (m). There are five different input examples in our data set here. 

You need to be careful about the APIs of these frameworks. I silently used `torch.tensor` with a lowercase 't', but there is also a `torch.Tensor` class with a capital 'T'. Both will output tensors, and you can call both, but it can be confusing. It turns out that `torch.tensor` with a lowercase 't' infers the data type, while `torch.Tensor` with a capital 'T' just returns a float tensor. I recommend using `torch.tensor` with a lowercase 't'.  The lowercase tensor constructs a tensor without autograd history and infers the data type. The capital tensor returns float32. You should get used to reading the documentation and lots of Q&As to understand why and how these things work.
