We chose not to use a pad ID, so there isn't one here. These are individual byte tokens. We saw that byte fallback in Llama was turned on, so it's true. What follows are the 256 byte tokens and their IDs. After the byte tokens come the merges. These are the parent nodes in the merges, so we're not seeing the children, just the parents and their ID. After the merges come the individual tokens and their IDs. These are the individual code point tokens. This is the ordering in which SentencePiece represents its vocabularies: special tokens, then byte tokens, then merge tokens, and then individual code point tokens. All these raw code point tokens are ones encountered in the training set, the entire set of code points that occurred here. These all get put in there. Those that are extremely rare, as determined by character coverage, are ignored. If a code point occurred only a single time out of a million sentences, it would not be added to our vocabulary. Once we have a vocabulary, we can encode into IDs. We can get a list. I am also decoding the individual tokens back into little pieces. Let's look at what happened here. "Hello space on" gives these token IDs. A few things jump out. First, the Korean characters were not part of the training set. SentencePiece is encountering code points that it has not seen during training. Those code points do not have a token associated with them. These are unknown tokens. Because byte fallback is true, SentencePiece falls back to bytes. It encodes the characters with UTF-8 and uses byte tokens to represent those bytes. That's what we are getting here, this UTF-8 encoding shifted by three because of the special tokens that have IDs earlier on. With respect to the byte fallback, let me remove it. If this is false and we retrain, the first thing that happened is that all the byte tokens disappeared. Now we just have the merges and a lot more merges, because we have more space, since we're not taking up space in the vocab size with all the bytes. If we encode this, we get a zero, because this entire string is unknown. Unknown is an "unk" token, so it's zero because the "unk" token is token zero. This feeds into your language model. What is a language model supposed to do when all unrecognized things, because they're rare, just map to "unk"? That's not the property that we want. Llama correctly used byte fallback as true, because we want to feed these unknown or rare code points into the model in some manner. When decoding all the individual tokens, notice how spaces end up being this bold underline. I'm not sure why SentencePiece switches white space into these bold underscore characters; maybe it's for visualization. Notice why there's an extra space in the front of "hello". It's coming from the option "add dummy prefix is true." The documentation states this adds white space at the beginning of the text to treat "world" in isolation and "hello world" the same way. If we go back to the TikToken tokenizer, "world" as a token by itself has a different ID than "space world." Those are two different tokens for the language model. The language model has to learn from data that they are actually a very similar concept. In the TikToken world, words at the beginning of sentences and words in the middle of sentences look completely different. It has to learn they are roughly the same. Add dummy prefix is trying to fight that. It adds a dummy prefix. As part of preprocessing, it will take the string and add a space. It does this in an effort to make "world" and "space world" the same; they will both be "space world". That is another preprocessing option that is turned on, and Llama 2 also uses this option. That's everything for this preview of SentencePiece and how it is different. What I've done here is put in the raw protocol buffer representation of the tokenizer. Feel free to step through this. If you would like your tokenization to look identical to that of Meta's Llama 2, you would copy these settings. My summary for SentencePiece is that there's a lot of historical baggage, concepts that are confusing, and potentially contain foot guns, like the concept of a sentence and its maximum length. It is commonly used because it is efficient and can do both training and inference. It has quirks, such as the "unk" token must exist, and the way byte fallbacks are done. I don't find it particularly elegant, and it's not very well documented. It took me a lot of time working with this, visualizing things, and trying to understand what is happening, because the documentation is not super amazing. It is a very nice repository if you'd like to train your own tokenizer. Let's revisit how we should set the vocab size and some considerations around it. This was the file where we defined the Transformer model. Let's look specifically at vocab size and where it appears in this file. We defined the vocab size, which was a small number. Vocab size doesn't come up too much in most of these layers. The only place it comes up is in these two places: the token embedding table and the LM head layer. The token embedding table is a two-dimensional array where vocab size is the number of rows, and each vocabulary element has a vector that we're going to train. That vector is of size "embed," which is the number of channels. As vocab size increases, this embedding table will grow, adding rows. At the end of the Transformer, there is the LM head layer, which is a linear layer. That layer is used to produce the logits, which become the probabilities for the next token in sequence. We're trying to produce a probability for every single token that might come next. If we have more tokens, we need to produce more probabilities. Every single token introduces an additional dot product in this linear layer. Why can't vocab size be infinite? The token embedding table and the linear layer will grow. We'll be doing a lot more computation because the LM head layer will become more computationally expensive. Because we have more parameters, we could be worried that we are going to be under-training some of these parameters. If you have a very large vocabulary size, say a million tokens, every one of these tokens is going to come up more and more rarely in the training data, and we're going to be seeing fewer and fewer examples for each individual token. The vectors associated with each token will be under-trained because they don't come up often. As your vocab size grows, you're going to start shrinking your sequences, and we will be attending to more text. You might be worrying that large chunks are being squished into single tokens, and the model doesn't have as much time to think per some number of characters in the text. We're squishing too much information into a single token, and the forward pass of the Transformer is not enough to process that information appropriately. These are some considerations when you are designing the vocab size. It's an empirical hyperparameter. It seems that, in state-of-the-art architectures, this is usually in the high ten thousands or around 100,000 today. What if we want to take a pre-trained model and extend the vocab size? It is done fairly commonly. When you're doing fine-tuning, more new special tokens get introduced on top of the base model to maintain the metadata and the structure of conversation objects between a user and an assistant. You might also throw in more special tokens for using the browser or any other tool. It's tempting to add tokens for all kinds of special functionality. If you want to add a token, you have to resize the embedding table, adding rows. You initialize the parameters from scratch to be small random numbers. Then, you extend the weights inside the linear layer to calculate the probabilities for these new tokens. Both of these are resizing operations, a very mild model surgery, and can be done easily. It's common that you would freeze the base model, introduce these new parameters, and only train them to introduce new tokens. You can freeze arbitrary parts of it or train arbitrary parts of it. Minor surgery is required if you'd like to introduce new tokens. There's an entire design space of applications in terms of introducing new tokens into a vocabulary that go way beyond adding special tokens and new functionality. Here is an example from a paper on learning to compress prompts with gist tokens. Suppose that you're using language models in a setting that requires very long prompts. These slow everything down. Instead, they introduce new tokens. Imagine having a few new tokens. You put them in a sequence and then you train the model by distillation. You're keeping the entire model frozen and only training the representations of the new tokens and optimizing over them such that the behavior of the language model is identical to the model that has a very long prompt. It's a compression technique of compressing that very long prompt into those few new gist tokens. At test time, you can discard your old prompt and swap in those tokens. It has an almost identical performance. This is a class of parameter efficient fine-tuning techniques where most of the model is fixed, and you are training token embeddings. There's a whole design space here that is potentially worth exploring. There's a lot of momentum in how you construct Transformers that can process not just text as input but other modalities like images, videos, and audio. You can feed in all these modalities and potentially predict them from a Transformer. You don't have to change the architecture. You stick with the Transformer; you just tokenize your input domains, pretend it's text tokens, and do everything else in the same manner. You can take an image and chunk it into integers that become the tokens. These tokens can be hard tokens where you force them to be integers. They can also be soft tokens where you don't require them to be discrete but force them to go through bottlenecks like in autoencoders. In the paper from OpenAI on SORA, they talk about how LLMs have text tokens, and SORA has visual patches. They came up with a way to chunk videos into tokens with their own vocabularies. You can either process discrete tokens with auto-regressive models or soft tokens with diffusion models. All of that is being actively worked on. Let's loop back to the beginning and see why some things happen. First, why can't my LLM spell words well or do other spell-related tasks? These characters are chunked into tokens, and some of these tokens are fairly long. For example, "default style" is a single token. That's a lot of characters for a single token. The model might not be very good at tasks related to spelling. I asked how many L's there are in "default style" and it thought there were three, but there are actually four. I asked GPT-4 to reverse the string "default style," and it gave me a jumble. It doesn't really know how to reverse this string from right to left. I tried a different approach. I said, "Step one, just print out every single character separated by spaces. Step two, reverse that list." It produced all the characters correctly, and then it reversed them. Somehow, it can't reverse it directly, but it can when it's broken up this way. Now, this is much easier for it to see these individual tokens and reverse them. Why are LLMs worse at non-English languages? The language model sees less non-English data during training, and the tokenizer is not sufficiently trained on non-English data. "Hello how are you" is five tokens, and its translation is 15 tokens. It's a three-times blow-up. "Annyeong" is hello in Korean, and it ends up being three tokens. Everything is more bloated and diffuse. That's partly why the model works worse on other languages. Why is the LM bad at simple arithmetic? That has to do with the tokenization of numbers. Addition has an algorithm that is character-level. These numbers are represented arbitrarily based on whatever happened to merge during the tokenization process. There is an entire blog post about this: "Integer tokenization is insane." It explores the tokenization of numbers, and it noticed that for four-digit numbers, it can be a single token, two tokens, or some combination of tokens. All the different numbers have different combinations. The model sees four digits sometimes as one token, sometimes as two, sometimes as three. This is a headwind for the language model, but it's also kind of not ideal. When Meta trained Llama 2, they made sure to split up all the digits to improve simple arithmetic. Why is GPT-2 not as good in Python? This is partially a modeling issue, but also partially tokenization. The encoding efficiency of the tokenizer for handling spaces in Python is terrible. Every single space is an individual token. This dramatically reduces the context length the model can attend to. It's almost like a tokenization bug that was later fixed with GPT-4. My LLM abruptly halts when it sees the string "end of text". I told GPT-4 to print the string "end of text" and it couldn't do it. Something is breaking here with respect to the handling of the special token. I don't know what OpenAI is doing under the hood and whether they are parsing this as an actual token instead of individual pieces of it without special token handling logic. If someone passes "end of text" as a special character in the user prompt, that would be an attacker-controlled text. You would hope that they don't really parse or use special tokens from that kind of input. Your knowledge of these special tokens ends up being an attack surface. If you'd like to confuse LLMs, just give them special tokens and see if you are breaking something. Here's a fun one: the trailing white space issue. If you come to the playground and go to GPT-3.5, it does completion and will continue the token sequence. If I put a space at the end, I get a warning that text ends in a trailing space, which causes worse performance. Here's what's happening: When I have "here's a tagline for an ice cream shop" and the model generates the next token, it can sample the "space o" token because it has been trained on such sequences. However, if I add a space, when I encode this string, the space at the very end becomes a token by itself and it is not seen in such a context in the training set. The space is part of the next token, but we're putting it here like this. This is out of distribution, and now arbitrary bad things happen. It's a rare example for the model to see something like that. The fundamental issue is that the LLM is trained on these tokens, not characters. Here's a "default cell sta" example. I bet the model has never seen this without "le" because it's seen "default style" as a single token. If I take "default cell sta" and try to complete from it, the model gives me an error. It says the model predicted a completion that begins with a stop sequence. The model immediately emitted an end-of-text token. We're off the data distribution, and the model is predicting arbitrary things. I tried it again, and it completed, but the request violated usage policies. The model is extremely unhappy with just this and doesn't know how to complete it because it's never occurred in the training set that way. It always appears as one token. These kinds of issues, where tokens are either completed with the first character of the next token, or you have long tokens that are then missing some of the characters, are issues with partial tokens. If you dig into the tiktoken repository, you'll see a lot of special case handling of unstable tokens. These unstable tokens are not documented anywhere. What you would like from a completion API is something fancier, but it's not the way it currently is set up. My favorite one by far is the solid gold Magikarp. This comes from a blog post. This person went to the token embedding table and clustered the tokens based on their embedding representation. This person noticed that there was a cluster of tokens that looked really strange, like "rot e stream Fame solid gold Magikarp Signet message." What are these tokens and where do they come from? Then, they noticed that if you ask the model about these tokens, like "Please can you repeat back to me the string 'solid gold Magikarp'," then you get a variety of totally broken LLM behavior.
