Okay, let's begin. There is no loss to create if there are no targets, so the loss is none. However, if there are targets, then we can calculate a loss. This setup allows us to obtain a loss when targets are provided, and if no targets are provided, we will simply get the logits. This code will generate output from the model.

I have another code chunk here, which will generate output from the model. Let me break this down. The `idx` variable is used to create a batch, which will be of size one. This is a one-by-one tensor holding a zero. The data type is an integer. The zero will be used to kick off the generation. Remember that zero represents a new line character, so it is a reasonable starting character. We feed `idx` into the model, requesting 100 tokens. The `generate` function will continue generating. Since `generate` operates on batches, we index into the zero-th row to remove the batch dimension. This gives us a one-dimensional array of indices. We convert this array, which is a PyTorch tensor, into a Python list. This list is then passed into our decode function to convert the integers into text. We are generating 100 tokens. When we run this, we get garbage output because the model is currently random.

We will want to train the model to make the output less random. This function is written to be general, but it is somewhat inefficient. We are building out the context and feeding it all into the model. This is unnecessary for the current simple bigram model. To make a prediction about K, we only need W. However, we are feeding the entire sequence into the model and only looking at the last piece. The reason for this is that while this is a bigram model now, I want to keep this function fixed so that it will work later when characters look further into the history. Right now, the history is not used, so this looks silly, but eventually, the history will be used, which is why we implement it this way. 

Now that we have seen that the model is random, let's train it. First, I will create a PyTorch optimization object. Here, we are using the Adam optimizer. In past videos, we used stochastic gradient descent (SGD). Adam is a more advanced and popular optimizer. A typical learning rate setting for Adam is 3e-4. However, for very small networks, we can use much higher learning rates, like 3e-3 or higher. Let me create the optimizer object that will take the gradients and update the parameters. Then, the batch size above was four, so let's use 32. For some number of steps, we sample a new batch of data, evaluate the loss, zero out the gradients, get the gradients for all parameters, and update the parameters. This is a typical training loop.

Let's run this for 100 iterations. The loss starts around 4.7 and goes down to 4.6 and 4.5, so optimization is definitely happening. Let's try to increase the number of iterations and only print at the end. We likely need to train for longer. We are now roughly down to three. It's working. Let's just do 10,000 iterations. Hopefully, we'll get something reasonable. It won't be Shakespeare, but at least the loss is improving. We are expecting something more reasonable. Okay, we are down to about 2.5. Here is the output, a dramatic improvement over what we had before. Let's increase the number of tokens. We see that we are starting to get something at least reasonable. The model is making progress. That is the simplest possible model.

Now, the tokens are not talking to each other. Given the previous context, we are only looking at the last character to make predictions. We want these tokens to start talking to each other and figuring out the context. This will enable better predictions. This will start us on the path to creating a Transformer. I took the code from the Jupyter notebook and converted it into a script. This is to simplify the work into a final product. At the top, I put the hyperparameters. I introduced a few, which I will discuss. Otherwise, a lot of this should be recognizable. We have reproducibility, data reading, encoder and decoder creation, train-test splits, and the data loader to get batches of inputs and targets. This data loader part is new and I will discuss it in a bit. We also have the bigram language model that we developed which can forward, give us logits and loss, and generate. The optimizer is created and we have the training loop.

Some small changes were added. First, I added the ability to run on a GPU if you have one. If you have a GPU, this will use CUDA instead of just the CPU, making everything much faster. If the device is CUDA, then when we load the data, we move it to the device. When we create the model, we need to move the model parameters to the device. For example, the embedding table has a weight inside that stores the lookup table, which would be moved to the GPU so the calculations happen there. When creating the context that feeds into generate, we must create it on the device. Second, in the training loop, instead of printing the loss item inside the loop, which can be noisy, I have added an `estimate_loss` function. This function averages the loss over multiple batches. We iterate `eval_iters` times and get the loss to obtain the average loss for both splits. This will be less noisy and give a more accurate train and validation loss. We set the model to evaluation phase before calling `estimate_loss` and reset it to training phase afterwards. Our current model does nothing in evaluation phase but it is good practice to think through which mode your neural network is in as some layers will have different behavior at inference and training time. There's also the context manager `torch.no_grad` which tells PyTorch not to call backward on code within it so that PyTorch can be more memory efficient. This is a good practice when we don't intend to do back propagation.

Right now, this script is about 120 lines of code, and this is our starting code, called `b.py`. Running this script gives the output in the terminal, showing the train loss, validation loss, and the sample produced at the end. We are now packaged into a script and are ready to iterate. Before writing our first self-attention block, I want to show a mathematical trick that is used in self-attention and is at the heart of an efficient implementation. Let's work with a toy example to understand this operation. We create a tensor with dimensions B by T by C, where B, T, and C are 4, 8, and 2, respectively. These represent batches, time components, and channels. Currently, the eight tokens are not interacting with each other, but we want to couple them. The token at the fifth location should not communicate with future tokens, such as the sixth, seventh, or eighth positions. Instead, it should only communicate with tokens in the fourth, third, second, and first positions. Information should only flow from the past to the current time step.

The easiest way for tokens to communicate is to average the preceding elements. For the fifth token, we would take the channels of that token, plus the channels from the fourth, third, second, and first steps, and average them. This would be a feature vector summarizing the token in the context of its history. Averaging is a weak form of interaction, and a lot of information about the spatial arrangement is lost, but that's okay for now. We are going to calculate the average of all the vectors in all the previous tokens and at this token for every single batch element independently for every token in the sequence. I have a small snippet here that I will paste. We will create `x_bag`, short for bag of words, and it will be initialized to zero. We iterate over all batch dimensions and then over time. The previous tokens are at the current batch dimension and everything up to and including the current time step.

When we slice `X` this way, `x_prev` becomes of shape how many T elements there were in the past, and then C, the two-dimensional information. That's the previous chunk of tokens in the sequence. We take the mean over the zero dimension. This means we're averaging over time. The result is a one-dimensional vector, `c`, which we store in `x_bag`. `x_bag` at the first location is equal to the initial token, because it is averaging just one token. The second location averages the first two tokens and so on, so the last is the average of all elements. This is all well and good, but very inefficient.

The trick is that we can be very efficient using matrix multiplication. I have a toy example here with a 3x3 matrix of all ones called `a`, a 3x2 matrix of random numbers called `b`, and a matrix `c` which will be the result of matrix multiplication `a` and `b`. This is `a` times `b` giving us `c`. The numbers in `c` are calculated through dot products between rows of `a` and columns of `b`. The top left element of `c` is the dot product of the first row of `a` and the first column of `b`. Because `a`'s first row is all ones, this operation sums the first column of `b`. Similarly, the top right number is the sum of the second column of `b`. Because rows of `a` are all ones we get repeating sums. The trick is this, the matrix `a` is a boring array of all ones, but torch has this function called `tril` which is short for triangular, and it returns a lower triangular portion of a matrix. Let's use this `tril` on our array of ones. What happens if we do this?

We have an `a` like this, and a `b` like this. What do we get for `c`? The first number is the first row of `a` times the first column of `b`. Because of the zeros, we are ignoring the zeros and only take the first element of `b`, which is two. The second number of `c` is the first row of `a` times the second column of `b`, which is the first number, seven. We are plucking out the row of `b`. Now, with one-one-zero, we get two plus six, which is eight, and seven plus four, which is 11. With 111, we get the sum of all of them. Depending on how many ones and zeros we have, we're summing a variable number of rows, and this gets deposited into `c`. We are doing sums because these are ones, but we can do averages. You can see how we could average the rows of `b` in an incremental fashion. If we normalize these rows so they sum to one, we can get an average. If we divide `a` by the sum of a along the first dimension, and keep that dimension, then the rows now sum to one.

If we do the matrix multiply again, we get the first row, then the average of the first two rows, and the average of these three rows. By manipulating the elements of this multiplying matrix and multiplying it with any given matrix, we can do these averages in an incremental fashion because we can manipulate that. That's very convenient. Let's go back up and see how we can vectorize this and make it much more efficient. We produce an array `a`, which I'm calling `we`, short for weights. This is our `a`, and this is how much of every row we want to average. Because the rows sum to one, it will be an average. Our `b` is `X`. What will happen here now is that we will have an `x_bag2`. `x_bag2` will be `we` multiplying `X`. `we` is T by T, and `X` is B by T by C. PyTorch creates a batch dimension, applying the matrix multiplication in all batch elements individually in parallel. For each batch element, there will be a T by T multiplying T by C, which will create B by T by C. `x_bag2` is identical to `x_bag`.

`torch.allclose` should return true, convincing us that these are, in fact, the same. If I print `x_bag` and `x_bag2` they are the same. The trick was using batched matrix multiply to do this aggregation. It's a weighted aggregation. The weights are specified in the T by T array. We are doing weighted sums. The sums take a triangular form. A token at the Tth dimension will only get information from tokens preceding it.

I'd like to rewrite this one more way. It is identical to the first and second, but it uses softmax. `Tril` is the lower triangular ones matrix, and `we` begins as all zeros. If we print `we` in the beginning, it's all zero. Then, I use masked fill. This says to make the elements of `we` equal to negative infinity if the elements of `tril` are equal to zero. All the elements where `tril` is zero become negative infinity. Then, we take the softmax along every single row. Softmax is a normalization operation, and it will result in the exact same matrix. Recall that softmax will exponentiate every element and divide by the sum. If we exponentiate here, we will get one, then zeroes everywhere else. Normalizing gives one here. If we exponentiate the second row, we will get one one, and then zeroes. Softmax will again divide, giving 0.5 and 0.5. We are producing the same mask in a different way.

The reason this is more interesting is that these weights here start at zero, and they represent an interaction strength or affinity. They tell us how much of each token from the past we want to aggregate and average. This line is saying tokens from the past cannot communicate, so we will not aggregate anything from those tokens. This then goes through softmax and weighted aggregation through matrix multiplication. These zeros are currently set to zero, but these affinities between tokens are not going to be constant. They will be data dependent. Tokens will look at each other, finding some more or less interesting. Depending on values, tokens will find each other interesting to different degrees. I will call these affinities. We clamp the future tokens, and then when we normalize and sum, we aggregate the values depending on how interesting they find each other. That is a preview of self-attention. The long story short is you can do weighted aggregations of past elements using a matrix multiplication and a lower triangular mask. Elements in the lower triangular part tell you how much of each element fuses into a given position. We will use this to develop the self-attention block.

Let's get some preliminaries out of the way. I am bothered that we are passing `vocab_size` into the constructor. There is no need, since it's defined as a global variable. Let's create a level of indirection where we do not directly go from the embedding to the logits. We will introduce a variable, `n_embed`, which is short for the number of embedding dimensions. We set this to 32. It was a suggestion from GitHub Copilot. This is an embedding table with only 32-dimensional embeddings. This will give us token embeddings and not logits directly. To get from embeddings to logits, we need a linear layer. `self.lm_head` will be the language modeling head, which is `nn.Linear` from `n_embed` up to `vocab_size`. When we forward, we will get logits. We have to be careful because the channel dimension here is `n_embed`, while here it is `vocab_size`. I will set `n_embed` equal to `C`, creating a single, spurious layer of interaction with a linear layer. This should run, and it does. Currently, this looks kind of spurious, but we'll build on top of this.

Next, we will encode the positions of the tokens. We add a second position embedding table, called `self.position_embedding_table`. This will have a size of block size by `n_embed`. Each position from zero to `block_size` minus one will get its own embedding vector. We decode the batch and time dimensions from `idx.shape`. We also have `pos_embedding`, which will be the positional embeddings. This will arrange integers from zero to T-minus-1. The integers are then embedded through the table, creating a tensor of T by C. `X` is then renamed to just `x` and becomes the addition of the token embeddings with the positional embeddings. The broadcasting rules work out. The result, `x`, now holds not just the token identities, but also the positions at which the tokens occur. This is not useful currently, since we have a simple bigram model. The model is translation invariant, so the information doesn't help. However, as we work on the self-attention block, we will see why this matters.

Here we get to the crux of self attention. This is likely the most important part of this video. We will implement a small self-attention head. We will start where we were. All of this is familiar. We have a 4x8 arrangement of tokens and information for each token is 32-dimensional. We have seen that our code does a simple average of all past and present tokens. This is achieved by creating a lower triangular structure that masks out our weight matrix. We mask it out, normalize it and the weights in this weight matrix have these uniform numbers. The matrix multiply makes it so that we are doing a simple average. We don't want this to be uniform, because different tokens will find different other tokens more or less interesting. We want that to be data dependent. If I'm a vowel, maybe I'm looking for consonants. I want that information to flow to me. We want to gather information in a data dependent way, and this is the problem self-attention solves.

Every token will emit two vectors: a query and a key. The query is roughly "what am I looking for?" and the key is roughly "what do I contain?". We calculate the affinities by taking the dot product between the keys and the queries. My query dot products with all the keys of all the other tokens. The dot products become `we`. If a key and query are aligned, they will interact to a high amount, and we will learn more about that specific token. Let's implement this now. We will implement a single head of self-attention. There's a hyperparameter involved here called the head size. We initialize linear modules with `bias=False`. These will apply a matrix multiply with fixed weights. Let's produce `k` and `q` by forwarding these modules on `x`. The size of each of these becomes B by T by 16 because that is our head size. You can see that when I forward this linear module on top of `x`, it becomes B by T by 16.
