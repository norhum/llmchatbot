The code pointed to the CPU kernel and the CUDA kernel for `tanh` backward, which depends on whether you're using PyTorch on a CPU or GPU device. These are different devices. This is the `tanh` backward kernel for the CPU. It's large because it handles complex types, a specific data type of b-float 16, and also a general case. Deep inside, we see a backward pass with `a * (1 - b^2)`, where `b` is the output of `tanh` and `health.grad` is present. This was found inside the `binaryops` kernel, even though `tanh` is not a binary operation. Here is the GPU kernel, which is just one line of code for the non-complex case. While we found the code, the PyTorch implementations are very large, unlike Micrograd. PyTorch allows you to register a new type of function to add as a building block. For example, to add a Legendre polynomial, you would register a class that subclasses `storage.org.function`. You must define the forward pass and the backward pass of the function. PyTorch will backpropagate through your function if you implement the forward and local gradient (backward). Then, you can use this function as a component with other PyTorch components. This is the only information that you have to provide. You can register new function types this way. That is everything I wanted to cover in this lecture. I hope you enjoyed building out Micrograd and found it interesting. I will post relevant links in the video description. I may also post a link to a discussion forum where you can ask questions. I might also do a follow-up video to answer common questions. If you enjoyed this video, please like and subscribe. Now we know `dl` by wait. And that's everything I wanted to cover in this lecture. I hope you enjoyed us building up Microcraft. Let's do the same thing for multiplication since we cannot do something like `a times 2`.
