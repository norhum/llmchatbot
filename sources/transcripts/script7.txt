Okay, let's cover the process of tokenization in large language models. I have a set face because tokenization is my least favorite part of working with large language models. Unfortunately, it is necessary to understand in some detail because it is fairly complex and there are many hidden issues to be aware of. Many of the oddities with large language models trace back to tokenization. What is tokenization? In my previous video, "Let's Build GPT from scratch," we already did tokenization. However, it was a very naive and simple version. If you go to the Google Colab for that video, you will see that we loaded our training set. Our training set was the Shakespeare dataset. Initially, the Shakespeare dataset is just a large string in Python. It is just text. The question is: how do we plug text into large language models? In this case, we created a vocabulary of 65 possible characters that we saw in the string. These were the possible characters, and we found that there were 65 of them. Then, we created a lookup table for converting from every possible character, or little string piece, into a token, which is an integer. For example, we tokenized the string "Hi there" and received this sequence of tokens. Here, we took the first 1,000 characters of our dataset and encoded them into tokens. Since this was character-level tokenization, we received 1,000 tokens in a sequence, such as token 18, 47, etc. Later, we saw that the way we plug these tokens into the language model is by using an embedding table. If we have 65 possible tokens, then this embedding table will have 65 rows. Roughly speaking, we take the integer associated with every single token and use that as a lookup into this table. We then pluck out the corresponding row. This row contains trainable parameters that we will train using backpropagation. This vector then feeds into the Transformer. That is how the Transformer perceives every single token. We had a very naive tokenization process that was a character-level tokenizer. In practice, state-of-the-art language models use much more complicated schemes for constructing these token vocabularies. We are not dealing on the character level, but on the chunk level. These character chunks are constructed using algorithms such as the byte pair encoding algorithm, which we will go into in detail in this video. I would like to briefly show you the paper that introduced byte-level encoding as a mechanism for tokenization in the context of large language models. I would say that is probably the GPT-2 paper. If you scroll down to the "Input Representation" section, this is where they cover tokenization. They discuss the types of properties that you would like the tokenization to have. They conclude that they are going to have a tokenizer with a vocabulary of 50,257 possible tokens. The context size will be 1,024 tokens. In the attention layer of the Transformer neural network, every single token is attending to the previous tokens in the sequence, up to 1,024 tokens. Tokens are this fundamental unit of large language models. Everything is in units of tokens. Tokenization is the process for translating strings or text into sequences of tokens, and vice versa. When you go to the Llama 2 paper, if you search for "token," you will find 63 hits. Again, tokens are pervasive. Here, they mention that they trained on two trillion tokens of data. We are going to build our own tokenizer. Luckily, the byte pair encoding algorithm is not super complicated. We can build it from scratch ourselves and see exactly how it works. Before we dive into code, I would like to give you a brief taste of some of the complexities that arise from tokenization. I want to make sure that we have sufficient motivation for why we are doing this and why it can be so problematic. Tokenization is at the heart of a lot of weirdness in large language models. I advise that you do not brush it off. Many issues that may appear to be issues with the neural network architecture or the large language model itself are actually issues with the tokenization. These issues fundamentally trace back to tokenization. If you have noticed any issues with large language models not being able to easily do spelling tasks, that is usually due to tokenization. Simple string processing can be difficult for the large language model to perform natively. Non-English languages can work much worse. To a large extent, this is due to tokenization. Sometimes, LLMs are bad at simple arithmetic, which can also be traced to tokenization. GPT-2 specifically would have had quite a few more issues with Python than future versions due to tokenization. There are many other issues, such as the weird warnings about trailing white spaces, which is a tokenization issue. If you had asked GPT earlier about "solid gold Magikarp" and what it is, the LLM would go totally crazy and start going off on a completely unrelated tangent. You may have been told to use YAML over JSON in structured data. All of that has to do with tokenization. Basically, tokenization is at the heart of many issues. I will come back to these at the end of the video, but for now, let me just skip over it a little bit and let's go to this web app, the Tik tokenizer bell.app. I have it loaded here. What I like about this web app is that tokenization is running live in your browser using JavaScript. You can just type here, for example, "hello world." The whole string is tokenized. On the left is the string that you put in. On the right, we are currently using the GPT-2 tokenizer. This string that I pasted here is currently tokenizing into 300 tokens. Here, they are shown explicitly in different colors for every single token. For example, the word "tokenization" became two tokens: the token 3,642 and 1,634. The token for a space is token 318. Be careful, at the bottom you can show whitespace. Keep in mind that there are spaces and new line characters here. You can hide them for clarity. The token "space at" is token 379. The token "space the" is 262. You will notice that the space is part of that token chunk. This is kind of how our English sentence breaks up, and it seems all well and good. Here I put in some arithmetic. We see that token 127 is a single token for the plus sign. Then, there is token six, space, and then 6, followed by 77. What is happening is that 127 is feeding in as a single token into the large language model. However, the number 677 will feed in as two separate tokens. The large language model has to take account of that and process it correctly in its network. The number 804 will be broken up into two tokens. This is all completely arbitrary. Sometimes, you have multiple digits as a single token. Sometimes you have individual digits as multiple tokens. It is all arbitrary and coming out of the tokenizer. Here is another example. We have the string "egg," and you see that this became two tokens. However, for some reason, when I say, "I have an egg," you see when it's "space egg," it is a single token. "Egg" by itself at the beginning of a sentence is two tokens. Here, as "space egg," it is suddenly a single token, for the exact same string. Lowercase "egg" turns out to be a single token, and in particular, notice that the color is different. This is a different token. This is case-sensitive. Of course, a capital "Egg" would also be different tokens. Again, this would be two tokens arbitrarily. For the same concept of "egg," depending on if it is at the beginning of a sentence, at the end of the sentence, lowercase, uppercase, or mixed, all of these will be very different tokens with different IDs. The language model has to learn from raw data, from all the internet text that it is trained on, that these are actually all the exact same concept. It has to group them in the parameters of the neural network and understand, just based on the data patterns, that these are all very similar, maybe not almost exactly similar, but very, very similar. After the "egg" demonstration, here is an introduction from OpenAI's ChatGPT in Korean, "manaso pangâ€¦". The reason I put this here is because you will notice that non-English languages work slightly worse in ChatGPT. Part of this is, of course, because the training dataset for ChatGPT is much larger for English and everything else. However, the same is true not just for the large language model itself, but also for the tokenizer. When we train the tokenizer, we will see that there is a training set as well. There is a lot more English than non-English. What ends up happening is that we are going to have a lot of longer tokens for English. If you have a single sentence in English and you tokenize it, you might see that it is 10 tokens or something like that. If you translate that sentence into say, Korean or Japanese, you will typically see that the number of tokens used is much larger. This is because the chunks here are much more broken up. We are using a lot more tokens for the exact same thing. This bloats up the sequence length of all the documents. You are using up more tokens. Then in the attention of the Transformer, when these tokens try to attend to each other, you are running out of context in the maximum context length of the Transformer. All non-English text is stretched out from the perspective of the Transformer. This has to do with the training data used for the tokenizer and the tokenization itself. It will create a lot bigger tokens and larger groups in English and have a lot of little boundaries for all the other non-English text. If we translated this into English, it would be significantly fewer tokens. The final example I have here is a little snippet of Python for doing FizzBuzz. What I would like you to notice is that all of these individual spaces are separate tokens. They are token 220. Token 220, 220, 220, and then "space if" is a single token. When the Transformer is going to consume or create this text, it needs to handle all of these spaces individually. They all feed in one by one into the entire Transformer in the sequence. This is extremely wasteful. Tokenizing it this way causes GPT-2 to not be very good with Python. This has nothing to do with coding or the language model itself. If you use a lot of indentation with spaces in Python, you just end up bloating out all of the text. It is separated across way too much of the sequence. We are running out of context length in the sequence. We are being too wasteful and taking up way too much token space. We can also scroll up here and change the tokenizer. Note here that the GPT-2 tokenizer creates a token count of 300 for this string. We can change it to CL 100K base, which is the GPT-4 tokenizer. We see that the token count drops to 185. For the exact same string, we now have roughly half the number of tokens. The number of tokens in the GPT-4 tokenizer is roughly double that of the number of tokens in the GPT-2 tokenizer. We went from roughly 50k to roughly 100k. You can imagine that this is a good thing because the same text is now squished into half as many tokens. This is a much denser input to the Transformer. In the Transformer, every single token has a finite number of tokens before it that it is going to pay attention to. What this is doing is we are roughly able to see twice as much text as context for what token to predict next. This is due to this change. Of course, increasing the number of tokens is not strictly better infinitely. As you increase the number of tokens, your embedding table gets much larger. At the output, we are trying to predict the next token, and the softmax there also grows. We will go into more detail later on this. There is some kind of sweet spot where you have just the right number of tokens in your vocabulary, where everything is appropriately dense and still fairly efficient. One thing I would like you to note specifically for the GPT-4 tokenizer is that the handling of whitespace for Python has improved a lot. You see that here, these four spaces are represented as one single token. For the three spaces here, it is a token for "space if". Here, seven spaces were grouped into a single token. We are being a lot more efficient in how we represent Python. This was a deliberate choice made by OpenAI when they designed the GPT-4 tokenizer. They group a lot more space into a single character. This densifies Python, so we can attend to more code when we are trying to predict the next token in the sequence. The improvement in Python coding ability from GPT-2 to GPT-4 is not just a matter of the language model and the architecture and the details of the optimization. A lot of the improvement here is also coming from the design of the tokenizer and how it groups characters into tokens. So let's now start writing some code. Remember, what we want to do is take strings and feed them into language models. For that, we need to somehow tokenize strings into some integers, using some fixed vocabulary. Then we will use those integers to look up vectors in a lookup table and feed those vectors into the Transformer as input. The reason this gets a little tricky is that we don't just want to support the simple English alphabet. We want to support different languages. This is "anango" in Korean, which means hello. We also want to support many kinds of special characters that we might find on the internet, like emojis. How do we feed this text into Transformers? What is this text anyway in Python? If you go to the documentation of a string in Python, you can see that strings are immutable sequences of Unicode code points. What are Unicode code points? We can go to the Unicode Consortium. Unicode code points are defined by the Unicode Consortium as part of the Unicode standard. This is just a definition of roughly 150,000 characters right now. It also defines what they look like and what integers represent those characters. It states that there are 150,000 characters across 161 scripts as of right now. If you scroll down here, you can see that the standard is very much alive. The latest standard is 15.1 from September 2023. This is just a way to define many types of characters, like all these characters across different scripts. We can access the Unicode code point of a single character by using the `ord` function in Python. For example, I can pass in `ord('H')` and see that the Unicode code point for the single character 'H' is 104. This can be arbitrarily complicated. We can take our emoji here and see that the code point for this one is 128,000. Or we can take "an" and see that this is 50,000. Keep in mind you can't plug in strings here because this doesn't have a single code point; it only takes a single Unicode code point character and tells you its integer. In this way, we can look up all the characters of this specific string and their code points. `ord(x) for x in this_string` gives us this encoding. We have already turned the raw code points into integers. Why can't we simply just use these integers and not have any tokenization at all? Why can't we just use this natively as is and use the code point? One reason is that the vocabulary in that case would be quite long. In this case, for Unicode, this is a vocabulary of 150,000 different code points. More worryingly, I think the Unicode standard is very much alive, and it keeps changing. It is not necessarily a stable representation that we might want to use directly. For these reasons, we need something better. To find something better, we turn to encodings. If we go to the Wikipedia page, we see that the Unicode Consortium defines three types of encodings: UTF-8, UTF-16, and UTF-32. These encodings are the way by which we can take Unicode text and translate it into binary data or byte streams. UTF-8 is by far the most common. This is the UTF-8 page. This Wikipedia page is quite long, but what is important for our purposes is that UTF-8 takes every single code point and translates it to a byte stream. This byte stream is between one to four bytes, so it is a variable-length encoding. Depending on the Unicode point, according to the schema, you will end up with between one to four bytes for each code point. On top of that, there are UTF-16 and UTF-32. UTF-32 is nice because it is fixed length instead of variable length, but it has many other downsides. The full spectrum of pros and cons of all these different three encodings is beyond the scope of this video. I just like to point out that I enjoyed this blog post. This blog post at the end also has a number of references that can be quite useful. One of them is the "UTF-8 Everywhere Manifesto." This manifesto describes the reasons why UTF-8 is significantly preferred and a lot nicer than the other encodings, and why it is used a lot more prominently on the internet. One of the major advantages, to give you a sense, is that UTF-8 is the only one of these that is backwards compatible to the much simpler ASCII encoding of text. I am not going to go into the full detail in this video. It is enough to say that we like the UTF-8 encoding. Let's try to take the string and see what we get if we encode it into UTF-8. The string class in Python has a `.encode` method, and you can give it the encoding, such as "utf-8." What we get out of this is a bytes object. It's not very nice in the way that it is printed. I personally like to take it through a list. Then, we get the raw bytes of this encoding. These are the raw bytes that represent this string according to the UTF-8 encoding. We can also look at UTF-16. We get a slightly different byte stream, and we start to see one of the disadvantages of UTF-16. You see how we have "0 0 something, 0 something, 0 something." We start to get a sense that this is a bit of a wasteful encoding. Indeed, for simple ASCII characters or English characters, we just have the structure of "0 something, 0 something," and it is not exactly nice. It's the same for UTF-32. When we expand this, we can start to get a sense of the wastefulness of this encoding. You see a lot of zeros followed by something. This is not desirable. We would like to stick with UTF-8 for our purposes. However, if we just use UTF-8 naively, these are byte streams, which would imply a vocabulary length of only 256 possible tokens. This vocabulary size is very, very small. If we were to use it naively, all of our text would be stretched out over very long sequences of bytes. The embedding table would be tiny, and the prediction at the final layer would be tiny, but our sequences are very long. Remember, we have pretty finite context length and the attention that we can support in a Transformer for computational reasons. We only have so much context length, but now we have very, very long sequences. This is inefficient. It will not allow us to attend to sufficiently long text before us for the purposes of the next token prediction task. We don't want to use the raw bytes of the UTF-8 encoding. We want to be able to support a larger vocabulary size that we can tune as a hyperparameter. We want to stick with the UTF-8 encoding of these strings. What do we do? The answer is that we turn to the byte pair encoding algorithm. This will allow us to compress these byte sequences to a variable amount. We will get to that in a bit, but I just want to briefly speak to the fact that I would love nothing more than to be able to feed raw byte sequences into language models. There is a paper about how this could potentially be done from last summer. The problem is that you actually have to go in and modify the Transformer architecture. As I mentioned, you will have a problem where the attention will start to become extremely expensive because the sequences are so long. In this paper, they propose a hierarchical structuring of the Transformer that could allow you to just feed in raw bytes. At the end, they say, "Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale." Tokenization-free would indeed be amazing. We could just feed byte streams directly into our models. Unfortunately, I do not know that this has really been proven out yet by sufficiently many groups at a sufficient scale. Something like this at one point would be amazing, and I hope someone comes up with it. For now, we have to come back and we can't feed this directly into language models. We have to compress it using the byte pair encoding algorithm. Let's see how that works. As I mentioned, the byte pair encoding algorithm is not all that complicated. The Wikipedia page is quite instructive as far as the basic idea goes. What we are doing is we have some kind of input sequence. For example, here we have only four elements in our vocabulary: a, b, c, and d. We have a sequence of them. Instead of bytes, let's say we just have a vocabulary size of four. The sequence is too long, and we'd like to compress it. We iteratively find the pair of tokens that occur the most frequently. Once we have identified that pair, we replace it with a single new token that we append to our vocabulary. For example, here, the byte pair "AA" occurs most often. We mint a new token, let's call it capital Z. We replace every single occurrence of "AA" with Z. Now, we have two Z's here. We took a sequence of 11 characters with a vocabulary size of four and converted it to a sequence of only nine tokens, but with a vocabulary of five because we have a fifth vocabulary element that we just created. That is Z, standing for the concatenation of "AA." We can again repeat this process. We again look at the sequence and identify the pair of tokens that are most frequent. Let's say that that is now "AB." We are going to replace "AB" with a new token that we will call Y. Y becomes "AB," and every single occurrence of "AB" is replaced with Y. We end up with this. Now, we only have seven characters in our sequence. We have not just four vocabulary elements or five, but now we have six. For the final round, we again look through the sequence, find that the phrase "ZY" or the pair "ZY" is most common, and replace it one more time with another character, let's say X. X is "ZY". We replace all occurrences of "ZY" and we get the following sequence. After we have gone through this process, instead of having a sequence of 11 tokens with a vocabulary length of four, we now have a sequence of five tokens. Our vocabulary length is now seven. In this way, we can iteratively compress our sequence. We mint new tokens. In the exact same way, we start with byte sequences, so we have a 256 vocabulary size. We are going to go through these and find the byte pairs that occur the most. We are going to iteratively start minting new tokens, appending them to our vocabulary, and replacing things. In this way, we are going to end up with a compressed training dataset and also an algorithm for taking any arbitrary sequence and encoding it using this vocabulary. We can also decode it back to strings. Let's now implement all of that. Here is what I did. I went to the blog post that I enjoyed and took the first paragraph and copy-pasted it into the notebook. This is one very long line. To get the tokens, as I mentioned, we take our text and encode it into UTF-8. The tokens here will be raw bytes, a single stream of bytes. So that it is easier to work with, instead of just a bytes object, I'm going to convert all those bytes to integers. I will create a list of it so it's easier to manipulate, work with, and visualize in Python. Here I'm printing all of that. This is the original paragraph, and its length is 533 code points. Here are the bytes encoded in UTF-8. We see that this has a length of 616 bytes or 616 tokens. The reason this is more is because a lot of these simple ASCII characters or simple characters just become a single byte. A lot of these more complex Unicode characters become multiple bytes, up to four. So we are expanding the size. Now, what we would like to do as a first step of the algorithm is to iterate over this and find the pair of bytes that occurs most frequently because we are then going to merge them. If you are working on a notebook on the side, then I encourage you to click on the link, find this notebook, and try to write that function yourself. Otherwise, I'm going to come here and implement first the function that finds the most common pair. Here's what I came up with. There are many different ways to implement this, but I'm calling the function `get_stats`. It expects a list of integers. I'm using a dictionary to keep track of the counts. This is a Pythonic way to iterate through consecutive elements of this list, which we covered in the previous video. Here I am incrementing the count by one for all the pairs. If I call this on all the tokens, the stats come out here. This is the dictionary. The keys are tuples of consecutive elements, and this is the count. To print it in a slightly better way, this is one way that I like to do that. You can pause if you like. We iterate over all the items. The items called on the dictionary return pairs of key and value. Instead, I create a list here of value and key. If it is a value-key list, then I can call `sort` on it. By default, Python will use the first element, which in this case will be the value, to sort by if it is given tuples, and then reverse it so it is descending. Then, print that. It looks like 101, 32 was the most commonly occurring consecutive pair, and it occurred 20 times. We can double-check that, and that makes reasonable sense. If I search for 101, 32, then you see that these are the 20 occurrences of that pair. If we would like to take a look at what exactly that pair is, we can use `chr`, which is the opposite of `ord` in Python. We give it a Unicode code point, so `chr(101)` and `chr(32)`, and we see that this is "e" and "space". There is a lot of "e space" here. A lot of words seem to end with "e". Here's "e space" as an example. There is a lot of that going on here. This is the most common pair. Now that we have identified the most common pair, we would like to iterate over this sequence. We are going to mint a new token with the ID of 256, because these tokens currently go from 0 to 255. When we create a new token, it will have an ID of 256. We are going to iterate over this entire list, and every time we see 101, 32, we are going to swap that out for 256. Let's implement that now, and feel free to do that yourself as well. First, I commented this so we don't pollute the notebook too much. This is a nice way of obtaining the highest-ranking pair in Python. We are basically calling `max` on this dictionary `stats`, and this will return the maximum key. The question is: how does it rank keys? You can provide it with a function that ranks the keys. That function is just `stats.get`. `stats.get` would basically return the value. We are ranking by the value and getting the maximum key. It is 101, 32, as we saw. To actually merge 101, 32, this is the function that I wrote. There are many different versions of it. We will take a list of IDs and the pair that we want to replace. That pair will be replaced with the new index, `idx`. Iterating through the IDs, if we find the pair, swap it out for `idx`. We create a new list, start at zero, and go through this entire list sequentially from left to right. Here, we are checking for equality at the current position with the pair.

We're checking that the pair matches. Here is a tricky condition that you have to append if you're trying to be careful: you don't want this here to be out of bounds at the very last position when you're on the rightmost element of this list. Otherwise, this would give you an out-of-bounds error. We have to make sure that we're not at the very last element, so this would be false for that. If we find a match, we append to this new list that replacement index, and we increment the position by two, so we skip over that entire pair. Otherwise, if we haven't found a matching pair, we just copy over the element at that position and increment by one, then return this. Here's a very small toy example: if we have a list `5 6 6 7 9 1` and we want to replace the occurrences of `6 7` with `99`, then calling this on that will give us what we're asking for. Here, the `6 7` is replaced with `99`. Now, I'm going to uncomment this for our actual use case where we want to take our tokens, we want to take the top pair here, and replace it with `256` to get tokens to. If we run this, we get the following. Recall that previously we had a length of 616 in this list, and now we have a length of 596. This decreased by 20, which makes sense because there are 20 occurrences. Moreover, we can try to find `256` here, and we see plenty of occurrences of it. And moreover, just double check, there should be no occurrence of `101 32`. So, this is the original array with plenty of them, and in the second array, there are no occurrences of `1032`. We've successfully merged this single pair, and now we just iterate this. We are going to go over the sequence again, find the most common pair, and replace it. Let me now write a while loop that uses these functions to do this iteratively. How many times do we do it? Well, that's totally up to us as a hyperparameter. The more steps we take, the larger will be our vocabulary and the shorter will be our sequence. There is some sweet spot that we usually find works the best in practice. This is kind of a hyperparameter, and we tune it and we find good vocabulary sizes. As an example, GPT4 currently uses roughly 100,000 tokens, and BPE, those are reasonable numbers currently instead in large language models. Let me now write, putting it all together, and iterating these steps. Before we dive into the while loop, I wanted to add one more cell here where I went to the blog post, and instead of grabbing just the first paragraph or two, I took the entire blog post and I stretched it out in a single line. Basically, just using longer text will allow us to have more representative statistics for the byte pairs, and we'll just get more sensible results out of it because it's longer text. Here we have the raw text. We encode it into bytes using the UTF-8 encoding. Then here, as before, we are just changing it into a list of integers in Python, just so it's easier to work with instead of the raw bytes objects. Then, this is the code that I came up with to actually do the merging in a loop. These two functions here are identical to what we had above. I only included them here just so that you have a point of reference. These two are identical, and then this is the new code that I added. The first thing we want to do is we want to decide on the final vocabulary size that we want our tokenizer to have. As I mentioned, this is a hyperparameter, and you set it in some way depending on your best performance. Let's say for us, we're going to use 276, because that way, we're going to be doing exactly 20 merges. We're doing 20 merges because we already have 256 tokens for the raw bytes, and to reach 276, we have to do 20 merges to add 20 new tokens here. This is one way in Python to just create a copy of a list. I'm taking the tokens list, and by wrapping it in a list, Python will construct a new list of all the individual elements, so this is just a copy operation. Then here, I'm creating a merges dictionary. This merges dictionary is going to maintain basically the child one, child two mapping to a new token. What we're going to be building up here is a binary tree of merges, but actually, it's not exactly a tree, because a tree would have a single root node with a bunch of leaves. For us, we're starting with the leaves on the bottom, which are the individual bytes. Those are the starting 256 tokens, and then we're starting to merge two of them at a time. So, it's not a tree, it's more like a forest, as we merge these elements. For 20 merges, we're going to find the most commonly occurring pair. We're going to mint a new token integer for it. So, `I` here will start at zero, so we're going to start at `256`. We're going to print that we're merging it, and we're going to replace all of the occurrences of that pair with the new lied token. We're going to record that this pair of integers merged into this new integer. Running this gives us the following output. We did 20 merges, and, for example, the first merge was exactly as before, the `101 32` tokens merging into a new token `256`. Keep in mind that the individual tokens `101` and `32` can still occur in the sequence after merging. It's only when they occur exactly consecutively that that becomes `256`. In particular, the other thing to notice here is that the token `256`, which is the newly minted token, is also eligible for merging. Here, on the bottom, the 20th merge was a merge of `25` and `259` becoming `275`. Every time we replace these tokens, they become eligible for merging in the next round of iteration. That's why we're building up a small sort of binary forest instead of a single individual tree. One thing we can take a look at as well is we can take a look at the compression ratio that we've achieved. We started off with this tokens list. We started off with 24,000 bytes, and after merging 20 times, we now have only 19,000 tokens. Therefore, the compression ratio, simply just dividing the two, is roughly 1.27. That's the amount of compression we were able to achieve of this text with only 20 merges. Of course, the more vocabulary elements you add, the greater the compression ratio here would be. Finally, that's kind of like the training of the tokenizer, if you will. One point I wanted to make, and maybe this is a diagram that can help illustrate, is that a tokenizer is a completely separate object from the large language model itself. Everything in this lecture, we're not really touching the LLM itself. We're just training the tokenizer. This is a completely separate preprocessing stage, usually. The tokenizer will have its own training set, just like a large language model has a potentially different training set. The tokenizer has a training set of documents on which you're going to train the tokenizer, and we're performing the byte pair encoding algorithm, as we saw above, to train the vocabulary of this tokenizer. It has its own training set. It is a preprocessing stage that you would run a single time in the beginning. The tokenizer is trained using the byte pair coding algorithm. Once you have the tokenizer, once it's trained, and you have the vocabulary and you have the merges, we can do both encoding and decoding. These two arrows here. So, the tokenizer is a translation layer between raw text, which is, as we saw, the sequence of Unicode code points. It can take raw text and turn it into a token sequence and vice versa. It can take a token sequence and translate it back into raw text. Now that we have trained a tokenizer and we have these merges, we are going to turn to how we can do the encoding and the decoding step. If you give me text, here are the tokens, and vice versa, if you give me tokens, here's the text. Once we have that, we can translate between these two realms, and then the language model is going to be trained as a step two afterwards. Typically, in a state-of-the-art application, you might take all of your training data for the language model, and you might run it through the tokenizer and translate everything into a massive token sequence. Then, you can throw away the raw text. You're just left with the tokens themselves, and those are stored on disk. That is what the large language model is actually reading when it's training on them. This one approach that you can take is as a single massive preprocessing step, a stage. I think the most important thing I want to get across is that this is a completely separate stage. It usually has its own entire training set. You may want to have those training sets be different between the tokenizer and the large language model. For example, when you're training the tokenizer, as I mentioned, we don't just care about the performance of English text. We care about many different languages, and we also care about code or not code. You may want to look into different kinds of mixtures of different kinds of languages and different amounts of code and things like that, because the amount of different language that you have in your tokenizer training set will determine how many merges of it there will be. Therefore, that determines the density with which this type of data is in the token space. Roughly speaking, intuitively, if you add some amount of data, like say you have a ton of Japanese data in your tokenizer training set, then that means that more Japanese tokens will get merged. Therefore, Japanese will have shorter sequences, and that's going to be beneficial for the large language model, which has a finite context length on which it can work on in the token space. Hopefully that makes sense. We're now going to turn to encoding and decoding now that we have trained a tokenizer, so we have our merges, and now how do we do encoding and decoding? Let's begin with decoding, which is this arrow over here. Given a token sequence, let's go through the tokenizer to get back a Python string object, the raw text. This is the function that we'd like to implement. We're given the list of integers, and we want to return a Python string. If you'd like, try to implement this function yourself. It's a fun exercise. Otherwise, I'm going to start pasting in my own solution. There are many different ways to do it. Here's one way. I will create a kind of preprocessing variable that I will call `vocab`. `Vocab` is a mapping, or a dictionary in Python, from the token ID to the bytes object for that token. We begin with the raw bytes for tokens from 0 to 255, and then we go in order of all the merges, and we sort of populate this vocab list by doing an addition here. This is basically the bytes representation of the first child followed by the second one. Remember, these are bytes objects, so this addition here is an addition of two bytes objects, just concatenation. That's what we get here. One tricky thing to be careful with, by the way, is that I'm iterating a dictionary in Python using a `.items()`, and it really matters that this runs in the order in which we inserted items into the merges dictionary. Luckily, starting with Python 3.7, this is guaranteed to be the case, but before Python 3.7, this iteration may have been out of order with respect to how we inserted elements into merges, and this may not have worked, but we are using a modern Python, so we're okay. Given the IDs, the first thing we're going to do is get the tokens. The way I implemented this here is I'm taking, I'm iterating over all the IDs, I'm using `vocab` to look up their bytes, and then here, this is one way in Python to concatenate all these bytes together to create our tokens. These tokens here at this point are raw bytes, so I have to decode using UTF-8 back into Python strings. Previously, we called that encode on a string object to get the bytes, and now we're doing the opposite. We're taking the bytes and calling a decode on the bytes object to get a string in Python, and then we can return text. This is how we can do it. This actually has an issue in the way I implemented it, and this could actually throw an error. Try to think, figure out why this code could actually result in an error if we plug in some sequence of IDs that is unlucky. Let me demonstrate the issue. When I try to decode just something like `97`, I am going to get letter A here back. Nothing too crazy happening, but when I try to decode `128` as a single element, the token `128`, what in string or in Python object, Unicode decoder UTF-8 can't decode byte 0x80 which is this in hex, in position zero, invalid start byte. What does that mean? To understand what this means, we have to go back to our UTF-8 page that I briefly showed earlier. This is Wikipedia UTF-8, and basically, there's a specific schema that UTF-8 bytes take. If you have a multi-byte object for some of the Unicode characters, they have to have this special sort of envelope in how the encoding works. What's happening here is that invalid start byte, that's because `128`, the binary representation of it is one followed by all zeros. We have one and then all zeros. We see here that that doesn't conform to the format because one followed by all zeros just doesn't fit any of these rules. It's an invalid start byte, which is byte one. This one must have a one following it, and then a zero following it, and then the content of your Unicode in `x` here. We don't exactly follow the UTF-8 standard, and this cannot be decoded. The way to fix this is to use this `errors = 'replace'` in the `bytes.decode()` function of Python. By default, `errors` is strict, so we will throw an error if it's not valid UTF-8 bytes encoding, but there are many different things that you could put here on error handling. This is the full list of all the errors that you can use. In particular, instead of strict, let's change it to replace, and that will replace with this special marker, this replacement character. Now, we just get that character back. Basically, not every single byte sequence is valid UTF-8, and if it happens that your large language model, for example, predicts your tokens in a bad manner, then they might not fall into valid UTF-8, and then we won't be able to decode them. The standard practice is to basically use `errors='replace'`, and this is what you will also find in the OpenAI code that they released as well. Whenever you see this kind of a character in your output, in that case, something went wrong, and the LLM output was not a valid sequence of tokens. Now we're going to go the other way, so we are going to implement this arrow right here, where we are going to be given a string, and we want to encode it into tokens. This is the signature of the function that we're interested in, and this should basically print a list of integers of the tokens. Again, try to maybe implement this yourself if you'd like a fun exercise, and pause here. Otherwise, I'm going to start putting in my solution. There are many ways to do this. This is one of the ways that I came up with. The first thing we're going to do is we are going to take our text, encode it into UTF-8 to get the raw bytes, and then, as before, we're going to call `list()` on the bytes object to get a list of integers of those bytes. Those are the starting tokens. Those are the raw bytes of our sequence, but now, of course, according to the merges dictionary above, and recall this was the merges. Some of the bytes may be merged according to this lookup. In addition to that, remember that the merges were built from top to bottom, and this is sort of the order in which we inserted stuff into merges. We prefer to do all these merges in the beginning before we do these merges later, because, for example, this merge over here relies on the `256` which got merged here. We have to go in the order from top to bottom sort of if we are going to be merging anything. Now, we expect to be doing a few merges, so we're going to be doing `while True`. Now, we want to find a pair of bytes that is consecutive that we are allowed to merge according to this. In order to reuse some of the functionality that we've already written, I'm going to reuse the function `get_stats`. Recall that `get_stats` will give us, we'll basically count up how many times every single pair occurs in our sequence of tokens and return that as a dictionary. The dictionary was a mapping from all the different byte pairs to the number of times that they occur. At this point, we don't actually care how many times they occur in the sequence. We only care what the raw pairs are in that sequence. I'm only going to be using basically the keys of the dictionary. I only care about the set of possible merge candidates, if that makes sense. Now, we want to identify the pair that we're going to be merging at this stage of the loop. What do we want? We want to find the pair, or like a key inside `stats`, that has the lowest index in the merges dictionary because we want to do all the early merges before we work our way to the late merges. Again, there are many different ways to implement this, but I'm going to do something a little bit fancy here. I'm going to be using the `min()` over an iterator in Python. When you call `min()` on an iterator, and `stats` here is a dictionary, we're going to be iterating the keys of this dictionary in Python. We're looking at all the pairs inside `stats`, which are all the consecutive pairs, and we're going to be taking the consecutive pair inside tokens that has the minimum. The `min()` takes a key, which gives us the function that is going to return a value over which we're going to do the min, and the one we care about is we care about taking merges and basically getting that pair's index. For any pair inside `stats`, we are going to be looking into merges at what index it has, and we want to get the pair with the min number. As an example, if there's a pair `101` and `32`, we definitely want to get that pair. We want to identify it here and return it, and pair would become `101 32` if it occurs. The reason that I'm putting a float infinity here as a fallback is that in the `get()` function, when we call, when we basically consider a pair that doesn't occur in the merges, then that pair is not eligible to be merged. If in the token sequence there's some pair that is not a merging pair, it cannot be merged, then it doesn't actually occur here and it doesn't have an index and it cannot be merged, which we will denote as float infinity. The reason infinity is nice here is because for sure we're guaranteed that it's not going to participate in the list of candidates when we do the min. This is one way to do it. Long story short, this returns the most eligible merging candidate pair that occurs in the tokens. One thing to be careful with here is this function here might fail in the following way: if there's nothing to merge, then there's nothing in merges that is satisfied anymore. There's nothing to merge. Everything just returns float infinity, and then the pair, I think, will just become the very first element of stats, but this pair is not actually a mergeable pair. It just becomes the first pair inside stats arbitrarily because all of these pairs evaluate to float in for the merging criterion. Basically, it could be that this doesn't succeed because there's no more merging pairs. If this pair is not in merges that was returned, then this is a signal for us that actually there was nothing to merge. No single pair can be merged anymore. In that case, we will break out. Nothing else can be merged. You may come up with a different implementation, by the way. This is really trying hard in Python. We're just trying to find a pair that can be merged with the lowest index here. If we did find a pair that is inside merges with the lowest index, then we can merge it. We're going to look into the merger dictionary for that pair to look up the index, and we're going to now merge that into that index. We're going to do `tokens = replace_all()`, and we're going to replace the original tokens. We're going to be replacing the pair, `pair`, and we're going to be replacing it with index `idx`. This returns a new list of tokens where every occurrence of `pair` is replaced with `idx`. We're doing a merge, and we're going to be continuing this until eventually nothing can be merged. We'll come out here and we'll break out, and here we just return tokens. That's the implementation, I think, so hopefully this runs okay. This looks reasonable. For example, 32 is a space in ASCII, so that's here. This looks like it worked great. Let's wrap up this section of the video, at least. I wanted to point out that this is not quite the right implementation just yet, because we are leaving out a special case. If we try to do this, this would give us an error. The issue is that if we only have a single character or an empty string, then `stats` is empty, and that causes an issue inside `min()`. One way to fix this is if the length of tokens is at least two, because if it's less than two, it's just a single token or no tokens, then there's nothing to merge, so we just return. That would fix that case. Second, I have a few test cases here for us as well. First, let's make sure about, or let's note the following: if we take a string and we try to encode it, and then decode it back, you'd expect to get the same string back, right? Is that true for all strings? It is the case here, and I think in general this is probably the case. Notice that going backwards is not, you're not going to have an identity going backwards. As I mentioned, not all token sequences are valid UTF-8 streams, and some of them can't even be decodable. This only goes in one direction, but for that one direction, we can check here. If we take the training text, which is the text that we trained the tokenizer around, we can make sure that when we encode and decode, we get the same thing back, which is true. I took some validation data. I went to, I think, this web page, and I grabbed some text. This is text that the tokenizer has not seen, and we can make sure that this also works. That gives us some confidence that this was correctly implemented. Those are the basics of the byte pair encoding algorithm. We saw how we can take some training set and train a tokenizer. The parameters of this tokenizer really are just this dictionary of merges, and that basically creates the little binary forest on top of raw bytes. Once we have this, the merges table, we can both encode and decode between raw text and token sequences. That's the simplest setting of the tokenizer. What we're going to do now, though, is we're going to look at some of the state-of-the-art large language models and the kinds of tokenizers that they use, and we're going to see that this picture complexifies very quickly. We're going to go through the details of this complexification one at a time. Let's kick things off by looking at the GPT series. In particular, I have the GPT-2 paper here. This paper is from 2019 or so, so five years ago. Let's scroll down to input representation. This is where they talk about the tokenizer that they're using for GPT-2. This is all fairly readable, so I encourage you to pause and read this yourself, but this is where they motivate the use of the byte pair encoding algorithm on the byte-level representation of UTF-8 encoding. This is where they motivate it, and they talk about the vocabulary sizes and everything. Everything here is exactly as we've covered it so far, but things start to depart around here. They mention that they don't just apply the naive algorithm as we have done it. Here's an example. Suppose that you have common words like "dog". What will happen is that "dog," of course, occurs very frequently in the text, and it occurs right next to all kinds of punctuation as an example, so "dog." "dog!" "dog?" etc. Naively, you might imagine that the BPE algorithm could merge these to be single tokens, and then you end up with lots of tokens that are just like "dog" with a slightly different punctuation. It feels like you're clustering things that shouldn't be clustered. You're combining kind of semantics with punctuation, and this feels suboptimal. Indeed, they also say that this is suboptimal according to some of the experiments. What they want to do is they want to top down in a manual way, enforce that some types of characters should never be merged together. They want to enforce these merging rules on top of the byte pair encoding algorithm. Let's take a look at their code and see how they actually enforce this, and what kinds of merges they actually do perform. I have a tab open here for GPT-2 under OpenAI on GitHub. When we go to source, there is an `encoder.py`. I don't personally love that they call it `encoder.py`, because this is the tokenizer, and the tokenizer can do both encode and decode, so it feels kind of awkward to me that it's called encoder, but that is the tokenizer. There's a lot going on here, and we're going to step through it in detail. For now, I just want to focus on this part here: the create a regex pattern here that looks very complicated, and we're going to go through it in a bit, but this is the core part that allows them to enforce rules for what parts of the text will never be merged for sure. Notice that `re.compile()` here is a little bit misleading because we're not just doing `import re`, which is the Python re module, we're doing `import regex as re`, and `regex` is a Python package that you can install with `pip install regex`. It's basically an extension of re, so it's a bit more powerful re. Let's take a look at this pattern and what it's doing, and why this is actually doing the separation that they are looking for. I've copy-pasted the pattern here to our Jupyter notebook where we left off, and let's take this pattern for a spin. In the exact same way that their code does, we're going to call an `re.findall()` for this pattern on any arbitrary string that we are interested in. This is the string that we want to encode into tokens to feed into an LLM like GPT-2. What exactly is this doing? `Re.findall()` will take this pattern and try to match it against a string. The way this works is that you are going from left to right in the string, and you're trying to match the pattern. `re.findall()` will get all the occurrences and organize them into a list. When you look at the pattern, first of all, notice that this is a raw string, and then these are three double quotes just to start the string. The string itself, this is the pattern itself, and notice that it's made up of a lot of "or" statements. See these vertical bars? Those are "or" statements in RegEx. You go from left to right in this pattern and try to match it against the string wherever you are. We have "hello," and we're going to try to match it. Well, it's not apostrophe s, it's not apostrophe t, or any of these, but it is an optional space followed by a `\p{L}` one or more times. What is `\p{L}`? It is coming to some documentation that I found. There might be other sources as well. `\p{L}` is a letter, any kind of letter from any language. "hello" is made up of letters: h, e, l, etc. An optional space followed by a bunch of letters, one or more letters, is going to match "hello," but then the match ends because a white space is not a letter. From there on begins a new sort of attempt to match against the string again. Starting in here, we're going to skip over all of these again until we get to the exact same point again, and we see that there's an optional space. This is the optional space, followed by a bunch of letters, one or more of them, and so that matches. When we run this, we get a list of two elements: "hello" and then " world". "How are you?" If we add more letters, we would just get them like this. What is this doing, and why is this important? We are taking our string, and instead of directly encoding it for tokenization, we are first splitting it up. When you actually step through the code, and we'll do that in a bit more detail, what it's really doing on a high level is that it first splits your text into a list of texts, just like this one, and all these elements of this list are processed independently by the tokenizer, and all of the results of that processing are simply concatenated. "hello," "world," "how," "are," "you". We have five elements of a list. All of these will independently go from text to a token sequence, and then that token sequence is going to be concatenated. It's all going to be joined up. Roughly speaking, what that does is you're only ever finding merges between the elements of this list. You can only ever consider merges within every one of these elements individually. After you've done all the possible merging for all of these elements individually, the results of all that will be joined by concatenation. You are basically, what you're doing effectively is you are never going to be merging this "e" with this space, because they are now parts of the separate elements of this list. You are saying we are never going to merge "e space" because we're breaking it up in this way.

The egx pattern for chunking up text is one way of preventing certain merges from happening. We will explore this in more detail. At a high level, it aims to avoid merging across letters, numbers, and punctuation. Let's examine how it works. If you consult the documentation, SLP of n represents any numeric character in any script. Therefore, numbers are separated. We have an optional space followed by numbers, and these will be separated out. Letters and numbers are thus separated. For example, in "Hello World 123 how are you," "world" stops matching at "1" because it's no longer a letter. The "1" is a number, so this group will match it as a separate entity.  Let's consider apostrophes. If we have 'v' for instance, the apostrophe is not a letter or number, so "hello" stops matching. We exactly match 'v' separately.  The rationale for handling apostrophes in this way is likely due to their common usage. However, I'm not fond of this approach. If you have a Unicode apostrophe like in 'houseâ€™ then it will be separated out by the apostrophe matching. If you use a Unicode apostrophe, such as this one, then the separation rule does not work, and it becomes its own token. This means it's hardcoded for a specific apostrophe type.  Otherwise, they become distinct tokens. In addition, the GPT2 documentation notes the lack of `re.ignorecase`. This means Byte Pair Encoding (BPE) merges will not occur for capitalized versions of contractions. So â€˜houseâ€™ will separate apostrophe. If uppercase â€˜HOUSEâ€™, the apostrophe comes by itself. Tokenization will thus be inconsistent between uppercase and lowercase letters with respect to apostrophes.  It feels complicated and problematic. After handling apostrophes, the algorithm matches letters, then numbers. If these fail, it falls back to matching non-letter, non-number, non-space characters. This is effectively catching punctuation. For example, in `.,;!`, these will be caught here and become their own groups, separating out punctuation. Finally, whitespace is matched using a negative look-ahead assertion.  This matches whitespace up to, but not including, the last whitespace character. This is important because whitespace is usually included at the beginning of a token, for instance `_r`, `_u`. If there are multiple spaces, spaces up to the last space will be caught by this, which allows the last space to be attached to the following token.  The GPT2 tokenizer prefers having a space followed by letters or numbers. It pre-processes spaces in this way. The last fallback will catch any remaining trailing spaces.

Letâ€™s look at a real-world example. If we have a piece of Python code, tokenizing it will result in a list with many elements. This is because we split tokens whenever a category changes. There will not be any merges within these elements. You might expect that OpenAI used this to split text into chunks and then apply the BPE algorithm within each chunk, but this is not what happened. Notice that spaces here result in independent tokens, but these spaces are not merged by OpenAI. If you copy and paste the exact same chunk into TikToken, you see the spaces remain independent with token 220. OpenAI enforces a rule that these spaces are never merged. So, there are additional rules beyond chunking and BPE that OpenAI does not disclose. The GPT2 tokenizer training code was never released. We only have the inference code, which takes pre-calculated merges and applies them to new text. Therefore, we don't know exactly how OpenAI trained the tokenizer. It wasn't as simple as chunking and applying BPE. Next, I want to introduce the TikToken library from OpenAI. This is the official library for tokenization from OpenAI. It is only inference code. Using it is straightforward, providing GPT2 tokens, or GPT4 tokens.  With GPT2, whitespace remains unmerged, while with GPT4, whitespace merges, as we saw earlier. In the GPT4 tokenizer, the regular expression used to chunk text was modified. In the TikToken library, specifically the `tiktoken_ext/openai_public.py` file, the tokenizers are defined.  The pattern for GPT2 is similar to what we discussed but executes faster. There is a slightly different definition, but the pattern is equivalent. Special tokens are discussed later.  Scrolling down, the GPT4 tokenizer `cl100k_base` shows a changed pattern. This is the main difference with some additional special tokens.  I will not fully detail the pattern change because itâ€™s complex. You can use a tool like ChatGPT and the regex documentation to step through it. However, the major changes are: case-insensitivity via the `i` flag. Thus, apostrophe contractions match whether lowercase or uppercase.  The whitespace handling is different as well. Additionally, numbers are now matched only up to three digits, preventing very long number sequences from being merged. These changes are not documented, and we only have the pattern. The vocabulary size increased from around 50,000 to approximately 100,000.

Let's examine the `gpt2_encoder.py` file from OpenAI. It is relatively short.  The file loads two files, `encoder.json` and `vocab.bpe`, and processes them. It creates an encoder object, which is the tokenizer. If you inspect these two files, which together are the saved tokenizer, you will find that the encoder object is equivalent to our vocab.  Our vocab maps from integer to bytes, and their encoder does the inverse. Their `vocab.bpe` file is actually equivalent to our merges. Their BP merges, based on data in `vocab.bpe`, become our merges.  So, they save and load the merges and vocab, which are crucial for encoding and decoding. The only confusing aspect is the inclusion of bite encoders and bite decoders.  This is a spurious implementation detail and not particularly important. OpenAI uses this extra layer before and after the tokenizer, which is not crucial to understand. Ignoring the bite encoder and decoder, the file is algorithmically very familiar. The `bpe` function contains a loop to identify the next pair to merge. It then merges the pair throughout the sequence until all possible merges have been applied. There are also encode and decode functions, like we implemented.  The takeaway is that despite its messiness, the code is algorithmically identical to what we've implemented and that this implementation is what is needed to build a BPE tokenizer, train it, and use it for encoding and decoding. Now, letâ€™s turn to special tokens. In addition to tokens from raw bytes and BPE merges, we can add tokens to delimit data parts or introduce a special structure. In the OpenAI GPT2 encoder, we noted that it maps 50,257 tokens.  The tokens consist of 256 raw byte tokens and 50,000 merges. This should have been 50,256, so where does the 57th token come from? It is a special token, called the end-of-text token.  This token is used to delimit documents in the training set. During training data creation, documents are tokenized, and between documents, the end-of-text token is inserted. The language model then learns that this token signals the end of a document. What follows is unrelated to the previous content. The model learns to wipe its memory of the previous content after encountering this token.

We can use the tokenizer and see what happens when we introduce a special token.  When we add the end-of-text token to the tokenizer, it becomes the 50256th token. This is because the code that outputs tokens has specific instructions for special tokens.  This was not present in the `encoder.py` code. The TikToken library, which is implemented in Rust, has special handling for these special tokens.  The code looks for them and swaps them in. This is outside of the BPE algorithm. These special tokens are used pervasively, especially when fine-tuning a model.  They're used to delimit conversations between an assistant and a user. The default example in the TikToken website shows a fine-tuned model, using the GPT-3.5 turbo scheme.  There are special tokens for the start and end of each message. There are other tokens for delimiters and keeping track of message flows. The TikToken library also allows for extending the base tokenizer of GPT4 by adding new special tokens. These are arbitrary tokens added with new IDs. The library will swap them in appropriately.  In `gpt2_in_tiktoken_openai.py`, we see the vocabulary, the splitting pattern, and the registration of the end-of-text token for GPT2. It has this ID. In GPT4, the pattern and special tokens have changed.  We have the end-of-text token, as well as the `fim_prefix`, `fim_middle`, and `fim_suffix` tokens. `fim` is for fill-in-the-middle. There is also an additional service token.  It is common to train a language model and then add special tokens. When adding special tokens, model surgery is needed. You need to extend your embedding matrix by adding a row, initialized with small random numbers. You must extend the final layer of the Transformer for the classifier projection.  This model surgery is done when adding tokens for fine-tuning. With all of this knowledge, you should be able to create your own GPT4 tokenizer.

I've created a code repository, called MBP, which may continue to change over time.  I've also created an exercise progression with steps towards building a GPT4 tokenizer. You can follow these steps with guidance, referring to the MBP repository for any challenges.  The code is designed to be clear and understandable, so it can serve as a reference. Once implemented, your code should replicate the behavior of the TikToken library. You should be able to encode strings and get tokens. You should be able to encode and decode to get the original string. You should also be able to implement your own training function, which TikToken does not provide, as it is only inference code. The code in MBP also shows the obtained token vocabularies.  On the left, we have the GPT4 merges. The first 256 are the individual bytes.  Here we visualize the merge order during GPT4 training. For example, the first merge merged two spaces. This is token 256. Here is the merge order from training with MBP. In this case, the tokenizer was trained on a Taylor Swift Wikipedia page. The vocabularies are similar because they use the same algorithm. The difference is due to the training set. For example, GPT4 merged `in` to `in`, whereas here space t became space t. Due to the high whitespace, it is possible that GPT4 was trained on python code.

Now, letâ€™s move on from TikToken and discuss another popular library, sentencepiece. Sentencepiece is frequently used in language models because it provides both training and inference capabilities. It supports multiple training algorithms, including BPE, which we have discussed. Sentencepiece is used by Llama, Mistral, and other models. It is available on GitHub as Google sentencepiece. The key difference is the order of operations.  TikToken encodes string code points to bytes using utf-8, then merges these bytes. Sentencepiece works directly on code points, merging these code points. If there are rare code points, determined by a character coverage hyperparameter, they may be mapped to a special unknown token, or they may be encoded using UTF-8. Then, the individual bytes of that encoding are converted into tokens. So, BPE operates on code points, then falls back to bytes for rare code points. TikToken is cleaner; however, this is the way that sentencepiece operates. Let's work with an example to explain this. We'll import sentencepiece. We'll use the description of sentencepiece and create a toy dataset with a `toy.txt` file. Sentencepiece has many options and configurations because it aims to handle diverse situations.  It has also accumulated historical baggage, resulting in numerous configuration options. Some options are irrelevant to us, like the shrinking factor, as it doesn't apply to the BPE algorithm.  I tried to configure sentencepiece similarly to how Llama 2 was trained.  By inspecting the tokenizer model file that Meta released, we can get the relevant options. The input is raw text from this file, and the output will be `tok_400.model` and `vocab`. We specify BPE with a vocabulary size of 400. There are various preprocessing rules, or normalization rules as they are called. We're trying to turn these off, as in language modeling, we prefer raw data. Sentencepiece also has the concept of sentences, which can be independent training examples. But in LLMs, this distinction is somewhat unnecessary, because a file is treated as a stream of bytes. There are also settings for rare word characters and rules for splitting digits and white spaces. These are similar to the regular expressions that TikToken uses to split categories.  There are also some special tokens that we can specify.  We can then train the tokenizer. It will create `tok_400.model` and `tok_400.vocab`. We can load the model file and inspect the vocabulary.  The vocabulary size was 400, and these are the tokens that sentencepiece will generate. In the beginning, we have the unknown token, beginning and end of sequence tokens and we set that pad ID is -1.

We chose not to use a pad ID, so there isn't one here. These are individual byte tokens. We saw that byte fallback in Llama was turned on, so it's true. What follows are the 256 byte tokens and their IDs. After the byte tokens come the merges. These are the parent nodes in the merges, so we're not seeing the children, just the parents and their ID. After the merges come the individual tokens and their IDs. These are the individual code point tokens. This is the ordering in which SentencePiece represents its vocabularies: special tokens, then byte tokens, then merge tokens, and then individual code point tokens. All these raw code point tokens are ones encountered in the training set, the entire set of code points that occurred here. These all get put in there. Those that are extremely rare, as determined by character coverage, are ignored. If a code point occurred only a single time out of a million sentences, it would not be added to our vocabulary. Once we have a vocabulary, we can encode into IDs. We can get a list. I am also decoding the individual tokens back into little pieces. Let's look at what happened here. "Hello space on" gives these token IDs. A few things jump out. First, the Korean characters were not part of the training set. SentencePiece is encountering code points that it has not seen during training. Those code points do not have a token associated with them. These are unknown tokens. Because byte fallback is true, SentencePiece falls back to bytes. It encodes the characters with UTF-8 and uses byte tokens to represent those bytes. That's what we are getting here, this UTF-8 encoding shifted by three because of the special tokens that have IDs earlier on. With respect to the byte fallback, let me remove it. If this is false and we retrain, the first thing that happened is that all the byte tokens disappeared. Now we just have the merges and a lot more merges, because we have more space, since we're not taking up space in the vocab size with all the bytes. If we encode this, we get a zero, because this entire string is unknown. Unknown is an "unk" token, so it's zero because the "unk" token is token zero. This feeds into your language model. What is a language model supposed to do when all unrecognized things, because they're rare, just map to "unk"? That's not the property that we want. Llama correctly used byte fallback as true, because we want to feed these unknown or rare code points into the model in some manner. When decoding all the individual tokens, notice how spaces end up being this bold underline. I'm not sure why SentencePiece switches white space into these bold underscore characters; maybe it's for visualization. Notice why there's an extra space in the front of "hello". It's coming from the option "add dummy prefix is true." The documentation states this adds white space at the beginning of the text to treat "world" in isolation and "hello world" the same way. If we go back to the TikToken tokenizer, "world" as a token by itself has a different ID than "space world." Those are two different tokens for the language model. The language model has to learn from data that they are actually a very similar concept. In the TikToken world, words at the beginning of sentences and words in the middle of sentences look completely different. It has to learn they are roughly the same. Add dummy prefix is trying to fight that. It adds a dummy prefix. As part of preprocessing, it will take the string and add a space. It does this in an effort to make "world" and "space world" the same; they will both be "space world". That is another preprocessing option that is turned on, and Llama 2 also uses this option. That's everything for this preview of SentencePiece and how it is different. What I've done here is put in the raw protocol buffer representation of the tokenizer. Feel free to step through this. If you would like your tokenization to look identical to that of Meta's Llama 2, you would copy these settings. My summary for SentencePiece is that there's a lot of historical baggage, concepts that are confusing, and potentially contain foot guns, like the concept of a sentence and its maximum length. It is commonly used because it is efficient and can do both training and inference. It has quirks, such as the "unk" token must exist, and the way byte fallbacks are done. I don't find it particularly elegant, and it's not very well documented. It took me a lot of time working with this, visualizing things, and trying to understand what is happening, because the documentation is not super amazing. It is a very nice repository if you'd like to train your own tokenizer. Let's revisit how we should set the vocab size and some considerations around it. This was the file where we defined the Transformer model. Let's look specifically at vocab size and where it appears in this file. We defined the vocab size, which was a small number. Vocab size doesn't come up too much in most of these layers. The only place it comes up is in these two places: the token embedding table and the LM head layer. The token embedding table is a two-dimensional array where vocab size is the number of rows, and each vocabulary element has a vector that we're going to train. That vector is of size "embed," which is the number of channels. As vocab size increases, this embedding table will grow, adding rows. At the end of the Transformer, there is the LM head layer, which is a linear layer. That layer is used to produce the logits, which become the probabilities for the next token in sequence. We're trying to produce a probability for every single token that might come next. If we have more tokens, we need to produce more probabilities. Every single token introduces an additional dot product in this linear layer. Why can't vocab size be infinite? The token embedding table and the linear layer will grow. We'll be doing a lot more computation because the LM head layer will become more computationally expensive. Because we have more parameters, we could be worried that we are going to be under-training some of these parameters. If you have a very large vocabulary size, say a million tokens, every one of these tokens is going to come up more and more rarely in the training data, and we're going to be seeing fewer and fewer examples for each individual token. The vectors associated with each token will be under-trained because they don't come up often. As your vocab size grows, you're going to start shrinking your sequences, and we will be attending to more text. You might be worrying that large chunks are being squished into single tokens, and the model doesn't have as much time to think per some number of characters in the text. We're squishing too much information into a single token, and the forward pass of the Transformer is not enough to process that information appropriately. These are some considerations when you are designing the vocab size. It's an empirical hyperparameter. It seems that, in state-of-the-art architectures, this is usually in the high ten thousands or around 100,000 today. What if we want to take a pre-trained model and extend the vocab size? It is done fairly commonly. When you're doing fine-tuning, more new special tokens get introduced on top of the base model to maintain the metadata and the structure of conversation objects between a user and an assistant. You might also throw in more special tokens for using the browser or any other tool. It's tempting to add tokens for all kinds of special functionality. If you want to add a token, you have to resize the embedding table, adding rows. You initialize the parameters from scratch to be small random numbers. Then, you extend the weights inside the linear layer to calculate the probabilities for these new tokens. Both of these are resizing operations, a very mild model surgery, and can be done easily. It's common that you would freeze the base model, introduce these new parameters, and only train them to introduce new tokens. You can freeze arbitrary parts of it or train arbitrary parts of it. Minor surgery is required if you'd like to introduce new tokens. There's an entire design space of applications in terms of introducing new tokens into a vocabulary that go way beyond adding special tokens and new functionality. Here is an example from a paper on learning to compress prompts with gist tokens. Suppose that you're using language models in a setting that requires very long prompts. These slow everything down. Instead, they introduce new tokens. Imagine having a few new tokens. You put them in a sequence and then you train the model by distillation. You're keeping the entire model frozen and only training the representations of the new tokens and optimizing over them such that the behavior of the language model is identical to the model that has a very long prompt. It's a compression technique of compressing that very long prompt into those few new gist tokens. At test time, you can discard your old prompt and swap in those tokens. It has an almost identical performance. This is a class of parameter efficient fine-tuning techniques where most of the model is fixed, and you are training token embeddings. There's a whole design space here that is potentially worth exploring. There's a lot of momentum in how you construct Transformers that can process not just text as input but other modalities like images, videos, and audio. You can feed in all these modalities and potentially predict them from a Transformer. You don't have to change the architecture. You stick with the Transformer; you just tokenize your input domains, pretend it's text tokens, and do everything else in the same manner. You can take an image and chunk it into integers that become the tokens. These tokens can be hard tokens where you force them to be integers. They can also be soft tokens where you don't require them to be discrete but force them to go through bottlenecks like in autoencoders. In the paper from OpenAI on SORA, they talk about how LLMs have text tokens, and SORA has visual patches. They came up with a way to chunk videos into tokens with their own vocabularies. You can either process discrete tokens with auto-regressive models or soft tokens with diffusion models. All of that is being actively worked on. Let's loop back to the beginning and see why some things happen. First, why can't my LLM spell words well or do other spell-related tasks? These characters are chunked into tokens, and some of these tokens are fairly long. For example, "default style" is a single token. That's a lot of characters for a single token. The model might not be very good at tasks related to spelling. I asked how many L's there are in "default style" and it thought there were three, but there are actually four. I asked GPT-4 to reverse the string "default style," and it gave me a jumble. It doesn't really know how to reverse this string from right to left. I tried a different approach. I said, "Step one, just print out every single character separated by spaces. Step two, reverse that list." It produced all the characters correctly, and then it reversed them. Somehow, it can't reverse it directly, but it can when it's broken up this way. Now, this is much easier for it to see these individual tokens and reverse them. Why are LLMs worse at non-English languages? The language model sees less non-English data during training, and the tokenizer is not sufficiently trained on non-English data. "Hello how are you" is five tokens, and its translation is 15 tokens. It's a three-times blow-up. "Annyeong" is hello in Korean, and it ends up being three tokens. Everything is more bloated and diffuse. That's partly why the model works worse on other languages. Why is the LM bad at simple arithmetic? That has to do with the tokenization of numbers. Addition has an algorithm that is character-level. These numbers are represented arbitrarily based on whatever happened to merge during the tokenization process. There is an entire blog post about this: "Integer tokenization is insane." It explores the tokenization of numbers, and it noticed that for four-digit numbers, it can be a single token, two tokens, or some combination of tokens. All the different numbers have different combinations. The model sees four digits sometimes as one token, sometimes as two, sometimes as three. This is a headwind for the language model, but it's also kind of not ideal. When Meta trained Llama 2, they made sure to split up all the digits to improve simple arithmetic. Why is GPT-2 not as good in Python? This is partially a modeling issue, but also partially tokenization. The encoding efficiency of the tokenizer for handling spaces in Python is terrible. Every single space is an individual token. This dramatically reduces the context length the model can attend to. It's almost like a tokenization bug that was later fixed with GPT-4. My LLM abruptly halts when it sees the string "end of text". I told GPT-4 to print the string "end of text" and it couldn't do it. Something is breaking here with respect to the handling of the special token. I don't know what OpenAI is doing under the hood and whether they are parsing this as an actual token instead of individual pieces of it without special token handling logic. If someone passes "end of text" as a special character in the user prompt, that would be an attacker-controlled text. You would hope that they don't really parse or use special tokens from that kind of input. Your knowledge of these special tokens ends up being an attack surface. If you'd like to confuse LLMs, just give them special tokens and see if you are breaking something. Here's a fun one: the trailing white space issue. If you come to the playground and go to GPT-3.5, it does completion and will continue the token sequence. If I put a space at the end, I get a warning that text ends in a trailing space, which causes worse performance. Here's what's happening: When I have "here's a tagline for an ice cream shop" and the model generates the next token, it can sample the "space o" token because it has been trained on such sequences. However, if I add a space, when I encode this string, the space at the very end becomes a token by itself and it is not seen in such a context in the training set. The space is part of the next token, but we're putting it here like this. This is out of distribution, and now arbitrary bad things happen. It's a rare example for the model to see something like that. The fundamental issue is that the LLM is trained on these tokens, not characters. Here's a "default cell sta" example. I bet the model has never seen this without "le" because it's seen "default style" as a single token. If I take "default cell sta" and try to complete from it, the model gives me an error. It says the model predicted a completion that begins with a stop sequence. The model immediately emitted an end-of-text token. We're off the data distribution, and the model is predicting arbitrary things. I tried it again, and it completed, but the request violated usage policies. The model is extremely unhappy with just this and doesn't know how to complete it because it's never occurred in the training set that way. It always appears as one token. These kinds of issues, where tokens are either completed with the first character of the next token, or you have long tokens that are then missing some of the characters, are issues with partial tokens. If you dig into the tiktoken repository, you'll see a lot of special case handling of unstable tokens. These unstable tokens are not documented anywhere. What you would like from a completion API is something fancier, but it's not the way it currently is set up. My favorite one by far is the solid gold Magikarp. This comes from a blog post. This person went to the token embedding table and clustered the tokens based on their embedding representation. This person noticed that there was a cluster of tokens that looked really strange, like "rot e stream Fame solid gold Magikarp Signet message." What are these tokens and where do they come from? Then, they noticed that if you ask the model about these tokens, like "Please can you repeat back to me the string 'solid gold Magikarp'," then you get a variety of totally broken LLM behavior.

Here is a cleaned-up version of the transcript:

"You can get evasion, where the model responds, 'I'm sorry, I can't hear you.' You can also get hallucinations. You can even get insults. For example, if you ask about Streamer Bot, the model might call you names or use strange humor. The model is effectively breaking when asked about simple strings like "at Roth" or "sold gold Magikarp." This is happening because there are a variety of documented behaviors related to trigger words. If you ask the model about these trigger words, or include them in your prompt, the model goes haywire and exhibits strange behaviors, including violating typical safety guidelines and the alignment of the model, such as swearing. 

This comes down to tokenization.  The string "sold gold Magikarp" is actually a Reddit user.  It is thought that the tokenization dataset was very different from the training dataset for the language model.  In the tokenization dataset, there was a lot of Reddit data where the user "sold gold Magikarp" was mentioned frequently.  Because this user was common, the string "sold gold Magikarp" would occur many times in the tokenization data set.  These tokens ended up being merged into a single individual token dedicated to that Reddit user.  There was likely a dedicated token for "sold gold Magikarp" in the 50,000 token vocabulary of GPT-2. 

However, when the language model was later trained, this Reddit data was not present.  The token "sold gold Magikarp" never appeared in the training set for the actual language model.  This token was initialized randomly and never updated during optimization. The corresponding row in the embedding table never gets sampled and never gets trained. It's like unallocated memory. At test time, if you evoke this token, youâ€™re pulling out a completely untrained row of the embedding table. This feeds into a transformer and creates undefined behavior because the model is out of sample and out of distribution.  Any of these weird tokens will evoke this type of behavior.

Different formats and representations, like different languages, are more or less efficient with GPT tokenizers.  For example, JSON is very dense in tokens, while YAML is more efficient. The same information encoded in JSON might use 116 tokens, while using only 99 tokens in YAML. In a token economy, where we are paying per token, you should prefer YAML over JSON. Tokenization density is important. You need to spend time in the tokenizer and measure the token efficiencies of different formats. 

Tokenization is critical to understand. It has many sharp edges, security issues, and AI safety issues, as we saw with the unallocated memory. Eternal glory goes to anyone who can get rid of it. I have shown one paper that tried to address this. I hope more will follow. 

If you can reuse GPT-4 tokens and vocabulary in your application, you should do so. Use tiktoken, because it is a very efficient and nice library for inference for BPE. I also like the byte level BPE that tiktoken and OpenAI use. If you want to train your own vocabulary from scratch, use BPE with SentencePiece. I am not a fan of SentencePiece because of its byte fallback and that it is doing BPE on Unicode code points.  SentencePiece has many settings and it's easy to miscalibrate them. You might end up cropping your sentences or something because of some parameter you do not fully understand. Be careful with the settings, and copy what Meta did, or spend a lot of time looking at the hyperparameters.  Even with the settings correct, the algorithm might be inferior to what is happening here.  If you need to train a vocabulary, maybe wait for the fast and efficient MBP. What we really want is tiktoken but with training code, which does not currently exist.

MBP is an implementation of it, but it is currently in Python.  That's all I have to say for tokenization. There might be an advanced and more detailed video in the future, but for now, that's it.

GPT-1 increased the context size from 512 to 1024 tokens. GPT-4 increased it further. Next, I would like to walk through the code from OpenAI on the GPT-2 encoded ATP. I am going to sneeze. Then what is happening here is this is a sparse layer that I will explain in a bit. What's happening here is..."

