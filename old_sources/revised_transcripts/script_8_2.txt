The egx pattern for chunking up text is one way of preventing certain merges from happening. We will explore this in more detail. At a high level, it aims to avoid merging across letters, numbers, and punctuation. Let's examine how it works. If you consult the documentation, SLP of n represents any numeric character in any script. Therefore, numbers are separated. We have an optional space followed by numbers, and these will be separated out. Letters and numbers are thus separated. For example, in "Hello World 123 how are you," "world" stops matching at "1" because it's no longer a letter. The "1" is a number, so this group will match it as a separate entity.  Let's consider apostrophes. If we have 'v' for instance, the apostrophe is not a letter or number, so "hello" stops matching. We exactly match 'v' separately.  The rationale for handling apostrophes in this way is likely due to their common usage. However, I'm not fond of this approach. If you have a Unicode apostrophe like in 'house’ then it will be separated out by the apostrophe matching. If you use a Unicode apostrophe, such as this one, then the separation rule does not work, and it becomes its own token. This means it's hardcoded for a specific apostrophe type.  Otherwise, they become distinct tokens. In addition, the GPT2 documentation notes the lack of `re.ignorecase`. This means Byte Pair Encoding (BPE) merges will not occur for capitalized versions of contractions. So ‘house’ will separate apostrophe. If uppercase ‘HOUSE’, the apostrophe comes by itself. Tokenization will thus be inconsistent between uppercase and lowercase letters with respect to apostrophes.  It feels complicated and problematic. After handling apostrophes, the algorithm matches letters, then numbers. If these fail, it falls back to matching non-letter, non-number, non-space characters. This is effectively catching punctuation. For example, in `.,;!`, these will be caught here and become their own groups, separating out punctuation. Finally, whitespace is matched using a negative look-ahead assertion.  This matches whitespace up to, but not including, the last whitespace character. This is important because whitespace is usually included at the beginning of a token, for instance `_r`, `_u`. If there are multiple spaces, spaces up to the last space will be caught by this, which allows the last space to be attached to the following token.  The GPT2 tokenizer prefers having a space followed by letters or numbers. It pre-processes spaces in this way. The last fallback will catch any remaining trailing spaces.

Let’s look at a real-world example. If we have a piece of Python code, tokenizing it will result in a list with many elements. This is because we split tokens whenever a category changes. There will not be any merges within these elements. You might expect that OpenAI used this to split text into chunks and then apply the BPE algorithm within each chunk, but this is not what happened. Notice that spaces here result in independent tokens, but these spaces are not merged by OpenAI. If you copy and paste the exact same chunk into TikToken, you see the spaces remain independent with token 220. OpenAI enforces a rule that these spaces are never merged. So, there are additional rules beyond chunking and BPE that OpenAI does not disclose. The GPT2 tokenizer training code was never released. We only have the inference code, which takes pre-calculated merges and applies them to new text. Therefore, we don't know exactly how OpenAI trained the tokenizer. It wasn't as simple as chunking and applying BPE. Next, I want to introduce the TikToken library from OpenAI. This is the official library for tokenization from OpenAI. It is only inference code. Using it is straightforward, providing GPT2 tokens, or GPT4 tokens.  With GPT2, whitespace remains unmerged, while with GPT4, whitespace merges, as we saw earlier. In the GPT4 tokenizer, the regular expression used to chunk text was modified. In the TikToken library, specifically the `tiktoken_ext/openai_public.py` file, the tokenizers are defined.  The pattern for GPT2 is similar to what we discussed but executes faster. There is a slightly different definition, but the pattern is equivalent. Special tokens are discussed later.  Scrolling down, the GPT4 tokenizer `cl100k_base` shows a changed pattern. This is the main difference with some additional special tokens.  I will not fully detail the pattern change because it’s complex. You can use a tool like ChatGPT and the regex documentation to step through it. However, the major changes are: case-insensitivity via the `i` flag. Thus, apostrophe contractions match whether lowercase or uppercase.  The whitespace handling is different as well. Additionally, numbers are now matched only up to three digits, preventing very long number sequences from being merged. These changes are not documented, and we only have the pattern. The vocabulary size increased from around 50,000 to approximately 100,000.

Let's examine the `gpt2_encoder.py` file from OpenAI. It is relatively short.  The file loads two files, `encoder.json` and `vocab.bpe`, and processes them. It creates an encoder object, which is the tokenizer. If you inspect these two files, which together are the saved tokenizer, you will find that the encoder object is equivalent to our vocab.  Our vocab maps from integer to bytes, and their encoder does the inverse. Their `vocab.bpe` file is actually equivalent to our merges. Their BP merges, based on data in `vocab.bpe`, become our merges.  So, they save and load the merges and vocab, which are crucial for encoding and decoding. The only confusing aspect is the inclusion of bite encoders and bite decoders.  This is a spurious implementation detail and not particularly important. OpenAI uses this extra layer before and after the tokenizer, which is not crucial to understand. Ignoring the bite encoder and decoder, the file is algorithmically very familiar. The `bpe` function contains a loop to identify the next pair to merge. It then merges the pair throughout the sequence until all possible merges have been applied. There are also encode and decode functions, like we implemented.  The takeaway is that despite its messiness, the code is algorithmically identical to what we've implemented and that this implementation is what is needed to build a BPE tokenizer, train it, and use it for encoding and decoding. Now, let’s turn to special tokens. In addition to tokens from raw bytes and BPE merges, we can add tokens to delimit data parts or introduce a special structure. In the OpenAI GPT2 encoder, we noted that it maps 50,257 tokens.  The tokens consist of 256 raw byte tokens and 50,000 merges. This should have been 50,256, so where does the 57th token come from? It is a special token, called the end-of-text token.  This token is used to delimit documents in the training set. During training data creation, documents are tokenized, and between documents, the end-of-text token is inserted. The language model then learns that this token signals the end of a document. What follows is unrelated to the previous content. The model learns to wipe its memory of the previous content after encountering this token.

We can use the tokenizer and see what happens when we introduce a special token.  When we add the end-of-text token to the tokenizer, it becomes the 50256th token. This is because the code that outputs tokens has specific instructions for special tokens.  This was not present in the `encoder.py` code. The TikToken library, which is implemented in Rust, has special handling for these special tokens.  The code looks for them and swaps them in. This is outside of the BPE algorithm. These special tokens are used pervasively, especially when fine-tuning a model.  They're used to delimit conversations between an assistant and a user. The default example in the TikToken website shows a fine-tuned model, using the GPT-3.5 turbo scheme.  There are special tokens for the start and end of each message. There are other tokens for delimiters and keeping track of message flows. The TikToken library also allows for extending the base tokenizer of GPT4 by adding new special tokens. These are arbitrary tokens added with new IDs. The library will swap them in appropriately.  In `gpt2_in_tiktoken_openai.py`, we see the vocabulary, the splitting pattern, and the registration of the end-of-text token for GPT2. It has this ID. In GPT4, the pattern and special tokens have changed.  We have the end-of-text token, as well as the `fim_prefix`, `fim_middle`, and `fim_suffix` tokens. `fim` is for fill-in-the-middle. There is also an additional service token.  It is common to train a language model and then add special tokens. When adding special tokens, model surgery is needed. You need to extend your embedding matrix by adding a row, initialized with small random numbers. You must extend the final layer of the Transformer for the classifier projection.  This model surgery is done when adding tokens for fine-tuning. With all of this knowledge, you should be able to create your own GPT4 tokenizer.

I've created a code repository, called MBP, which may continue to change over time.  I've also created an exercise progression with steps towards building a GPT4 tokenizer. You can follow these steps with guidance, referring to the MBP repository for any challenges.  The code is designed to be clear and understandable, so it can serve as a reference. Once implemented, your code should replicate the behavior of the TikToken library. You should be able to encode strings and get tokens. You should be able to encode and decode to get the original string. You should also be able to implement your own training function, which TikToken does not provide, as it is only inference code. The code in MBP also shows the obtained token vocabularies.  On the left, we have the GPT4 merges. The first 256 are the individual bytes.  Here we visualize the merge order during GPT4 training. For example, the first merge merged two spaces. This is token 256. Here is the merge order from training with MBP. In this case, the tokenizer was trained on a Taylor Swift Wikipedia page. The vocabularies are similar because they use the same algorithm. The difference is due to the training set. For example, GPT4 merged `in` to `in`, whereas here space t became space t. Due to the high whitespace, it is possible that GPT4 was trained on python code.

Now, let’s move on from TikToken and discuss another popular library, sentencepiece. Sentencepiece is frequently used in language models because it provides both training and inference capabilities. It supports multiple training algorithms, including BPE, which we have discussed. Sentencepiece is used by Llama, Mistral, and other models. It is available on GitHub as Google sentencepiece. The key difference is the order of operations.  TikToken encodes string code points to bytes using utf-8, then merges these bytes. Sentencepiece works directly on code points, merging these code points. If there are rare code points, determined by a character coverage hyperparameter, they may be mapped to a special unknown token, or they may be encoded using UTF-8. Then, the individual bytes of that encoding are converted into tokens. So, BPE operates on code points, then falls back to bytes for rare code points. TikToken is cleaner; however, this is the way that sentencepiece operates. Let's work with an example to explain this. We'll import sentencepiece. We'll use the description of sentencepiece and create a toy dataset with a `toy.txt` file. Sentencepiece has many options and configurations because it aims to handle diverse situations.  It has also accumulated historical baggage, resulting in numerous configuration options. Some options are irrelevant to us, like the shrinking factor, as it doesn't apply to the BPE algorithm.  I tried to configure sentencepiece similarly to how Llama 2 was trained.  By inspecting the tokenizer model file that Meta released, we can get the relevant options. The input is raw text from this file, and the output will be `tok_400.model` and `vocab`. We specify BPE with a vocabulary size of 400. There are various preprocessing rules, or normalization rules as they are called. We're trying to turn these off, as in language modeling, we prefer raw data. Sentencepiece also has the concept of sentences, which can be independent training examples. But in LLMs, this distinction is somewhat unnecessary, because a file is treated as a stream of bytes. There are also settings for rare word characters and rules for splitting digits and white spaces. These are similar to the regular expressions that TikToken uses to split categories.  There are also some special tokens that we can specify.  We can then train the tokenizer. It will create `tok_400.model` and `tok_400.vocab`. We can load the model file and inspect the vocabulary.  The vocabulary size was 400, and these are the tokens that sentencepiece will generate. In the beginning, we have the unknown token, beginning and end of sequence tokens and we set that pad ID is -1.
