Okay, let's start. Today, we will continue our Zero to Hero series, and specifically, we will reproduce the 124 million parameter version of the GPT-2 model. OpenAI released GPT-2 in 2019, along with a blog post, a research paper, and code on GitHub. When we talk about reproducing GPT-2, we need to be careful because in this video, we are reproducing the 124 million parameter model. There was a miniseries of GPT-2 models released at different sizes. Typically, the largest model is called "GPT-2." The reason for this miniseries is to plot model size on the x-axis and downstream metrics on the y-axis to chart scaling laws. As model size increases, performance on downstream metrics improves. The GPT-2 paper includes four models, from 124 million to 1558 million parameters. These numbers disagree with the actual model sizes found on the GitHub repository, due to an error in the paper. The 124 million parameter model has 12 layers in the Transformer and 768 channels. I am assuming familiarity with these terms from my previous video, “Let's Build GPT From Scratch." If everything goes well, by the end of this video, we should see the validation loss decrease as the model improves at predicting the next token in a sequence on validation data. We will hopefully beat the original 124 million parameter GPT-2 model. Five years ago, this was a complex optimization, and compute resources were smaller. Today, you can reproduce this model in about an hour for roughly 10 dollars on a cloud compute service. You can achieve a model as good as the one released by OpenAI. Unlike many other models, OpenAI did release the weights for GPT-2 in their repository. The GPT-2 paper does not contain all the training details. We will also refer to the GPT-3 paper, which provides more concrete details on hyperparameters and optimization settings. The GPT-3 architecture is not a huge departure from the GPT-2 model. So, we will use both papers to reproduce the GPT-2 124M model. First, let's load the GPT-2 124M model as released by OpenAI and sample some tokens. The original GPT-2 code was written in TensorFlow, which is not as commonly used today. We want to use PyTorch because it is easier and more user-friendly. However, the initial code is in TensorFlow. To get our target model, we will use the Hugging Face Transformers library. They have a PyTorch implementation of GPT-2, including converted weights from TensorFlow, making it easier to load and work with. We can load the GPT-2 model using Hugging Face Transformers by importing `GPT2LMHeadModel` and loading the `pretrained` model with the string "gpt2." When you load "gpt2," you get the 124 million parameter model. To get the 1.5 billion parameter model you would load "gpt2-xl." From this loaded model, we can obtain the state dictionary, which is a raw set of tensors. This state dictionary is just a dictionary, so we can print the keys and the shapes of the corresponding tensors. The weight for the token embedding is of size 50,257 by 768, reflecting the 50,257 tokens in the GPT-2 vocabulary and a 768-dimensional embedding for each. The tokenization was discussed in detail in the previous videos on tokenization. This is the lookup table for tokens, where each token has a corresponding 768-dimensional vector. We also have the lookup table for positions, which, with a maximum sequence length of 1024, has 1024 positions, each with a fixed 768-dimensional vector learned during optimization. This is the position embedding. The remaining weights and biases belong to the Transformer. If we take the positional embeddings, flatten them out, and select the first 20 elements, we can see the parameters. These parameters, when plotted, show structure, with each row representing a different position in the sequence from 0 to 1023. The representation of each position creates a learned sinusoid, allowing the Transformer to understand relative token positions. If we look at individual columns in the positional embeddings, representing specific channels, we see that different channels respond to different parts of the position spectrum. This can look noisy, suggesting the model is not fully trained. The more trained the model, the smoother these curves should be. These curves do not have to be smooth, however, and should be just random noise at the start of optimization. The fact that they do have structure is a result of the optimization. The original Transformer paper used fixed sinusoids for positional encoding, but in GPT-2 these are trained from scratch, which works just as well, with these sinusoids being recovered during the optimization. We can also examine other matrices within the model, and see some structure. We are primarily interested in loading the weights from the released model using Hugging Face Transformers. We can use their pipeline to sample text. Using the prefix "hello, I am a language model," and sampling 30 tokens for 5 different sequences, it produces coherent text. I tried this example and sadly the results were slightly different from their example, presumably due to changes in the code. The important thing is that we are generating coherent text. We loaded the model, examined its parameters, and can generate text. We want to create our own GPT-2 class so we have a full understanding of what is happening. We do not want to use the Hugging Face implementation, so we will write this from scratch. Our first task will be loading the GPT-2 124M parameters into our class, giving us confidence that we can recreate the model. We will initialize the model from scratch and train it on documents, hoping to surpass the released model. We will obtain different weights but maintain the confidence that we can recreate the original model. So, let's write the GPT-2 model and load the weights to generate coherent text. Let's go back to the "Attention is All You Need" paper, which introduced the original Transformer. GPT-2 is a modified version of the Transformer. It is a decoder-only Transformer, missing the encoder. Therefore, the cross-attention mechanism, which used the encoder, is also missing. The rest of the model remains almost the same but with a few differences. In the GPT-2 paper, we see that the layer normalizations have been reshuffled and an additional layer normalization was added to the final self-attention block. Instead of being after the MLP or after attention, the layer normalizations are placed before it. An extra layer norm is added right before the classifier. Let's implement the skeleton NN modules in our GPT NN module. We will try to match the schema used by Hugging Face Transformers to make it easier to load weights from the state dictionary. The main container module is called "Transformer," and we reflect this using an NN module dict. This module allows indexing into sub-modules using string keys. Inside, we have "wte" for token embeddings, which is an NN embedding, and "wpe" for position embeddings, also an NN embedding. An NN embedding is a wrapper module around a tensor that allows access to its rows by indexing. In addition, we have "h," which is indexed using numbers, 0, 1, 2, up to 11, since there are 12 layers in the Transformer. We will use a module list to reflect that, allowing us to index using integers. The module list contains 'n_layer' blocks. We also need an additional final layer norm following the GPT-2 paper, and the final classifier, the language model head, which projects from 768 dimensions to the vocabulary size (50,257). GPT-2 uses no bias in this projection. This skeleton reflects the hugging face schema: WTE is token embeddings, PE is position embeddings, H is the layers, LNF is the added final normalization layer, and LM head is the linear output projection. Let’s implement the block. The block's forward pass is defined here. There is another change mentioned in the GPT-2 paper: layer normalizations are after attention or the feed forward network. Additionally, the normalizations are inside the residual stream. This places normalizations inside the residual pathway, which is not ideal. Ideally, the residual pathway should be clean from the top to the inputs. The gradient flows through the top and down to the inputs, unchanged through the residual pathway. The gradients also flow through the blocks, so the optimization changes over time, but a clean residual pathway is desirable for optimization purposes. In the pre-normalization version, "x" goes through layer normalization first, then attention, then another layer norm, then multi-layer perceptron (feed-forward), which feeds into the residual stream. Attention is a communication operation where all the tokens exchange information, acting as a pooling function. The MLP, on the other hand, occurs at every token individually, with no communication between tokens. Attention is the reduce, and the MLP is the map. The Transformer is a repeated application of map-reduce operations. The blocks refine the representation within the residual stream. This is our block implementation, slightly modified from the original. Let's look at the MLP block, implemented with two linear projections and a GELU nonlinearity. The GELU is approximated using tanh in GPT-2. In the PyTorch documentation, there are two versions of GELU: the original and the approximate. The original GELU is more like a ReLU with a smooth curve, it is described mathematically in the original paper. The approximate version was developed at the time because it was faster to compute in TensorFlow, even if it is less accurate. Today, there is no real reason to use the approximate version, but we want to reproduce GPT-2, so we stick with the approximate GELU. Using GELU over ReLU helps avoid the dead neuron problem, where ReLU neurons may have no gradient, as the GELU always has a local gradient, allowing change and adaptation. GELU is also empirically better. More modern networks use other variants, but for GPT-2, the approximate GELU is used. Finally, there is the attention operation. This is a multi-headed attention operation where, in the previous video, we showed that these heads function in parallel. Each head output is concatenated, creating the output of multi-headed attention. Here, instead of separate modules, we will put everything into a single self-attention module. We use a series of transpose and split tensor operations for efficiency, but the underlying algorithmic process is the same. Each token emits a query, key, and value vector. The queries and keys interact to give an attention weight. An autoaggressive mask prevents tokens from attending to tokens in the future. Then, a softmax normalizes these attention weights. The weighted sum of the value vectors generates a weighted representation of tokens found relevant. Then, another series of transpose/view operations reassembles all of the separate heads into a single output, effectively concatenating them. While this is more efficient in PyTorch, it is mathematically equivalent to our previous implementation. I am being careful with the variable names, such as 'c_attn', which should match 'attn'. These keys should align with the hugging face transformer code, making it easy to load weights. At this point, we have completed the GPT-2 implementation. This code is much shorter than the hugging face code. We can load the weights, set them, and then try generation. Let's see what that looks like. I've also changed the GPT config so that the numbers agree with the GPT-2 124M model. The block size is 1024. There are 50,257 tokens, including the merges and a special end-of-text token, 12 layers, 12 attention heads, and a 768 dimension for the transformer. Here is how we can load parameters from Hugging Face into our code and initialize the GPT class with these parameters. I am copying and pasting some code, which is not particularly interesting, but it loads the weights. There are four models in the miniseries. This is the Jupyter code from earlier. These are the hyper parameters of the GPT-2 models. We are creating a config object, creating our model, and then we're creating a state dictionary, for both our model and for the hugging face model. Then, we are looping over the hugging face model keys and copying those tensors, ignoring some of the buffers like attention biases, which are used for the auto regressive mask. One additional issue is that some of the weights are transposed, from the TensorFlow format, so I have manually hardcoded which weights to transpose. We return this model. The `from_pretrained` is a class method that returns a GPT object when given a model type, which in our case is "gpt2". This code loads the weights and biases into our `nn.Module`. Let’s test if this is working by generating from this model. Before generating, we need to forward the model, which we haven’t implemented yet. Here is the forward function: The input to the forward function is going to be the indices which represent our tokens. They are always of the shape 'B x T'. Where B is the batch dimension and T is the time dimension. T cannot be more than the block size (maximum sequence length). The indices are a two dimensional layout. Each row is a sequence of length up to the block size, and we have B independent sequences in a batch.
