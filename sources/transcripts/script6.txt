Okay, everyone, by now you have probably heard of ChatGPT. It has taken the world and the AI community by storm. It is a system that allows you to interact with an AI and give it text-based tasks. For example, we can ask ChatGPT to write a short haiku about the importance of people understanding AI so they can use it to improve the world and make it more prosperous. When we run this, we get, "AI knowledge brings prosperity for all to see. Embrace its power." Not bad. You can see that ChatGPT generated these words sequentially, from left to right.
I asked it the exact same prompt a little earlier, and it generated a slightly different outcome: "AI's power to grow; Ignorance holds us back, learn. Prosperity waits." Both are pretty good and slightly different. You can see that ChatGPT is a probabilistic system. For any one prompt, it can give us multiple answers.
This is just one example. People have created many, many examples. There are entire websites that index interactions with ChatGPT. Many of them are quite humorous, such as "Explain HTML to me like I'm a dog," "Write release notes for chess 2," and "Write a note about Elon Musk buying Twitter."
As another example, please write a breaking news article about a leaf falling from a tree: "In a shocking turn of events, a leaf has fallen from a tree in the local park. Witnesses report that the leaf, which was previously attached to a branch, detached itself and fell to the ground. Very dramatic." You can see that this is a pretty remarkable system.
It is what we call a language model because it models the sequence of words, characters, or more generally, tokens. It knows how words follow each other in the English language. From its perspective, it is completing a sequence. I give it the start of a sequence, and it completes the sequence with the outcome. It is a language model in that sense.
I would like to focus on the under-the-hood components of what makes ChatGPT work. What is the neural network under the hood that models the sequence of these words? It comes from a paper called "Attention is All You Need," published in 2017. This landmark paper in AI proposed the Transformer architecture. GPT is short for Generatively Pre-trained Transformer. The Transformer is the neural network that does all the heavy lifting under the hood. It comes from the 2017 paper.
The paper reads like a pretty random machine translation paper. I think the authors didn't fully anticipate the impact the Transformer would have on the field. This architecture, produced in the context of machine translation, ended up taking over the rest of AI in the next five years. This architecture, with minor changes, was copy-pasted into a huge amount of applications in AI in recent years. It is at the core of ChatGPT.
We are not going to reproduce ChatGPT. It is a serious, production-grade system. It is trained on a good chunk of the internet, with a lot of pre-training and fine-tuning stages. It is very complicated. I would like to focus on training a Transformer-based language model. In our case, it will be a character-level language model. I think that is very educational regarding how these systems work. We will not train on a chunk of the internet. We need a smaller data set. I propose that we work with my favorite toy dataset called "Tiny Shakespeare."
It is a concatenation of all the works of Shakespeare. This file is about one megabyte and is just all of Shakespeare. We are going to model how these characters follow each other. For example, given a chunk of characters like this, given some context of characters in the past, the Transformer neural network will look at the highlighted characters and predict that "g" is likely to come next in the sequence. We will train the Transformer on Shakespeare. It will try to produce character sequences that look like this. In that process, it will model all the patterns inside this data.
Once we have trained the system, we can generate infinite Shakespeare. Of course, it is fake but looks kind of like Shakespeare. Apologies for some jank that I have not been able to resolve. You can see how it goes character by character, predicting Shakespeare-like language. For instance, "verily my Lord the sites have left again the king coming with my curses with precious pale," and then Tranos says something else. This is coming out of the Transformer in a manner similar to how it would in ChatGPT. In our case, it is character by character. In ChatGPT, it is token by token. Tokens are sub-word pieces, not word-level but more like word chunk level.
I have already written all of the code to train these Transformers. It is in a GitHub repository called nanogpt. nanogpt is a repository for training Transformers on any given text. There are many ways to train Transformers, but this is a very simple implementation. It consists of two files of 300 lines of code each. One file defines the GPT model, the Transformer, and one file trains it on a given text dataset.
I am showing that if you train it on an Open Web Text dataset, which is a fairly large dataset of webpages, then I reproduce the performance of GPT-2. GPT-2 is an earlier version of OpenAI GPT from 2017, if I recall correctly. I have only reproduced the smallest 124-million-parameter model so far, but this proves that the codebase is correctly arranged. I can load the neural network weights that OpenAI released later. You can take a look at the finished code in nanogpt.
I would like to write this repository from scratch in this lecture. We will begin with an empty file and define a Transformer piece by piece. We will train it on the Tiny Shakespeare dataset. Then we will see how we can generate infinite Shakespeare. This can be copy-pasted to any arbitrary text dataset. My goal is to help you understand and appreciate how ChatGPT works under the hood. All that is required is proficiency in Python, a basic understanding of calculus and statistics, and it would help if you have seen my previous videos, in particular, my "Make More" series. Those videos define smaller and simpler neural network language models, like multi-layer perceptrons, and introduce the language modeling framework. In this video, we will focus on the Transformer neural network itself.
I created a new Google Colab Jupyter notebook. This will allow me to share the code with you. This will be in the video description. I downloaded the Tiny Shakespeare dataset from this URL. It is about a one-megabyte file. I open the "input.txt" file and read all of the text as a string. We see that we are working with roughly one million characters. The first 1,000 characters, if we print them, are what you would expect. These are the first 1,000 characters of the Tiny Shakespeare data set, roughly up to here. So far, so good.
Next, we take this text, which is a sequence of characters in Python. When I call the `set` constructor, I get the set of all characters that occur in this text. Then, I call `list` on that to create a list of those characters instead of a set, so that I have an arbitrary ordering. Then, I sort that list. We get all of the characters that occur in the data set, sorted. The number of them will be our vocabulary size. These are the possible elements of our sequences. When I print the characters, there are 65 of them. There is a space character, all kinds of special characters, and upper and lowercase letters. That is our vocabulary and the possible characters the model can see or emit.
Next, we need a strategy to tokenize the input text. When people say tokenize, they mean converting the raw text string to a sequence of integers according to a vocabulary of possible elements. As an example, we are building a character-level language model. We will translate individual characters into integers. I will show you a chunk of code that does that for us. We are building both the encoder and decoder.
When we encode an arbitrary text, like "hi there," we will receive a list of integers that represents that string, for example, 46, 47, etc. We also have the reverse mapping, so we can take this list and decode it to get back the exact same string. It is a translation to integers and back for any arbitrary string. For us, it is done on a character level.
We achieve this by iterating over all the characters and creating a lookup table from the character to the integer and vice versa. To encode a string, we translate all the characters individually. To decode it, we use the reverse mapping and concatenate everything. This is only one of many possible encodings or tokenizers. It is a very simple one, but there are many other schemas. For example, Google uses SentencePiece, which also encodes text into integers using a different schema and vocabulary. SentencePiece is a sub-word tokenizer. It encodes text into sub-word units, not entire words or individual characters. This is what is typically used in practice.
OpenAI also has a library called "tiktoken" that uses a byte pair encoding tokenizer. This is what GPT uses. You can also encode words into a list of integers, such as "hello world." Using tiktoken, I get the encoding for GPT-2. Instead of 65 possible characters or tokens, they have 50,000 tokens. When they encode "hi there," we only get a list of three integers. Those integers are not between 0 and 64 but between 0 and 50,256. You can trade off the codebook size and sequence lengths. You can have very long sequences of integers with small vocabularies or short sequences with large vocabularies. People typically use these sub-word encodings. I will keep our tokenizer simple. We are using a character-level tokenizer. That means we have very small code books and simple encode and decode functions. We get very long sequences as a result. We will stick with this level for this lecture because it is the simplest thing.
Now that we have an encoder and decoder, effectively a tokenizer, we can tokenize the entire training set of Shakespeare. Here is a chunk of code that does that. I will start to use the PyTorch library, specifically `torch.tensor`. We will take all of the text in Tiny Shakespeare, encode it, and wrap it into a `torch.tensor` to get the data tensor. Here is what the data tensor looks like when I look at the first 1,000 elements. We see a massive sequence of integers. This sequence of integers is an identical translation of the first 1,000 characters here. I believe, for example, that zero is a new line character and maybe one is a space, but I'm not 100% sure. From now on, the entire data set of text is re-represented as a single, very large sequence of integers.
Let me do one more thing before we move on. I will separate our dataset into a train and validation split. We will take the first 90% of the dataset and consider that the training data for the Transformer. We will withhold the last 10% as validation data. This will help us understand to what extent our model is overfitting. We want to hide and keep the validation data aside. We don't want a perfect memorization of Shakespeare. We want a neural network that creates Shakespeare-like text. It should be fairly likely to produce actual Shakespeare text. We will use this to get a sense of overfitting.
Now we would like to start plugging these text sequences or integer sequences into the Transformer so that it can train and learn the patterns. The important thing to realize is that we are never going to feed entire text into a Transformer all at once. That would be computationally expensive and prohibitive. When we train a Transformer on these data sets, we only work with chunks of the dataset. We sample random little chunks out of the training set and train on just chunks at a time. These chunks have a length, a maximum length. The maximum length, in my code, is typically called `block_size`. You can find it under other names, like context length. Let's start with a block size of eight. Let me look at the first training data characters, the first `block_size + 1` characters. I'll explain why plus one in a second. This is the first nine characters in the sequence in the training set.
When you sample a chunk of data like this, these nine characters out of the training set, it actually has multiple examples packed into it. All of these characters follow each other. When we plug this into a Transformer, we will train it to make a prediction at every one of these positions simultaneously. In a chunk of nine characters, there are actually eight individual examples. There's the example that when in the context of 18, 47 likely comes next. In a context of 18 and 47, 56 comes next. In a context of 18, 47, and 56, 57 can come next, and so on. There are eight individual examples.
Let me spell it out with code. X is the input to the Transformer. It will be the first `block_size` characters. Y will be the next `block_size` characters, offset by one. Y is the target for each position in the input. I am iterating over the `block_size` of eight. The context is always all of the characters in X up to T, including T. The target is always the Tth character but in the targets array Y. Let me run this, and it spells out what I said. These are the eight examples hidden in a chunk of nine characters that we sampled from the training set.
We train on all eight examples with context between one and up to `block_size`. It is not just for efficiency. It makes the Transformer Network used to seeing contexts all the way from as little as one up to the `block_size`. We like the Transformer to be used to seeing everything in between. This will be useful later during inference. While sampling, we can start the sampling generation with as little as one character of context. The Transformer knows how to predict the next character with a context of one. It can predict everything up to `block_size`. After `block_size`, we have to start truncating because the Transformer will never receive more than `block_size` inputs when it is predicting the next character.
We have looked at the time dimension of the tensors that will be fed into the Transformer. There is one more dimension to care about, and that is the batch dimension. As we sample these chunks of text, every time we feed them into a Transformer, we will have many batches of multiple chunks of text stacked up in a single tensor. That is for efficiency so that we can keep the GPUs busy. They are very good at parallel processing of data, so we want to process multiple chunks at the same time. Those chunks are processed completely independently.
Let me generalize this and introduce a batch dimension. Here is a chunk of code. I will run it and then explain what it does. Because we will sample random locations in the dataset to pull chunks from, I am setting the seed for the random number generator. The numbers I see here will be the same numbers you see if you reproduce this. The batch size is how many independent sequences we are processing every forward and backward pass of the Transformer. The `block_size`, as I explained, is the maximum context length to make the predictions.
Let's say the batch size is four and the block size is eight. Here is how we get a batch for any arbitrary split. If the split is a training split, then we look at train data. Otherwise, we look at valid data. That gives us the data array. When I generate random positions to grab a chunk out of, I generate a `batch_size` number of random offsets. Because this is four, `xs` will be four numbers randomly generated between zero and the length of the data minus the `block_size`. These are random offsets into the training set.
X's, as I explained, are the first `block_size` characters starting at I. The Y's are offset by one. Then, we will get those chunks for every integer I in `xs`. We use `torch.stack` to take all of those one-dimensional tensors and stack them up as rows. They become a row in a 4x8 tensor. When I sample a batch, `XB` and `YB`, the inputs to the Transformer are the input X, a 4x8 tensor, four rows of eight columns. Each one of these is a chunk of the training set. The targets are in the associated array, Y. They will come to the Transformer at the end to create the loss function. They will give us the correct answer for every single position inside X.
These are the four independent rows. Spelled out as we did before, this 4x8 array contains a total of 32 examples. They are completely independent. When the input is 24, the target is 43, or rather, 43 in the Y array. When the input is 24 and 43, the target is 58. When the input is 24, 43, and 58, the target is 5, etc. You can see this spelled out. These are the 32 independent examples packed into a single batch of the input X. The desired targets are in Y. This integer tensor of X will feed into the Transformer, which will simultaneously process all these examples and look up the correct integers to predict in every position of the tensor Y.
Now that we have a batch of input that we would like to feed into a Transformer, let's start feeding this into neural networks. We will start with the simplest possible neural network, which, in the case of language modeling, is the bigram language model. We have covered the bigram language model in my "Make More" series. Here, I will go faster. Let's implement a PyTorch module directly that implements the bigram language model. I am importing the PyTorch `nn` module for reproducibility. Here, I am constructing a bigram language model, which is a subclass of `nn.Module`. I call it and pass it the inputs and targets. I am just printing it now. When the inputs and targets come here, you see that I am taking the indices X, which I rename to `idx`, and passing them into this token embedding table.
Here, in the constructor, we are creating a token embedding table of size `vocab_size` by `vocab_size`. We are using an `nn.Embedding`, which is a thin wrapper around a tensor of shape `vocab_size` by `vocab_size`. When we pass `idx` here, every integer in our input refers to this embedding table. It plucks out a row of that table corresponding to its index. 24 here will go into the embedding table and pluck out the 24th row. 43 will go here and pluck out the 43rd row, etc. PyTorch will arrange all of this into a batch by time by channel tensor. In this case, the batch is four, the time is eight, and C, the channels, is `vocab_size`, or 65. We pluck out all those rows and arrange them in a B by T by C. We interpret this as the logits, which are the scores for the next character in the sequence. We are predicting what comes next based on the individual identity of a single token. Currently, the tokens are not talking to each other. They are not seeing any context except for themselves. Knowing they are token number five can make decent predictions about what comes next. Some characters follow other characters in typical scenarios. We saw this in a lot more depth in the "Make More" series. If I run this, we get the predictions, the scores, the logits, for every one of the 4x8 positions.
Now that we have made predictions about what comes next, we would like to evaluate the loss function. In the "Make More" series, we saw that a good way to measure loss is to use the negative log-likelihood loss, which is also implemented in PyTorch under the name `cross_entropy`. We would like the loss to be the cross-entropy of the predictions and the targets. This measures the quality of the logits with respect to the targets. We have the identity of the next character. How well are we predicting the next character based on the logits? Intuitively, the correct dimension of the logits, depending on whatever the target is, should have a very high number, and all the other dimensions should be very low numbers.
Unfortunately, this will not run. We get an error message. Intuitively, we want to measure this. When we go to the PyTorch `cross_entropy` documentation, we see that PyTorch expects certain input dimensions. If you have multi-dimensional input, which we do because we have a B by T by C tensor, it wants the channels to be the second dimension. It wants a B by C by T instead of a B by T by C. It is just the details of how PyTorch treats these kinds of inputs. We do not want to deal with that. We will reshape our logits. I like to take the logits' shape, which is B by T by C, unpack those numbers, and then say that logits equals `lit.view` We want it to be a B * T by C, a two-dimensional array. We take all of the positions here and stretch them out in a one-dimensional sequence and preserve the channel dimension as the second dimension. We are stretching out the array so it is two-dimensional. It will better conform to what PyTorch expects in its dimensions.
We have to do the same to targets. Currently, targets are of shape B by T, and we want it to be B * T, one-dimensional. Alternatively, you could do minus one because PyTorch will guess what this should be if you want to lay it out. Let me be explicit and say B * T. Once we have reshaped this, it will match the cross-entropy case, and we should be able to evaluate our loss. We can run that. The loss is 4.87. Because we have 65 possible vocabulary elements, we can guess what the loss should be. In particular, we covered negative log likelihood in detail. We expect the log of 1 over 65 and the negative of that. We expect the loss to be about 4.17. We are getting 4.87. That tells us that the initial predictions are not super diffuse. They have a little bit of entropy, and so we are guessing wrong. We are able to evaluate the loss.
Now that we can evaluate the quality of the model, we would also like to generate from the model. Let's generate now. I will go a little bit faster here because I covered all of this in previous videos. Here is a generate function for the model. We take an input `idx`. This is the current context of some characters in a batch, so it is B by T. The job of generate is to take this B by T and extend it to be B by T+1, +2, +3. It continues the generation in all batch dimensions in the time dimension. It does that for `max_new_tokens`. You can see that whatever is predicted is concatenated on top of the previous `idx` along the first dimension, the time dimension, to create a B by T+1. That becomes a new `idx`. The job of generate is to take a B by T and make it B by T + 1 + 2 + 3, as many as we want, `max_new_tokens`. This is the generation from the model.
Inside the generation, we take the current indices and get the predictions. Those are in the logits. The loss is ignored here because we are not using it. We have no ground truth targets that we are comparing with. Once we get the logits, we focus on the last step. Instead of a B by T by C, we pluck out the -1, the last element in the time dimension. Those are the predictions for what comes next. This gives us the logits, which we then convert to probabilities via softmax. Then we use `torch.multinomial` to sample from those probabilities, asking PyTorch to give us one sample. `idx_next` will become a B by one because, in each of the batch dimensions, we will have a single prediction for what comes next. The `num_samples=1` makes this one. We take those integers from the sampling process according to the probability distribution given here. Those integers are concatenated on top of the current running stream of integers. This gives us a B by T+1. Then we can return that.
You see how I am calling `self(idx)`, which will go to the forward function. I am not providing any targets. Currently, this would give an error because targets are not given. Targets have to be optional, so targets are none by default.
Okay, let's begin. There is no loss to create if there are no targets, so the loss is none. However, if there are targets, then we can calculate a loss. This setup allows us to obtain a loss when targets are provided, and if no targets are provided, we will simply get the logits. This code will generate output from the model.
I have another code chunk here, which will generate output from the model. Let me break this down. The `idx` variable is used to create a batch, which will be of size one. This is a one-by-one tensor holding a zero. The data type is an integer. The zero will be used to kick off the generation. Remember that zero represents a new line character, so it is a reasonable starting character. We feed `idx` into the model, requesting 100 tokens. The `generate` function will continue generating. Since `generate` operates on batches, we index into the zero-th row to remove the batch dimension. This gives us a one-dimensional array of indices. We convert this array, which is a PyTorch tensor, into a Python list. This list is then passed into our decode function to convert the integers into text. We are generating 100 tokens. When we run this, we get garbage output because the model is currently random.
We will want to train the model to make the output less random. This function is written to be general, but it is somewhat inefficient. We are building out the context and feeding it all into the model. This is unnecessary for the current simple bigram model. To make a prediction about K, we only need W. However, we are feeding the entire sequence into the model and only looking at the last piece. The reason for this is that while this is a bigram model now, I want to keep this function fixed so that it will work later when characters look further into the history. Right now, the history is not used, so this looks silly, but eventually, the history will be used, which is why we implement it this way. 
Now that we have seen that the model is random, let's train it. First, I will create a PyTorch optimization object. Here, we are using the Adam optimizer. In past videos, we used stochastic gradient descent (SGD). Adam is a more advanced and popular optimizer. A typical learning rate setting for Adam is 3e-4. However, for very small networks, we can use much higher learning rates, like 3e-3 or higher. Let me create the optimizer object that will take the gradients and update the parameters. Then, the batch size above was four, so let's use 32. For some number of steps, we sample a new batch of data, evaluate the loss, zero out the gradients, get the gradients for all parameters, and update the parameters. This is a typical training loop.
Let's run this for 100 iterations. The loss starts around 4.7 and goes down to 4.6 and 4.5, so optimization is definitely happening. Let's try to increase the number of iterations and only print at the end. We likely need to train for longer. We are now roughly down to three. It's working. Let's just do 10,000 iterations. Hopefully, we'll get something reasonable. It won't be Shakespeare, but at least the loss is improving. We are expecting something more reasonable. Okay, we are down to about 2.5. Here is the output, a dramatic improvement over what we had before. Let's increase the number of tokens. We see that we are starting to get something at least reasonable. The model is making progress. That is the simplest possible model.
Now, the tokens are not talking to each other. Given the previous context, we are only looking at the last character to make predictions. We want these tokens to start talking to each other and figuring out the context. This will enable better predictions. This will start us on the path to creating a Transformer. I took the code from the Jupyter notebook and converted it into a script. This is to simplify the work into a final product. At the top, I put the hyperparameters. I introduced a few, which I will discuss. Otherwise, a lot of this should be recognizable. We have reproducibility, data reading, encoder and decoder creation, train-test splits, and the data loader to get batches of inputs and targets. This data loader part is new and I will discuss it in a bit. We also have the bigram language model that we developed which can forward, give us logits and loss, and generate. The optimizer is created and we have the training loop.
Some small changes were added. First, I added the ability to run on a GPU if you have one. If you have a GPU, this will use CUDA instead of just the CPU, making everything much faster. If the device is CUDA, then when we load the data, we move it to the device. When we create the model, we need to move the model parameters to the device. For example, the embedding table has a weight inside that stores the lookup table, which would be moved to the GPU so the calculations happen there. When creating the context that feeds into generate, we must create it on the device. Second, in the training loop, instead of printing the loss item inside the loop, which can be noisy, I have added an `estimate_loss` function. This function averages the loss over multiple batches. We iterate `eval_iters` times and get the loss to obtain the average loss for both splits. This will be less noisy and give a more accurate train and validation loss. We set the model to evaluation phase before calling `estimate_loss` and reset it to training phase afterwards. Our current model does nothing in evaluation phase but it is good practice to think through which mode your neural network is in as some layers will have different behavior at inference and training time. There's also the context manager `torch.no_grad` which tells PyTorch not to call backward on code within it so that PyTorch can be more memory efficient. This is a good practice when we don't intend to do back propagation.
Right now, this script is about 120 lines of code, and this is our starting code, called `b.py`. Running this script gives the output in the terminal, showing the train loss, validation loss, and the sample produced at the end. We are now packaged into a script and are ready to iterate. Before writing our first self-attention block, I want to show a mathematical trick that is used in self-attention and is at the heart of an efficient implementation. Let's work with a toy example to understand this operation. We create a tensor with dimensions B by T by C, where B, T, and C are 4, 8, and 2, respectively. These represent batches, time components, and channels. Currently, the eight tokens are not interacting with each other, but we want to couple them. The token at the fifth location should not communicate with future tokens, such as the sixth, seventh, or eighth positions. Instead, it should only communicate with tokens in the fourth, third, second, and first positions. Information should only flow from the past to the current time step.
The easiest way for tokens to communicate is to average the preceding elements. For the fifth token, we would take the channels of that token, plus the channels from the fourth, third, second, and first steps, and average them. This would be a feature vector summarizing the token in the context of its history. Averaging is a weak form of interaction, and a lot of information about the spatial arrangement is lost, but that's okay for now. We are going to calculate the average of all the vectors in all the previous tokens and at this token for every single batch element independently for every token in the sequence. I have a small snippet here that I will paste. We will create `x_bag`, short for bag of words, and it will be initialized to zero. We iterate over all batch dimensions and then over time. The previous tokens are at the current batch dimension and everything up to and including the current time step.
When we slice `X` this way, `x_prev` becomes of shape how many T elements there were in the past, and then C, the two-dimensional information. That's the previous chunk of tokens in the sequence. We take the mean over the zero dimension. This means we're averaging over time. The result is a one-dimensional vector, `c`, which we store in `x_bag`. `x_bag` at the first location is equal to the initial token, because it is averaging just one token. The second location averages the first two tokens and so on, so the last is the average of all elements. This is all well and good, but very inefficient.
The trick is that we can be very efficient using matrix multiplication. I have a toy example here with a 3x3 matrix of all ones called `a`, a 3x2 matrix of random numbers called `b`, and a matrix `c` which will be the result of matrix multiplication `a` and `b`. This is `a` times `b` giving us `c`. The numbers in `c` are calculated through dot products between rows of `a` and columns of `b`. The top left element of `c` is the dot product of the first row of `a` and the first column of `b`. Because `a`'s first row is all ones, this operation sums the first column of `b`. Similarly, the top right number is the sum of the second column of `b`. Because rows of `a` are all ones we get repeating sums. The trick is this, the matrix `a` is a boring array of all ones, but torch has this function called `tril` which is short for triangular, and it returns a lower triangular portion of a matrix. Let's use this `tril` on our array of ones. What happens if we do this?
We have an `a` like this, and a `b` like this. What do we get for `c`? The first number is the first row of `a` times the first column of `b`. Because of the zeros, we are ignoring the zeros and only take the first element of `b`, which is two. The second number of `c` is the first row of `a` times the second column of `b`, which is the first number, seven. We are plucking out the row of `b`. Now, with one-one-zero, we get two plus six, which is eight, and seven plus four, which is 11. With 111, we get the sum of all of them. Depending on how many ones and zeros we have, we're summing a variable number of rows, and this gets deposited into `c`. We are doing sums because these are ones, but we can do averages. You can see how we could average the rows of `b` in an incremental fashion. If we normalize these rows so they sum to one, we can get an average. If we divide `a` by the sum of a along the first dimension, and keep that dimension, then the rows now sum to one.
If we do the matrix multiply again, we get the first row, then the average of the first two rows, and the average of these three rows. By manipulating the elements of this multiplying matrix and multiplying it with any given matrix, we can do these averages in an incremental fashion because we can manipulate that. That's very convenient. Let's go back up and see how we can vectorize this and make it much more efficient. We produce an array `a`, which I'm calling `we`, short for weights. This is our `a`, and this is how much of every row we want to average. Because the rows sum to one, it will be an average. Our `b` is `X`. What will happen here now is that we will have an `x_bag2`. `x_bag2` will be `we` multiplying `X`. `we` is T by T, and `X` is B by T by C. PyTorch creates a batch dimension, applying the matrix multiplication in all batch elements individually in parallel. For each batch element, there will be a T by T multiplying T by C, which will create B by T by C. `x_bag2` is identical to `x_bag`.
`torch.allclose` should return true, convincing us that these are, in fact, the same. If I print `x_bag` and `x_bag2` they are the same. The trick was using batched matrix multiply to do this aggregation. It's a weighted aggregation. The weights are specified in the T by T array. We are doing weighted sums. The sums take a triangular form. A token at the Tth dimension will only get information from tokens preceding it.
I'd like to rewrite this one more way. It is identical to the first and second, but it uses softmax. `Tril` is the lower triangular ones matrix, and `we` begins as all zeros. If we print `we` in the beginning, it's all zero. Then, I use masked fill. This says to make the elements of `we` equal to negative infinity if the elements of `tril` are equal to zero. All the elements where `tril` is zero become negative infinity. Then, we take the softmax along every single row. Softmax is a normalization operation, and it will result in the exact same matrix. Recall that softmax will exponentiate every element and divide by the sum. If we exponentiate here, we will get one, then zeroes everywhere else. Normalizing gives one here. If we exponentiate the second row, we will get one one, and then zeroes. Softmax will again divide, giving 0.5 and 0.5. We are producing the same mask in a different way.
The reason this is more interesting is that these weights here start at zero, and they represent an interaction strength or affinity. They tell us how much of each token from the past we want to aggregate and average. This line is saying tokens from the past cannot communicate, so we will not aggregate anything from those tokens. This then goes through softmax and weighted aggregation through matrix multiplication. These zeros are currently set to zero, but these affinities between tokens are not going to be constant. They will be data dependent. Tokens will look at each other, finding some more or less interesting. Depending on values, tokens will find each other interesting to different degrees. I will call these affinities. We clamp the future tokens, and then when we normalize and sum, we aggregate the values depending on how interesting they find each other. That is a preview of self-attention. The long story short is you can do weighted aggregations of past elements using a matrix multiplication and a lower triangular mask. Elements in the lower triangular part tell you how much of each element fuses into a given position. We will use this to develop the self-attention block.
Let's get some preliminaries out of the way. I am bothered that we are passing `vocab_size` into the constructor. There is no need, since it's defined as a global variable. Let's create a level of indirection where we do not directly go from the embedding to the logits. We will introduce a variable, `n_embed`, which is short for the number of embedding dimensions. We set this to 32. It was a suggestion from GitHub Copilot. This is an embedding table with only 32-dimensional embeddings. This will give us token embeddings and not logits directly. To get from embeddings to logits, we need a linear layer. `self.lm_head` will be the language modeling head, which is `nn.Linear` from `n_embed` up to `vocab_size`. When we forward, we will get logits. We have to be careful because the channel dimension here is `n_embed`, while here it is `vocab_size`. I will set `n_embed` equal to `C`, creating a single, spurious layer of interaction with a linear layer. This should run, and it does. Currently, this looks kind of spurious, but we'll build on top of this.
Next, we will encode the positions of the tokens. We add a second position embedding table, called `self.position_embedding_table`. This will have a size of block size by `n_embed`. Each position from zero to `block_size` minus one will get its own embedding vector. We decode the batch and time dimensions from `idx.shape`. We also have `pos_embedding`, which will be the positional embeddings. This will arrange integers from zero to T-minus-1. The integers are then embedded through the table, creating a tensor of T by C. `X` is then renamed to just `x` and becomes the addition of the token embeddings with the positional embeddings. The broadcasting rules work out. The result, `x`, now holds not just the token identities, but also the positions at which the tokens occur. This is not useful currently, since we have a simple bigram model. The model is translation invariant, so the information doesn't help. However, as we work on the self-attention block, we will see why this matters.
Here we get to the crux of self attention. This is likely the most important part of this video. We will implement a small self-attention head. We will start where we were. All of this is familiar. We have a 4x8 arrangement of tokens and information for each token is 32-dimensional. We have seen that our code does a simple average of all past and present tokens. This is achieved by creating a lower triangular structure that masks out our weight matrix. We mask it out, normalize it and the weights in this weight matrix have these uniform numbers. The matrix multiply makes it so that we are doing a simple average. We don't want this to be uniform, because different tokens will find different other tokens more or less interesting. We want that to be data dependent. If I'm a vowel, maybe I'm looking for consonants. I want that information to flow to me. We want to gather information in a data dependent way, and this is the problem self-attention solves.
Every token will emit two vectors: a query and a key. The query is roughly "what am I looking for?" and the key is roughly "what do I contain?". We calculate the affinities by taking the dot product between the keys and the queries. My query dot products with all the keys of all the other tokens. The dot products become `we`. If a key and query are aligned, they will interact to a high amount, and we will learn more about that specific token. Let's implement this now. We will implement a single head of self-attention. There's a hyperparameter involved here called the head size. We initialize linear modules with `bias=False`. These will apply a matrix multiply with fixed weights. Let's produce `k` and `q` by forwarding these modules on `x`. The size of each of these becomes B by T by 16 because that is our head size. You can see that when I forward this linear module on top of `x`, it becomes B by T by 16.
All the tokens in all the positions in the B by T arrangement produce a key and a query in parallel and independently. No communication has happened yet. All the queries will do a dot product with all the keys. We want the affinities between these to be the query multiplying the key. We cannot matrix multiply this directly; we need to transpose the key. We also need to be careful of the batch dimension. Specifically, we want to transpose the last two dimensions, -2 and -1. This matrix multiplication will do B by T by 16 matrix multiplies B by 16 by T, giving us B by T by T. For every row of B, we will have a T-squared matrix giving us the affinities. These are the weights, not zeros, coming from this dot product between the keys and the queries. The weighted aggregation is a data-dependent function between the keys and queries of these nodes. Before, the weight was a constant applied the same way to all the batch elements. Now, every single batch element will have a different weight because every single batch element contains different tokens at different positions. This is now data-dependent. For the zeroth row of the input, these are the weights that came out and they are not uniform. For example, the last row, the eighth token, knows what content it has and its position. The eighth token creates a query based on that. For example, “I'm a vowel in the E position, looking for a consonant at positions up to four.” All the nodes emit keys. One channel could be "I am a consonant, and I am in a position up to four.” That key would have a high number in that specific channel. When the query and key do a dot product, they can find each other and create a high affinity. If a high affinity exists, say this token was interesting to the eighth token, then, through the softmax, we aggregate a lot of its information into the position, allowing us to learn a lot about it. This is looking at the weight after this has already happened. Let’s erase the masking and the softmax to see the under-the-hood internals. Without masking and softmax, the weight comes out like this. These are the raw outputs of the dot products. They range from negative two to positive two. These are the raw interactions and raw affinities between all the nodes. If we are the fifth node, we do not want to aggregate anything from the sixth, seventh, or eighth nodes. So, we use upper triangular masking; those are not allowed to communicate. We want a nice distribution so we do not aggregate a negative value. Instead, we exponentiate and normalize to get a distribution that sums to one. This tells us, in a data-dependent manner, how much information to aggregate from any of these past tokens. This is the weight, and it is not zeros, but calculated this way. There is one more part to a single self-attention head. When we do the aggregation, we do not aggregate the tokens directly. We produce a value. In the same way that we produced the key and query, we create a value. Instead of aggregating X, we calculate a V, which is achieved by propagating a linear layer on top of X. We output the weight multiplied by V. V is the elements, the vectors, that we aggregate, instead of the raw X. This makes the output of this single head 16-dimensional, because that is the head size. X is like private information to this token. If you think of it this way, X is private to the token. The fifth token has some identity, and that information is in vector X. For the purposes of the single head, this is what the token is interested in, and if you find it interesting, this is what it will communicate to you. That information is stored in V. V is what gets aggregated for this single head between the different nodes. That is the self-attention mechanism. Attention is a communication mechanism. You can think of it as a communication mechanism where you have nodes in a directed graph with edges pointing between the nodes. Every node has a vector of information. It aggregates information via a weighted sum from all the nodes that point to it. This is done in a data-dependent manner. Our graph has a different structure. We have eight nodes because the block size is eight. There are always eight tokens. The first node is only pointed to by itself. The second node is pointed to by the first node and itself, and so on up to the eighth node, which is pointed to by all the previous nodes and itself. This is the structure of the directed graph in an autoregressive scenario like language modeling. In principle, attention can be applied to any arbitrary directed graph. It is just a communication mechanism between the nodes. There is no notion of space. Attention acts over a set of vectors in this graph. By default, the nodes have no idea where they are positioned. We encode them positionally to give them some information anchored to a specific position, so they know where they are. This is different than convolution. Convolution acts on the information in space. Attention is a set of vectors out in space that communicate. If you want them to have a notion of space, you need to add it. That is what we did when we calculated the positional encodings and added that information to the vectors. The elements across the batch dimension, which are independent examples, never talk to each other. They are always processed independently. This is a batched matrix multiply that applies matrix multiplication in parallel across the batch dimension. In this directed graph analogy, we have four separate pools of eight nodes because the batch size is four. Those eight nodes only talk to each other. In total, there are 32 nodes being processed in four separate pools of eight. In language modeling, the future tokens will not communicate to the past tokens. This doesn't have to be the constraint in the general case. In many cases, you may want to have all of the nodes talk to each other fully. For example, in sentiment analysis, you may want all the tokens to talk to each other because you are predicting the sentiment of the sentence. In those cases, you would use an encoder block of self-attention. An encoder block allows all the nodes to completely talk to each other. What we are implementing here is called a decoder block because it is a decoding language, an autoregressive format, where you mask with a triangular matrix. Nodes from the future never talk to the past because they would give away the answer. In encoder blocks, all the nodes talk. In decoder blocks, this triangular structure is present. Attention supports arbitrary connectivity between nodes. Self-attention means that the keys, queries, and values all come from the same source, X. The nodes are self-attending. In principle, attention is more general. In encoder-decoder transformers, the queries are produced from X, but the keys and values come from a separate external source. Sometimes, they come from encoder blocks that encode context. Cross-attention is used when there is a separate source of nodes to pull information from. Self-attention is when nodes look at each other. Our attention here is self-attention, but attention is more general. In the “Attention Is All You Need” paper, we have already implemented the attention. Given query, key, and value, we multiply the query and key, soft-max it, and aggregate the values. We are missing dividing by one over the square root of the head size. They call this scaled attention. It is important normalization because, if you have unit Gaussian inputs, then the variance of the weight will be on the order of the head size, which is 16. If you multiply by one over the square root of the head size, then the variance will be one. The weight feeds into the softmax, so it is important that we are fairly diffuse. Because of softmax, if the weight takes on very positive and negative numbers, then softmax will converge towards one-hot vectors. If applying softmax to small values, you get a diffuse thing. If you sharpen it by multiplying by eight, the softmax will start to sharpen towards the max. We don't want the values to be extreme at initialization. Otherwise, softmax will be too peaky, and each node will aggregate information from a single node. The scaling is used to control variance at initialization. In the code, the “head” module implements a single head of self-attention. You give it a head size, and it creates the key, query, and value linear layers, which typically do not have biases. These are the linear projections applied to the nodes. The “Trill” variable is not a parameter of the module; it is a buffer. You must assign it to the module using “register buffer.” This creates the lower triangular matrix. Given the input X, we calculate the keys and queries. We calculate the attention scores in the weight. We normalize it using scaled attention. We mask the future from the past, which makes it a decoder block. Then softmax and aggregate the value and output. In the language model, a head is created in the constructor. The head size is the same as the embedding, for now. Once we have encoded the information, we feed it into the self-attention head. The output goes into the decoder language modeling head to create the logits. This is the simplest way to plug in a self-attention component into the network. We must ensure that the input "idx" to the model has no more than the block size, or the positional embedding table will run out of scope. We crop the context to ensure no more than the block size elements are passed in. The learning rate was decreased, and the number of iterations was increased. We saw a little bit of improvement, going from 2.5 to 2.4, but the text is still not amazing. The self-attention head is doing some communication. There is still a long way to go. We have implemented scaled dot product attention. In the “Attention is All You Need” paper, there is multi-head attention. Multi-head attention is just applying multiple attentions in parallel and concatenating their results. It is multiple attentions in parallel. To implement multi-head attention, we create multiple heads, and we run them in parallel into a list and concatenate their outputs over the channel dimension. Instead of having one communication channel with a head size of 32, we have four communication channels in parallel. Each of these communication channels will be smaller. Because we have four channels, we have eight-dimensional self-attention. From each channel, we get eight-dimensional vectors, and the four are concatenated to give 32, the original embedding. This is like a group convolution. Instead of one large convolution, we do convolution in groups. This is multi-headed self-attention. We use the self-attention heads instead. Multi-head attention improves the validation loss from 2.4 to 2.28. The generation is still not amazing, but the tokens have a lot to talk about. They want to find the consonants, vowels, and different things. Creating multiple channels helps to gather data and decode the output. We have implemented positional encodings, token encodings, addition, and masked multi-headed attention. There is a feed-forward part. This is a simple multi-layer perceptron. The position-wise feed-forward networks are a simple MLP. We will add computation into the network. This computation is on a per-node level. The multi-headed self-attention did the communication, but we went too fast to calculate the logits. The tokens looked at each other but didn't think about what they found. We implemented a small, single-layer feed-forward. It is a linear followed by a ReLU nonlinearity. The feed-forward is called sequentially after the self-attention. The self-attention is the communication. Once they have gathered the data, they need to think on it individually. That is what feed-forward is doing. When trained, the validation loss went down to 2.24 from 2.28. The output still looks terrible, but we have improved the situation. We are interspersing the communication with the computation. That is also what the Transformer does. We have a block that intersperses communication and then computation. The communication is done using multi-headed self-attention, and computation is done with the feed-forward network on all tokens independently. The block uses the embedding dimension and number of heads, which is like group size in group convolution. The number of heads is four. Because this is 32, the head size should be eight. This is how the Transformer structures the sizes. The head size will be eight. We want to intersperse them. We create blocks, which are a sequential application of the block, to intersperse communication and feed-forward many times and decode. This does not actually give a very good answer. We are starting to get a pretty deep neural net. Deep neural nets suffer from optimization issues. We need one more idea from the transformer paper to resolve those difficulties. Skip connections dramatically help with the depth of these networks and make sure that the networks remain optimizable. These are skip or residual connections. You transform the data, but have a skip connection with addition from the previous features. The computation happens from top to bottom. You have a residual pathway and can fork off from it, perform computation, and project back via addition. You go from input to targets only via plus. During back propagation, addition distributes gradients equally to both branches. Supervision, or gradients from the loss, hop through every addition node to the input and fork off into the residual blocks. This gradient Super Highway goes directly from the supervision to the input unimpeded. The blocks are initialized in the beginning, so they contribute very little to the residual pathway. They are initialized that way. In the beginning, they are almost not there. They come online over time. This helps with optimization. To implement this in our block, we do x = x + self-attention and x = x + self.feed-forward. X forks off for communication, then comes back. X forks off for computation, then comes back. Those are the residual connections. We also introduce the projection, which is a linear transformation of the concatenated self-attention outputs and projects back into the residual pathway. In feed-forward, it is the same thing. We have a self.projection. I simplified it by coupling it inside the same container. This is the projection layer back to the residual pathway. The inner layer of the feed-forward network should be multiplied by four in terms of channel sizes. We multiply the embedding by four for the feed-forward and then project back down to the embedding for the projection. The validation loss goes down to 2.08. The network is getting big enough that the train loss is ahead of the validation loss. We see a little overfitting. Generations are still not amazing, but we are seeing some patterns. The second innovation helpful for deep neural networks is layer norm. Layer norm is in pytorch, similar to batch norm. Batch norm ensured that across the batch dimension, any individual neuron had a unit Gaussian distribution; zero mean and unit standard deviation. We copied the batch norm 1D from the make more series. It guarantees that, if we look at the zeroth column, it is zero mean and one standard deviation. It is normalizing every single column. Rows are not normalized. To implement layer norm, we change from zero to one. Now, we are normalizing the rows. Columns are not normalized. Rows are normalized for every individual example. This 100-dimensional vector is normalized this way. Our computation does not span across examples, so we can delete all the buffers stuff. We don't maintain any buffers and don’t need the distinction between training and test time. We keep the gamma and beta. We don’t care about momentum. This is a layer norm that normalizes the rows instead of the columns. Before incorporating layer norm, I noted that very few details about the transformer have changed in the last five years, but this slightly departs from the original paper. The add and norm are applied after the transformation, but it is more common to apply the layer norm before the transformation. This is the pre-norm formulation. We need two layer norms: layer norm one and layer norm two. The layer norms are applied immediately on X, before self-attention and feed-forward. The size of the layer norm is the embedding size, 32. When the layer norm normalizes our features, the mean and variance are taken over 32 numbers. The batch and time act as batch dimensions. The normalization is per-token, which normalizes features and makes them unit Gaussian at initialization.
Layer Norms, with their gamma and beta training parameters, may eventually create outputs that are not unit Gaussian, but the optimization will determine that. For now, this incorporates the layer norms. Let's train them. After running, we see a loss of 2.06, which is better than the previous 2.08, a slight improvement by adding layer norms. I expect they would help even more with a bigger and deeper network. I also forgot to add a layer norm at the end of the Transformer, right before the final linear layer that decodes into the vocabulary. I've added that as well. At this stage, we have a pretty complete Transformer according to the original paper. It’s a decoder-only Transformer, which I’ll explain shortly. The major pieces are now in place. We can try to scale this up and see how well we can push this number.
To scale the model, I made some cosmetic changes. I introduced the variable `n_layer` to specify how many layers of blocks we'll have, created a bunch of blocks, and also have a new variable for `number_of_heads`. I pulled out the layer norm. This part is now identical. I also added dropout. Dropout can be added right before the residual connection, before the connection back into the residual pathway, after the multi-headed attention, and after the softmax when calculating affinities. This randomly prevents some nodes from communicating. Dropout, from a 2014 paper, randomly shuts off a subset of neurons during each forward and backward pass and trains without them. Because the mask of what's being dropped out changes every forward and backward pass, it trains an ensemble of sub-networks. At test time, everything is enabled, merging all sub-networks into a single ensemble. I recommend reading the paper for full details. For now, it's a regularization technique, added because I'm about to scale up the model and was concerned about overfitting.
Looking at the hyperparameters, the batch size is now 64, and the block size is now 256, previously eight. So, we have 256 characters of context to predict the 257th character. The learning rate was brought down a bit, due to the now much larger neural network. The embedding dimension is 384, and there are six heads. Thus, each head is 64-dimensional. There will be six layers and the dropout rate is 0.2, so 20% of intermediate calculations are disabled each pass. I already trained this, with a validation loss of 1.48, which is a significant improvement from 2.07. This ran for about 15 minutes on my A100 GPU. This would not be reproducible on a CPU or MacBook, you would need to break down the number of layers and the embedding dimension. With that time we got this result.
I printed some Shakespeare, but then printed 10,000 characters to a file. The output looks more like the input text file, with someone speaking in a certain manner. The predictions take on that form, though nonsensical when read. It's a Transformer trained on a character level for 1 million Shakespeare characters. It blabbers in a Shakespeare-like manner, but doesn't make sense at this scale. I think it's still a good demonstration.
That concludes the programming section of this video. We did a good job implementing the Transformer, but the picture doesn't exactly match what we've done.  We implemented a decoder-only Transformer, with no encoder and no cross-attention block. Our block has only self-attention and the feed-forward network. We are missing the cross-attention piece. We have a decoder only because we are just generating text, unconditioned on anything. We're blabbering according to a given data set.  The triangular mask makes it a decoder, allowing us to sample from it. It can be used for language modeling. The original paper had an encoder-decoder architecture because it is a machine translation paper.  It takes tokens that encode French, for example, and decodes the translation in English. Typically, there are special tokens at the beginning. We are expected to condition on the input and start generation with a special “start” token, followed by the output, and a special “end” token to finish the generation. The decoding part is the same as what we did. However, they condition the generation on the French sentence. The encoder reads the French part, creates tokens, and puts a Transformer on it, but with no triangular mask. All tokens can talk to each other. It encodes the content of the French sentence. The outputs feed into the decoder, which does the language modeling. There's an additional connection through a cross-attention block. The queries are from X, and the keys and values are from the encoder, the keys and values feed in on the side into every single block of the decoder. The decoder is conditioned on the fully encoded French prompt. This is why we have the two Transformers, with the additional block, and why we did not do this, because we have no conditioning information. We only have a text file and want to imitate it; that's why we are using a decoder-only Transformer as is done in GPT.
I want to briefly walk through nanogPT, found on my GitHub. There are two files of interest: `train.py` and `model.py`. `train.py` is all the boilerplate code for training, the training loop from before, just more complicated with saving, loading, checkpoints, pre-trained weights, decaying learning rates, compiling the model, and using distributed training. The model.py should look very familiar. The model is almost identical. We have the causal self-attention block, which should be very recognizable. We produce queries, keys, and values, do dot products, masking, apply softmax, optionally dropout, and pull the values. In my code, the multi-headed attention was separated into individual heads, then I concatenate them. Here, it's implemented in a batched manner inside a single causal self-attention block, with a fourth "heads" dimension. It is mathematically equivalent but more efficient. We also have the multi-layer perceptron, using the GeLU nonlinearity, because OpenAI uses it and I want to be able to load their checkpoints. The blocks of the Transformer are identical, and the GPT will be as well. We have position encodings, token encodings, blocks, the layer norm at the end, and the final linear layer. This should look very recognizable. There is more here, because I'm loading checkpoints and separating parameters into those that should be weight-decayed and those that shouldn't. The generate function should be very similar. Some details differ, but you should be able to understand it.
Now let's bring things back to ChatGPT. To train it yourself, there are roughly two stages: pre-training and fine-tuning. In pre-training, you train on a large chunk of internet to get the first decoder-only Transformer to babble text, very similar to what we've done, but we've done a tiny baby pre-training. The Transformer I created had about 10 million parameters, trained on 1 million characters. Opening AI’s vocabulary is different; they use sub-word chunks of words with roughly 50,000 vocabulary elements. Their sequences are more condensed, so the Shakespeare data set would be around 300,000 tokens. We trained a 10-million-parameter model on roughly 300,000 tokens. In the GPT-3 paper, the largest Transformer has 175 billion parameters. Their number of layers, embedding dimension, heads, head size, batch size, and learning rate are shown here, while ours had a batch size of 65. They trained on 300 billion tokens, whereas ours was about 300,000, about a millionfold increase. This number isn't even that large by today's standards; they would be going up 1 trillion and above. They are training a significantly larger model on the internet. The architecture is almost identical to what we have, but it's a massive infrastructure challenge to train that, typically with thousands of GPUs.
After pre-training, you get a document completer, not an assistant. It babbles internet, creating arbitrary news articles, because it's trained to complete the sequence. It might answer questions with more questions, ignore your questions, or try to complete a news article, completely unaligned. The second, fine-tuning stage is to align it to be an assistant. This ChatGPT blog post talks about how this stage is achieved. The first step is to collect training data that looks like what an assistant would do, documents with questions at the top and answers below, likely in the thousands of examples, but not on the scale of the internet. You fine-tune the model to focus on documents that look like that, so it expects a question and then completes the answer. The models are sample-efficient during fine-tuning. The second step is to let the model respond and have raters rank the responses. This is used to train a reward model, which predicts how desirable any response would be. Then, they run policy gradient reinforcement learning to fine-tune the sampling policy, so the answers score a high reward according to the reward model. This whole aligning stage takes it from a document completer to a question answerer. This data is not available publicly, it's internal to OpenAI. This stage is harder to replicate. NanogPT focuses on the pre-training stage.
To summarize, we trained a decoder-only Transformer, a GPT, on tiny Shakespeare and got sensible results, with about 200 lines of code. I will release the code base with all git log commits and a Google Colab notebook. Hopefully, it gives you a sense of how to train models like GPT-3. The architecture is identical, but they are 10,000 to 1 million times bigger. We didn't talk about fine-tuning stages for tasks or alignment, or sentiment detection.  That requires further fine-tuning, either simple supervised learning or more complex reinforcement learning, like that used in ChatGPT to train the reward model and use policy gradient methods. We are now at the two-hour mark, so I will end it here.  I hope you enjoyed the lecture. Go forth and transform!
