Here's the cleaned-up transcript:

We're looking at activations, which start blue and become very diffuse by layer four. The gradient statistics are purple on the top layer and diminish deeper in the layers. There's an asymmetry in the neural network. If you have very deep neural networks, like 50 layers, this is not ideal. Before batch normalization, this was incredibly tricky to set. If the gain is too large, diffusion happens, and if it's too little, shrinking occurs. The correct gain is exactly one, as we do at initialization. Then, the statistics for the forward and backward passes are well-behaved. Before normalization layers, advanced optimizers like Adam, and residual connections, training neural networks was a balancing act. You had to precisely orchestrate everything, considering activations, gradients, and their statistics. It was difficult to train very deep networks. This is fundamentally why you had to be very careful with your initialization. You might ask why we need these tanh layers at all. If you only have linear layers, you get nice activations, but it collapses to a single linear layer. The output as a function of the input would be a linear function, no matter how many linear layers you stack. However, although the forward pass collapses to a linear layer, the optimization is not identical. You end up with interesting dynamics in the backward pass due to chain rule calculations. Optimizing a single linear layer and a sandwich of ten linear layers have different training dynamics. There are papers that analyze infinitely layered linear layers. The tanh non-linearities turn this linear sandwich into a neural network that can approximate any arbitrary function. The code is reset to use the linear tanh sandwich with a gain of 5/3. We can run a single step of optimization and look at the activation statistics. I've added a plot to look at parameters, their values, and gradients. I am iterating over all the parameters, focusing on the two-dimensional weights of the linear layers, skipping biases, gammas, and betas. The weights have different shapes. There's the embedding layer, the first linear layer, and the last linear layer. We can view the mean, standard deviation, and histogram of these parameters. Something is not ideal, even though the gradients look okay. The gradient-to-data ratio is also shown. This gives a sense of the scale of the gradient compared to the scale of the actual values. The gradient values are much smaller than the data values in most weights. This is not true of the last layer. The last layer takes on much larger values. The standard deviations are roughly 1 and 3 throughout, except for the last layer, which has a standard deviation of roughly 10 to the negative 2. The gradients on the last layer are about 10 times greater than the other weights. This is problematic because in simple stochastic gradient descent, the last layer would be trained about 10 times faster than the other layers at initialization. This kind of fixes itself with more training. After 1,000 steps, the neurons start saturating a bit, but the forward and backward passes look good and equal, with no shrinking or exploding to infinity. The weights are stabilizing, though the last layer is still a little troubling. We must track the update-to-data ratio to understand parameter updates. We're introducing a new update-to-data ratio list. I am comparing the update, learning rate times gradient, to the data. I am iterating over all the parameters, taking the standard deviation of the update divided by the standard deviation of the data. This ratio indicates how big the updates are compared to the values in tensors. I'm taking the log base 10 of this value. Then, I add it to the list and keep track of it for all the parameters. We are plotting these update ratios over time. The update ratios take on certain values during initialization and stabilize during training. There's an approximate value of roughly 10 to the negative 3 as a guide. Updates to tensors are roughly 1000th of the magnitude of those tensors. If it was much larger, like negative 1 on log plot, the values would be changing quite a lot. The final layer is an outlier because it was artificially shrunk to keep the softmax less confident during initialization. We multiplied the weight by 0.1, making the values too low, resulting in a higher update ratio, which stabilizes with training. I like to look at the update ratios and make sure they are not much above 10 to the negative 3. If it's below negative 3 on the log plot, it indicates the parameters aren’t training fast enough. If the learning rate is too low, the updates will be very small. If the learning rate is one thousandth, the size of the update is about 10,000 times smaller than the values in the tensor. This shows that training is too slow. This is another way to set the learning rate. The learning rate is a little high. It's above the black line, so the rate is around -2.5. Everything is somewhat stabilizing and looks like a decent setting of learning rates. When things are miscalibrated, you'll see it very quickly. If we forget to apply the fan-in normalization, the weights in the linear layers will be sampled from a gaussian. The activation plot will show saturation. The gradients will be messed up. There will be an asymmetry. The update ratios will be messed up too. There will be discrepancies in how fast different layers are learning. Some will learn too fast. Miscalibrations will manifest in these ways. These plots can bring those to your attention so that you can address them. With the linear tanh sandwich, we can calibrate the gains and make the activations, gradients, and parameters look decent. It is like balancing a pencil, because the gain has to be very precisely calibrated. Let's introduce batch normalization layers to fix this. I'm going to place the batch norm 1D class inside the network. The typical placement is between the linear layer and the non-linearity, but placing it after the non-linearity works too. It is fine to place it at the end before the loss function. Because the last layer is batch-normalized, we'll change the gamma to make the softmax less confident. We can train, and the activations will look very good. This is because every tanh layer is preceded by a batch normalization. The standard deviation is roughly 65 2% and equal throughout. The gradients look good, and so do the weights. The updates are reasonable, with all the parameters training at about the same rate. We are going to be less brittle with respect to the gain. We can make the gain be two, but the activations will be unaffected due to explicit normalization. The gradients and weights will also look okay. The updates will change. The backward pass of the batch norm changes the scale of the updates on the parameters. So we don’t get a free pass with the weights, but everything is more robust. We may have to retune the learning rate if we change the scale of the activations going into the batch norms. If we use batch norms, we don't need to normalize by fan-in. I am taking out the fan-in. We’ll see that it’s relatively well-behaved. The statistics in the forward pass look good. The gradients look good. The weight updates look okay. We're significantly below negative 3 so we’d have to bump up the learning rate to get to about one-third. After increasing the learning rate everything looks good. We are more robust to the gain of the linear layers. We still have to worry about the update scales and ensure that the learning rate is properly calibrated. The activations of the forward and backward pass look well-behaved. The purpose was to introduce you to batch normalization, which stabilizes very deep neural networks, and how it works and would be used. We py-torified the code and wrapped it into modules that can be stacked up into neural networks. These layers exist in PyTorch. By prepending “nn.” everything should work, as the API developed is identical to the one in PyTorch. I introduced the diagnostic tools you would use to understand whether your neural network is in a good state. We looked at statistics, histograms, forward pass activations, backward pass gradients, and the weights being updated. We looked at means, standard deviations, and the ratio of gradients to data. People look at this over time. The update to data ratio should be roughly negative 3 on the log scale. If it's too high, the learning rate is too big, and if it's too low, it’s too small. We did not try to beat our previous performance. Our performance is bottlenecked by the context length. We need to look at more powerful architectures. I also didn't give a full explanation of the activations and gradients. There was no full detail on how to pick learning rate based on gains. The purpose was to introduce the diagnostic tools. There is still work on understanding the initialization, backward pass, and how all of that interacts. We are getting to The Cutting Edge of research, people are still trying to figure out the best initialization, the best update rule, and so on. We have tools to tell us if things are on the right track for now.
