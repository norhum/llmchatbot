Matrix multiplications will now use Tensor Flow 32 when it's available. My GPU is an A100, so it's an Ampere series, and therefore TF32 is available. If you have an older GPU, this might not be available for you. But for my GPU, it's available, and so what I expect Pytorch to do is that every single place where we see an `nn.linear` inside, there's a matrix multiplication, and I expect that matrix multiplication now to be running on Tensor Cores utilizing the TF32. This is the single line of change that is, I believe, necessary. Let's rerun this. Now, we saw that, in terms of the throughput that is promised to us, we're supposed to be getting roughly 8x. That 8x came from here, and it also came from looking at it here, 156 TFLOPS instead of 19.5. So, what actually happened? We're seeing that our throughput is roughly 3x, not 8x. We're going from 1,000 milliseconds down to 300 milliseconds, and our throughput is now about 50,000 tokens per second. So we have a roughly 3x improvement instead of 8x. What's happening here is, again, a lot of these workloads are memory-bound. Even though the TF32 offers, in principle, a lot faster throughput, all of these numbers everywhere are still float 32s, and it's float 32 numbers that are being shipped all over the place through the memory system. It's costing us way too much time to shuttle around all this data. Even though we've made the multiply itself much faster, we are memory-bound and we're not actually seeing the full benefit that would come from this napkin math here. That said, we are getting 3x faster throughput, and this is free with a single line of code in Pytorch. All your variables are still float 32 everywhere, it just runs faster, and it's slightly more approximate, but we're not going to notice it. That's TF32. 

Let's now continue. We've exercised this row, and we saw that we can crop out some of the precision inside the operation itself, but we saw that we're still memory-bound, and we're still moving around all these floats, and we're paying that cost because of this. Let's now decrease the amount of stuff that we're going to be moving around, and we're going to do that by dropping down to BFloat16. We're only going to be maintaining 16 bits per float, and we're going to use the BFloat16. I'll explain the FP16 difference in a bit. We're going to be in this row. When we go back to the documentation here for the A100, we see here the precisions that are available. This is the original FP32. The TF32 crops out the precision, and then here in BFloat16, you see that it is very similar to TF32, but it's even more aggressive in cropping off of the precision, the mantissa of this float. The important thing with BFloat16 is that the exponent bits and the sign bit, of course, remain unchanged. If you're familiar with float numbers, the exponent sets the range that you can represent of your numbers, and the precision is how much precision you have for your numbers. The range of numbers is identical, but we have fewer possibilities within that range because we are truncating the mantissa. We have less precision in that range. What that means is that things are actually fairly nice because we have the original range of numbers that are representable in float, but we just have less precision for it. The difference with FP16 is that they actually touch and change the range. FP16 cannot represent the full range of FP32. It has a reduced range, and that's where you start to actually run into issues because now you need these gradient scalers and things like that. I'm not going to go into the detail of that in this video because that's a whole video by itself. But FP16 actually historically came first. That was available in the Volta series before Ampere. FP16 came first, and everyone started to train in FP16, but everyone had to use all these gradient scaling operations, which are kind of annoying, and it's an additional source of state and complexity. The reason for that was because the exponent range was reduced in FP16. That's the IEEE FP16 spec. Then they came out with BFloat16 and the Ampere, and they made it much simpler because we're just truncating the mantissa. We have the exact same range, and we do not need gradient scalers. Everything is much, much simpler. 

Now, when we use BFloat16, though, we are impacting the numbers that we might be seeing in our Pytorch code. This change is not just local to the operation itself. Let's see how that works. There's some documentation here. I think this is probably the best page to explain how to use mixed precision in Pytorch because there are many other tutorials and so on, even within the Pytorch documentation, that are a lot more confusing. I recommend specifically this one because there's five other copies that I would not recommend. When we come here, ignore everything about gradient scalers and only look at `torch.autocast`. Basically, this comes down to a single line of code at the end. This is the context manager that we want, and we want to use that in our network. When you click into the `torch.autocast`, it has a few more guidelines for you. It's telling you, do not call BFloat16 on any of your tensors, just use `autocast` and only surround the forward pass of the model and the loss calculation. Those are the only two things that you should be surrounding. Leave the backward and the optimizer step alone. That's the guidance that comes from the Pytorch team. We're going to follow that guidance. For us, because the loss calculation is inside of the model forward pass, we are going to be doing this. We don't want to be using `torch.float16` because if we do that, we need to start using gradient scalers as well. We are going to be using BFloat16. This is only possible to do on Ampere, but this means that the changes are extremely minimal, like basically just this one line of code. Let me first break in here before we actually run this. Right after the logits, I'd like to show you that different from the TF32 that we saw, this is actually going to impact our tensors. This `logits` tensor, if we now look at this and we look at the `dtype`, we suddenly see that this is now BFloat16. It's not float32 anymore. So our activations have been changed. The activations tensor is now BFloat16, but not everything has changed. `model.transformer.wte`, this is the weight token embedding table. It has a weight inside it, and the `dtype` of this weight, this parameter, is still `torch.float32`. Our parameters seem to still be in float32, but our activations, the logits, are now in BFloat16. Clearly, this is why we get the mixed precision. Some things Pytorch is keeping in float32, some things Pytorch is converting to lower precision. What gets converted at what point is not super clear. I remember scrolling down. Is it here? Okay, I can't find it. I thought it was here. Okay, there we go. There are a few docs on when you're using this `autocast`, what gets converted to BFloat16 and when. For example, only these matrix multiply-like operations get converted to float16, but a lot of operations remain in float32. In particular, a lot of normalizations like layer norms and things like that, not all of those layers might be converted. Only some layers selectively would be running BFloat16. But things like Softmax, layer norms, log, log softmax, and loss function calculations, a lot of those things might remain in float32 because they are more susceptible to precision changes. Matrix multiplies are fairly robust to precision changes. Some parts of the network are impacted more or less by the precision change. Basically, only some parts of the model are running in reduced precision. Let's take it for a spin and let's actually see what kind of improvement we achieve here.

We used to be at 333 milliseconds, we're now at 300. We used to be somewhere around 50,000 tokens per second, we're now at 55,000. We're definitely running faster, but maybe not a lot faster, and that's because there are still many, many bottlenecks in our GPT2. We're just getting started. But we have dropped down the precision as far as we can with my current GPU, which is an A100. We're using Pytorch `autocast`. Unfortunately, I don't exactly know what Pytorch `autocast` does. I don't actually know exactly what's in BFloat16, what's in float32. We could go in and we could start to scrutinize it, but these are the kinds of rules that Pytorch has internally, and unfortunately, they don't document them very well. We're not going to go into that in too much detail, but for now, we are training in BFloat16. We do not need a gradient scaler. The reason things are running faster is because we are able to run Tensor Cores in BFloat16 now. That means we are in this row. But we are also paying in precision for this. We expect slightly less accurate results with respect to the original FP32, but empirically, in many cases, this is a worthwhile kind of trade-off because it allows you to run faster, and you could, for example, train longer and make up for that precision decrease. That's BFloat16 for now. As we can see, we are currently at about 300 milliseconds per iteration, and we're now going to reach for some really heavy weapons in the Pytorch arsenal. In particular, we're going to introduce `torch.compile`. `torch.compile` is really quite incredible infrastructure from the Pytorch team, and it's basically a compiler for neural networks. It's almost like GCC for C/C++ code. This is just the GCC of neural nets. It came out a while ago and is extremely simple to use. The way to use `torch.compile` is to do this. It's a single line of code to compile your model and return it. Now, this line of code will cost you compilation time, but as you might guess, it's going to make the code a lot faster. Let's actually run that because this will take some time to run. Remember, we're currently at 300 milliseconds, and we'll see what happens. While this is running, I'd like to explain a little bit of what `torch.compile` does under the hood. Feel free to read this page of Pytorch, but basically, there's no real good reason for you to not use `torch.compile` in your Pytorch. I kind of feel like you should be using it almost by default if you're not, unless you're debugging and you want your code to run really fast. There's one line here in `torch.compile` that I found that actually kind of gets to why this is faster. Speed-up mainly comes from reducing Python overhead and GPU read/writes. Let me unpack that a little bit. 

Here we are. We went from 300 milliseconds, we're now running at 129 milliseconds. This is a 2.3x improvement from a single line of code in Pytorch, quite incredible. What is happening? What's happening under the hood? When you pass the model to `torch.compile`, what we have here in this `nn.Module` is really just the algorithmic description of what we'd like to happen in our network. `torch.compile` will analyze the entire thing, and it will look at what operations you'd like to use. With the benefit of knowing exactly what's going to happen, it doesn't have to run in what's called the eager mode. It doesn't have to just kind of go layer by layer like the Python interpreter normally would. It starts at the forward and the Python interpreter will go, "Okay, let's do this operation," and then, "let's do that operation," and it kind of materializes all the operations as it goes through. These calculations are dispatched and run in this order, and the Python interpreter and this code doesn't know what kind of operations are going to happen later. But `torch.compile` sees your entire code at the same time, and it's able to know what operations you intend to run, and it will optimize that process. The first thing it will do is it will take out the Python interpreter from the forward pass entirely, and it will compile this entire neural net as a single object with no Python interpreter involved. It knows exactly what's going to run, and it will just run that, and it's all going to be running in efficient code. The second thing that happens is the read/write that they mentioned very briefly. A good example of that, I think, is the GELU nonlinearity that we've been looking at. Here, we use the `nn.GELU`. Now, this here is me just breaking up the `nn.GELU`, which you remember has this formula. This here is the equivalent implementation to what's happening inside `nn.GELU`. It's identical. By default, if we were just using this instead of `nn.GELU`, what would happen without `torch.compile`? Well, the Python interpreter would make its way here, and then it would be, "Okay, well, there's an input. Let me first raise this input to the third power." It's going to dispatch a kernel that takes your input and raises it to the third power, and that kernel will run. When this kernel runs, what ends up happening is this input is stored in the memory of the GPU. Here's a helpful example of the layout of what's happening. You have your CPU. This is in every single computer. There's a few cores in there, and you have your RAM, your memory, and the CPU can talk to the memory. This is all well known. Now, we've added the GPU, and the GPU is a slightly different architecture. Of course, they can communicate, but it's different in that it's got a lot more cores than a CPU. All of those cores are individually a lot simpler too, but it also has memory. This high bandwidth memory (HBM). This is the memory, and it's very equivalent to RAM in the computer. What's happening is that input is living in the memory. When you do `input**3`, this has to travel to the GPU, to the cores, and to all the caches and registers on the actual chip of this GPU. It has to calculate all the elements to the third, and then it saves the result back to the memory. It's this travel time that actually causes a lot of issues. Here, remember, this memory bandwidth, we can communicate about 2 terabytes per second, which is a lot, but also we have to traverse this link, and it's very slow. Here, on the GPU, we're on-chip, and everything is super fast within the chip, but going to the memory is extremely expensive and takes a very long amount of time. We load the input, do the calculations, and load back the output, and this round trip takes a lot of time. Now, right after we do that, we multiply by this constant. What happens then is we dispatch another kernel, and then the result travels back. All the elements get multiplied by a constant, and then the results travel back to the memory. Then, we take the result and we add back the input. This entire thing again travels to the GPU, adds the inputs, and gets written back. We're making all these round trips from the memory to actually where the computation happens because all the tensor cores and ALUs and everything like that is all stored on the chip in the GPU. We're doing a ton of round trips, and Pytorch, without using `torch.compile`, doesn't know to optimize this because it doesn't know what kind of operations you're running later. You're just telling it to raise the power to the third, then do this, then do that, and it will just do that in that sequence. But `torch.compile` sees your entire code. It will come here, and it will realize, "Wait, all of these are element-wise operations, and actually, what I'm going to do is I'm going to do a single trip of the input to the GPU. Then, for every single element, I'm going to do all of these operations while that memory is on the GPU, or chunks of it rather, and then I'm going to write back a single time." We're not going to have these round trips. That's one example of what's called kernel fusion and is a major way in which everything is sped up. Basically, if you have your benefit of insight and you know exactly what you're going to compute, you can optimize your round trips to the memory, and you're not going to pay the memory bandwidth cost. That's fundamentally what makes some of these operations a lot faster, and what they mean by read/writes here. Let me erase this because we are not using it. We should be using `torch.compile`, and our code is now significantly faster. We're doing about 125,000 tokens per second, but we still have a long way to go. 

Before we move on, I wanted to supplement the discussion a little bit with a few more figures because this is a complicated topic, but it's worth understanding at a high level what's happening here. I could probably spend an entire video of like two hours on this, but just a preview of that basically. This chip here, the GPU, is where all the calculations happen mostly, but this chip also does have some memory in it. But most of the memory, by far, is here in the high bandwidth memory (HBM) and is connected. They're connected, but these are two separate chips basically. Here, this is a zoom-in of a cartoon diagram of a GPU, and what we're seeing here is, number one, you see this HBM. On the sides here, it says HBM, and so that's the links to the HBM. The HBM is, again, off-chip. On the chip, there are a large number of these streaming multiprocessors. Every one of these is an SM. There are 120 of them in total, and this is where a lot of the calculations happen. This is a zoom-in of a single individual SM. It has these four quadrants, and, for example, the Tensor Core is where a lot of the matrix multiply stuff happens, but there's all these other units to do all different kinds of calculations for FP64, FP32, and for integers, and so on. We have all this logic here to do the calculations, but in addition to that, on the chip, there is memory sprinkled throughout the chip. L2 cache is some amount of memory that lives on the chip, and then on the SMs themselves, there's L1 cache. This blue bar is L1. There are also registers. There is memory stored here, but the way this memory is stored is very different from the way memory is stored in HBM. This is a very different implementation using transistors and capacitors. It's a very different implementation with SRAM and what that looks like. Long story short is, there is memory inside the chip, but it's not a lot of memory. That's the critical point. This is an example diagram of a slightly different GPU just like here where it shows that, for example, typical numbers for CPU RAM memory, which is this thing here, you might have one terabyte of this, but it would be extremely expensive to access, especially for a GPU. You have to go through the CPU here. Next, we have the HBM. We have tens of gigabytes of HBM memory on a typical GPU here, but it's, as I mentioned, very expensive to access. On the chip itself, everything is extremely fast within the chip, but we only have a couple of 10 megabytes of memory collectively throughout the chip. There's just not enough space because the memory is very expensive on the chip, and so there's not a lot of it. It is lightning fast to access in relative terms. Whenever we have these kernels, the more accurate picture of what's happening here is that we take these inputs, which live by default on the global memory, and now we need to perform some calculation. We start streaming the data from the global memory to the chip. We perform the calculations on the chip, and then stream it back and store it back to the global memory. If we don't have `torch.compile`, we are streaming the data through the chip, doing the calculations, and saving to the memory, and we're doing those round trips many, many times. If it's `torch.compiled`, then we start streaming the memory as before, but then, while we're on the chip, we have a chunk of the data that we're trying to process. That chunk now lives on the chip. While it's on the chip, it's extremely fast to operate on. If we have kernel fusion, we can do all the operations right there in an element-wise fashion, and those are very cheap, and then we do a single round trip back to the global memory. Operator fusion basically allows you to keep your chunk of data on the chip and do lots of calculations on it before you write it back, and that gives huge savings, and that's why `torch.compile` ends up being a lot faster, or that's one of the major reasons. Again, just a very brief intro to the memory hierarchy and roughly what `torch.compile` does for you. 

`torch.compile` is amazing, but there are operations `torch.compile` will not find, and an amazing example of that is Flash Attention, to which we turn next. Flash Attention comes from this paper from Stanford in 2022, and it's this incredible algorithm for performing attention and running it a lot faster. Flash Attention will come here, and we will take out these four lines, and Flash Attention implements these four lines really, really quickly. How does it do that? Flash Attention is a kernel fusion operation. Here, in this diagram, they're showing Pytorch, and you have these four operations. They're including dropout, but we are not using dropout here. We just have these four lines of code here, and instead of those, we are fusing them into a single fused kernel of Flash Attention. It's a kernel fusion algorithm, but it's a kernel fusion that `torch.compile` cannot find. The reason that it cannot find it is that it requires an algorithmic rewrite of how attention is actually implemented in this case. What's remarkable about it is that Flash Attention, if you just count the number of FLOPS, Flash Attention does more FLOPS than this attention here. But Flash Attention is actually significantly faster. In fact, they cite 7.6 times faster potentially, and that's because it is very mindful of the memory hierarchy as I described it just now. It's very mindful about what's in high bandwidth memory, what's in the shared memory, and it is very careful with how it orchestrates the computation such that we have fewer reads and writes to the high bandwidth memory. Even though we're doing more FLOPS, the expensive part is the load and store into HBM, and that's what they avoid. In particular, they do not ever materialize this N by N attention matrix, this `ATT` here. Flash Attention is designed such that this matrix never gets materialized at any point, and it never gets read or written to the HBM. This is a very large matrix because this is where all the queries and keys interact, and we're sort of getting, for each head, for each batch element, we're getting a T by T matrix of attention, which is a million numbers, even for a single head at a single batch index. This is a ton of memory, and this is never materialized. The way that this is achieved is that, basically, the fundamental algorithmic rewrite here relies on this online softmax trick, which was proposed previously. The online softmax trick coming from a previous paper shows how you can incrementally evaluate a softmax without having to realize all of the inputs to the softmax to do the normalization. You do that by having these intermediate variables, M and L, and there's an update to them that allows you to evaluate the softmax in an online manner. Flash Attention actually, recently Flash Attention 2 came out as well. It has additional gains to how it calculates Flash Attention. The original paper that this is based on is this online normalizer calculation for softmax. It came out of NVIDIA really early in 2018. This paper says that they propose a way to compute the classical softmax with fewer memory accesses and hypothesize that this reduction in memory accesses should improve softmax performance on actual hardware. They are extremely correct in this hypothesis. It's really fascinating that they're from NVIDIA and that they had this realization, but they didn't actually take it to the actual Flash Attention. That had to come four years later from Stanford. I don't fully understand the historical how this happened, but they do basically propose this online update to the softmax. This is fundamentally what they reuse here to calculate the softmax in a streaming manner. They realize they can actually fuse all the other operations with the online softmax calculation into a single fused kernel, Flash Attention. That's what we are about to use. Great example, I think, of being aware of memory hierarchy, the fact that FLOPS don't matter, that the entire memory access pattern matters, and that `torch.compile` is amazing, but there are many optimizations that are still available to us that potentially `torch.compile` cannot find. Maybe one day it could, but right now, it seems like a lot to ask. Here's what we're going to do. We're going to use Flash Attention. The way to do that basically in Pytorch is we are going to comment out these four lines and we're going to replace them with a single line. Here, we are calling this compound operation in Pytorch called `scaled_dot_product_attention`, and Pytorch will call Flash Attention when you use it in this way. I'm not 100% sure why `torch.compile` doesn't realize that these four lines should just call Flash Attention in this exact way. We have to do it again for it, which in my opinion, is a little bit odd, but here we are. You have to use this compound op. Let's wait for a few moments before `torch.compile` gets around to it. Let's remember that we achieved 6.05661. That's the loss we were expecting to see, and we took 130 milliseconds before this change. We're expecting to see the exact same result by iteration 49, but we expect to see faster runtime because Flash Attention is just an algorithmic rewrite and it's a faster kernel, but it doesn't actually change any of the computation. We should have the exact same optimization. 

We're a lot faster. We're at about 95 milliseconds, and we achieved 6.58. They're basically identical up to a floating-point fudge factor. It's the identical computation, but it's significantly faster, going from 130 to roughly 90, 96. This is maybe a 27% improvement. That is Flash Attention.

We are now getting to one of my favorite optimizations, and it is simultaneously the dumbest and the most brilliant optimization, and it's always a little bit surprising to me. I mentioned a few minutes ago that there are some numbers that are nice and some numbers that are ugly. 64 is a beautiful, nice number. 128 is even nicer. 256 is beautiful. What makes these numbers beautiful is that there are many powers of two inside them. You can divide by two many times. Examples of ugly numbers are like 13 and 17, and something like that, prime numbers, numbers that are not even, and so on. Pretty much you always want to use nice numbers in all of your code that deals with neural networks or CUDA because everything in CUDA works in sort of like powers of two. Lots of kernels are written in terms of powers of two, and there are lots of blocks of sizes 16 and 64, and so on. Everything is written in those terms, and you always have special case handling for all kinds of logic when your inputs are not made of nice numbers. Let's see what that looks like. Basically, scan your code and look for ugly numbers, is roughly the heuristic. 3 times is kind of ugly. I'm not 100% sure, maybe this can be improved, but this is ugly and not ideal. 4 times is nice. That's nice. 1024 is very nice. That's a power of two. 12 is a little bit suspicious, not too many powers of two. 768 is great. 50,257 is a really, really ugly number. First of all, it's odd, and there are not too many powers of two in there. This is a very ugly number, and it's highly suspicious. When we scroll down, all these numbers are nice, except for 25. In this configuration of GPT2 XL, the number of heads is 25. That's a really ugly number. That's an odd number. This did cause a lot of headaches for us recently when we're trying to optimize some kernels to run this fast, and it required a bunch of special case handling. Basically, these numbers, we have some ugly numbers, and some of them are easier to fix than others. In particular, the vocabulary size being 50,257. That's a very ugly number, very suspicious, and we want to fix it. When you fix these things, one of the easy ways to do that is you basically increase the number until it's the nearest power of two that you like. Here's a much nicer number: 50,304. Why is that? Because 50,304 can be divided by 8, or by 16, or by 32, or 64. It can even be divided by 128. It's a very nice number. What we're going to do here is the GPT
