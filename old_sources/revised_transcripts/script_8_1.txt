We're checking that the pair matches. Here is a tricky condition that you have to append if you're trying to be careful: you don't want this here to be out of bounds at the very last position when you're on the rightmost element of this list. Otherwise, this would give you an out-of-bounds error. We have to make sure that we're not at the very last element, so this would be false for that. If we find a match, we append to this new list that replacement index, and we increment the position by two, so we skip over that entire pair. Otherwise, if we haven't found a matching pair, we just copy over the element at that position and increment by one, then return this. Here's a very small toy example: if we have a list `5 6 6 7 9 1` and we want to replace the occurrences of `6 7` with `99`, then calling this on that will give us what we're asking for. Here, the `6 7` is replaced with `99`. Now, I'm going to uncomment this for our actual use case where we want to take our tokens, we want to take the top pair here, and replace it with `256` to get tokens to. If we run this, we get the following. Recall that previously we had a length of 616 in this list, and now we have a length of 596. This decreased by 20, which makes sense because there are 20 occurrences. Moreover, we can try to find `256` here, and we see plenty of occurrences of it. And moreover, just double check, there should be no occurrence of `101 32`. So, this is the original array with plenty of them, and in the second array, there are no occurrences of `1032`. We've successfully merged this single pair, and now we just iterate this. We are going to go over the sequence again, find the most common pair, and replace it. Let me now write a while loop that uses these functions to do this iteratively. How many times do we do it? Well, that's totally up to us as a hyperparameter. The more steps we take, the larger will be our vocabulary and the shorter will be our sequence. There is some sweet spot that we usually find works the best in practice. This is kind of a hyperparameter, and we tune it and we find good vocabulary sizes. As an example, GPT4 currently uses roughly 100,000 tokens, and BPE, those are reasonable numbers currently instead in large language models. Let me now write, putting it all together, and iterating these steps. Before we dive into the while loop, I wanted to add one more cell here where I went to the blog post, and instead of grabbing just the first paragraph or two, I took the entire blog post and I stretched it out in a single line. Basically, just using longer text will allow us to have more representative statistics for the byte pairs, and we'll just get more sensible results out of it because it's longer text. Here we have the raw text. We encode it into bytes using the UTF-8 encoding. Then here, as before, we are just changing it into a list of integers in Python, just so it's easier to work with instead of the raw bytes objects. Then, this is the code that I came up with to actually do the merging in a loop. These two functions here are identical to what we had above. I only included them here just so that you have a point of reference. These two are identical, and then this is the new code that I added. The first thing we want to do is we want to decide on the final vocabulary size that we want our tokenizer to have. As I mentioned, this is a hyperparameter, and you set it in some way depending on your best performance. Let's say for us, we're going to use 276, because that way, we're going to be doing exactly 20 merges. We're doing 20 merges because we already have 256 tokens for the raw bytes, and to reach 276, we have to do 20 merges to add 20 new tokens here. This is one way in Python to just create a copy of a list. I'm taking the tokens list, and by wrapping it in a list, Python will construct a new list of all the individual elements, so this is just a copy operation. Then here, I'm creating a merges dictionary. This merges dictionary is going to maintain basically the child one, child two mapping to a new token. What we're going to be building up here is a binary tree of merges, but actually, it's not exactly a tree, because a tree would have a single root node with a bunch of leaves. For us, we're starting with the leaves on the bottom, which are the individual bytes. Those are the starting 256 tokens, and then we're starting to merge two of them at a time. So, it's not a tree, it's more like a forest, as we merge these elements. For 20 merges, we're going to find the most commonly occurring pair. We're going to mint a new token integer for it. So, `I` here will start at zero, so we're going to start at `256`. We're going to print that we're merging it, and we're going to replace all of the occurrences of that pair with the new lied token. We're going to record that this pair of integers merged into this new integer. Running this gives us the following output. We did 20 merges, and, for example, the first merge was exactly as before, the `101 32` tokens merging into a new token `256`. Keep in mind that the individual tokens `101` and `32` can still occur in the sequence after merging. It's only when they occur exactly consecutively that that becomes `256`. In particular, the other thing to notice here is that the token `256`, which is the newly minted token, is also eligible for merging. Here, on the bottom, the 20th merge was a merge of `25` and `259` becoming `275`. Every time we replace these tokens, they become eligible for merging in the next round of iteration. That's why we're building up a small sort of binary forest instead of a single individual tree. One thing we can take a look at as well is we can take a look at the compression ratio that we've achieved. We started off with this tokens list. We started off with 24,000 bytes, and after merging 20 times, we now have only 19,000 tokens. Therefore, the compression ratio, simply just dividing the two, is roughly 1.27. That's the amount of compression we were able to achieve of this text with only 20 merges. Of course, the more vocabulary elements you add, the greater the compression ratio here would be. Finally, that's kind of like the training of the tokenizer, if you will. One point I wanted to make, and maybe this is a diagram that can help illustrate, is that a tokenizer is a completely separate object from the large language model itself. Everything in this lecture, we're not really touching the LLM itself. We're just training the tokenizer. This is a completely separate preprocessing stage, usually. The tokenizer will have its own training set, just like a large language model has a potentially different training set. The tokenizer has a training set of documents on which you're going to train the tokenizer, and we're performing the byte pair encoding algorithm, as we saw above, to train the vocabulary of this tokenizer. It has its own training set. It is a preprocessing stage that you would run a single time in the beginning. The tokenizer is trained using the byte pair coding algorithm. Once you have the tokenizer, once it's trained, and you have the vocabulary and you have the merges, we can do both encoding and decoding. These two arrows here. So, the tokenizer is a translation layer between raw text, which is, as we saw, the sequence of Unicode code points. It can take raw text and turn it into a token sequence and vice versa. It can take a token sequence and translate it back into raw text. Now that we have trained a tokenizer and we have these merges, we are going to turn to how we can do the encoding and the decoding step. If you give me text, here are the tokens, and vice versa, if you give me tokens, here's the text. Once we have that, we can translate between these two realms, and then the language model is going to be trained as a step two afterwards. Typically, in a state-of-the-art application, you might take all of your training data for the language model, and you might run it through the tokenizer and translate everything into a massive token sequence. Then, you can throw away the raw text. You're just left with the tokens themselves, and those are stored on disk. That is what the large language model is actually reading when it's training on them. This one approach that you can take is as a single massive preprocessing step, a stage. I think the most important thing I want to get across is that this is a completely separate stage. It usually has its own entire training set. You may want to have those training sets be different between the tokenizer and the large language model. For example, when you're training the tokenizer, as I mentioned, we don't just care about the performance of English text. We care about many different languages, and we also care about code or not code. You may want to look into different kinds of mixtures of different kinds of languages and different amounts of code and things like that, because the amount of different language that you have in your tokenizer training set will determine how many merges of it there will be. Therefore, that determines the density with which this type of data is in the token space. Roughly speaking, intuitively, if you add some amount of data, like say you have a ton of Japanese data in your tokenizer training set, then that means that more Japanese tokens will get merged. Therefore, Japanese will have shorter sequences, and that's going to be beneficial for the large language model, which has a finite context length on which it can work on in the token space. Hopefully that makes sense. We're now going to turn to encoding and decoding now that we have trained a tokenizer, so we have our merges, and now how do we do encoding and decoding? Let's begin with decoding, which is this arrow over here. Given a token sequence, let's go through the tokenizer to get back a Python string object, the raw text. This is the function that we'd like to implement. We're given the list of integers, and we want to return a Python string. If you'd like, try to implement this function yourself. It's a fun exercise. Otherwise, I'm going to start pasting in my own solution. There are many different ways to do it. Here's one way. I will create a kind of preprocessing variable that I will call `vocab`. `Vocab` is a mapping, or a dictionary in Python, from the token ID to the bytes object for that token. We begin with the raw bytes for tokens from 0 to 255, and then we go in order of all the merges, and we sort of populate this vocab list by doing an addition here. This is basically the bytes representation of the first child followed by the second one. Remember, these are bytes objects, so this addition here is an addition of two bytes objects, just concatenation. That's what we get here. One tricky thing to be careful with, by the way, is that I'm iterating a dictionary in Python using a `.items()`, and it really matters that this runs in the order in which we inserted items into the merges dictionary. Luckily, starting with Python 3.7, this is guaranteed to be the case, but before Python 3.7, this iteration may have been out of order with respect to how we inserted elements into merges, and this may not have worked, but we are using a modern Python, so we're okay. Given the IDs, the first thing we're going to do is get the tokens. The way I implemented this here is I'm taking, I'm iterating over all the IDs, I'm using `vocab` to look up their bytes, and then here, this is one way in Python to concatenate all these bytes together to create our tokens. These tokens here at this point are raw bytes, so I have to decode using UTF-8 back into Python strings. Previously, we called that encode on a string object to get the bytes, and now we're doing the opposite. We're taking the bytes and calling a decode on the bytes object to get a string in Python, and then we can return text. This is how we can do it. This actually has an issue in the way I implemented it, and this could actually throw an error. Try to think, figure out why this code could actually result in an error if we plug in some sequence of IDs that is unlucky. Let me demonstrate the issue. When I try to decode just something like `97`, I am going to get letter A here back. Nothing too crazy happening, but when I try to decode `128` as a single element, the token `128`, what in string or in Python object, Unicode decoder UTF-8 can't decode byte 0x80 which is this in hex, in position zero, invalid start byte. What does that mean? To understand what this means, we have to go back to our UTF-8 page that I briefly showed earlier. This is Wikipedia UTF-8, and basically, there's a specific schema that UTF-8 bytes take. If you have a multi-byte object for some of the Unicode characters, they have to have this special sort of envelope in how the encoding works. What's happening here is that invalid start byte, that's because `128`, the binary representation of it is one followed by all zeros. We have one and then all zeros. We see here that that doesn't conform to the format because one followed by all zeros just doesn't fit any of these rules. It's an invalid start byte, which is byte one. This one must have a one following it, and then a zero following it, and then the content of your Unicode in `x` here. We don't exactly follow the UTF-8 standard, and this cannot be decoded. The way to fix this is to use this `errors = 'replace'` in the `bytes.decode()` function of Python. By default, `errors` is strict, so we will throw an error if it's not valid UTF-8 bytes encoding, but there are many different things that you could put here on error handling. This is the full list of all the errors that you can use. In particular, instead of strict, let's change it to replace, and that will replace with this special marker, this replacement character. Now, we just get that character back. Basically, not every single byte sequence is valid UTF-8, and if it happens that your large language model, for example, predicts your tokens in a bad manner, then they might not fall into valid UTF-8, and then we won't be able to decode them. The standard practice is to basically use `errors='replace'`, and this is what you will also find in the OpenAI code that they released as well. Whenever you see this kind of a character in your output, in that case, something went wrong, and the LLM output was not a valid sequence of tokens. Now we're going to go the other way, so we are going to implement this arrow right here, where we are going to be given a string, and we want to encode it into tokens. This is the signature of the function that we're interested in, and this should basically print a list of integers of the tokens. Again, try to maybe implement this yourself if you'd like a fun exercise, and pause here. Otherwise, I'm going to start putting in my solution. There are many ways to do this. This is one of the ways that I came up with. The first thing we're going to do is we are going to take our text, encode it into UTF-8 to get the raw bytes, and then, as before, we're going to call `list()` on the bytes object to get a list of integers of those bytes. Those are the starting tokens. Those are the raw bytes of our sequence, but now, of course, according to the merges dictionary above, and recall this was the merges. Some of the bytes may be merged according to this lookup. In addition to that, remember that the merges were built from top to bottom, and this is sort of the order in which we inserted stuff into merges. We prefer to do all these merges in the beginning before we do these merges later, because, for example, this merge over here relies on the `256` which got merged here. We have to go in the order from top to bottom sort of if we are going to be merging anything. Now, we expect to be doing a few merges, so we're going to be doing `while True`. Now, we want to find a pair of bytes that is consecutive that we are allowed to merge according to this. In order to reuse some of the functionality that we've already written, I'm going to reuse the function `get_stats`. Recall that `get_stats` will give us, we'll basically count up how many times every single pair occurs in our sequence of tokens and return that as a dictionary. The dictionary was a mapping from all the different byte pairs to the number of times that they occur. At this point, we don't actually care how many times they occur in the sequence. We only care what the raw pairs are in that sequence. I'm only going to be using basically the keys of the dictionary. I only care about the set of possible merge candidates, if that makes sense. Now, we want to identify the pair that we're going to be merging at this stage of the loop. What do we want? We want to find the pair, or like a key inside `stats`, that has the lowest index in the merges dictionary because we want to do all the early merges before we work our way to the late merges. Again, there are many different ways to implement this, but I'm going to do something a little bit fancy here. I'm going to be using the `min()` over an iterator in Python. When you call `min()` on an iterator, and `stats` here is a dictionary, we're going to be iterating the keys of this dictionary in Python. We're looking at all the pairs inside `stats`, which are all the consecutive pairs, and we're going to be taking the consecutive pair inside tokens that has the minimum. The `min()` takes a key, which gives us the function that is going to return a value over which we're going to do the min, and the one we care about is we care about taking merges and basically getting that pair's index. For any pair inside `stats`, we are going to be looking into merges at what index it has, and we want to get the pair with the min number. As an example, if there's a pair `101` and `32`, we definitely want to get that pair. We want to identify it here and return it, and pair would become `101 32` if it occurs. The reason that I'm putting a float infinity here as a fallback is that in the `get()` function, when we call, when we basically consider a pair that doesn't occur in the merges, then that pair is not eligible to be merged. If in the token sequence there's some pair that is not a merging pair, it cannot be merged, then it doesn't actually occur here and it doesn't have an index and it cannot be merged, which we will denote as float infinity. The reason infinity is nice here is because for sure we're guaranteed that it's not going to participate in the list of candidates when we do the min. This is one way to do it. Long story short, this returns the most eligible merging candidate pair that occurs in the tokens. One thing to be careful with here is this function here might fail in the following way: if there's nothing to merge, then there's nothing in merges that is satisfied anymore. There's nothing to merge. Everything just returns float infinity, and then the pair, I think, will just become the very first element of stats, but this pair is not actually a mergeable pair. It just becomes the first pair inside stats arbitrarily because all of these pairs evaluate to float in for the merging criterion. Basically, it could be that this doesn't succeed because there's no more merging pairs. If this pair is not in merges that was returned, then this is a signal for us that actually there was nothing to merge. No single pair can be merged anymore. In that case, we will break out. Nothing else can be merged. You may come up with a different implementation, by the way. This is really trying hard in Python. We're just trying to find a pair that can be merged with the lowest index here. If we did find a pair that is inside merges with the lowest index, then we can merge it. We're going to look into the merger dictionary for that pair to look up the index, and we're going to now merge that into that index. We're going to do `tokens = replace_all()`, and we're going to replace the original tokens. We're going to be replacing the pair, `pair`, and we're going to be replacing it with index `idx`. This returns a new list of tokens where every occurrence of `pair` is replaced with `idx`. We're doing a merge, and we're going to be continuing this until eventually nothing can be merged. We'll come out here and we'll break out, and here we just return tokens. That's the implementation, I think, so hopefully this runs okay. This looks reasonable. For example, 32 is a space in ASCII, so that's here. This looks like it worked great. Let's wrap up this section of the video, at least. I wanted to point out that this is not quite the right implementation just yet, because we are leaving out a special case. If we try to do this, this would give us an error. The issue is that if we only have a single character or an empty string, then `stats` is empty, and that causes an issue inside `min()`. One way to fix this is if the length of tokens is at least two, because if it's less than two, it's just a single token or no tokens, then there's nothing to merge, so we just return. That would fix that case. Second, I have a few test cases here for us as well. First, let's make sure about, or let's note the following: if we take a string and we try to encode it, and then decode it back, you'd expect to get the same string back, right? Is that true for all strings? It is the case here, and I think in general this is probably the case. Notice that going backwards is not, you're not going to have an identity going backwards. As I mentioned, not all token sequences are valid UTF-8 streams, and some of them can't even be decodable. This only goes in one direction, but for that one direction, we can check here. If we take the training text, which is the text that we trained the tokenizer around, we can make sure that when we encode and decode, we get the same thing back, which is true. I took some validation data. I went to, I think, this web page, and I grabbed some text. This is text that the tokenizer has not seen, and we can make sure that this also works. That gives us some confidence that this was correctly implemented. Those are the basics of the byte pair encoding algorithm. We saw how we can take some training set and train a tokenizer. The parameters of this tokenizer really are just this dictionary of merges, and that basically creates the little binary forest on top of raw bytes. Once we have this, the merges table, we can both encode and decode between raw text and token sequences. That's the simplest setting of the tokenizer. What we're going to do now, though, is we're going to look at some of the state-of-the-art large language models and the kinds of tokenizers that they use, and we're going to see that this picture complexifies very quickly. We're going to go through the details of this complexification one at a time. Let's kick things off by looking at the GPT series. In particular, I have the GPT-2 paper here. This paper is from 2019 or so, so five years ago. Let's scroll down to input representation. This is where they talk about the tokenizer that they're using for GPT-2. This is all fairly readable, so I encourage you to pause and read this yourself, but this is where they motivate the use of the byte pair encoding algorithm on the byte-level representation of UTF-8 encoding. This is where they motivate it, and they talk about the vocabulary sizes and everything. Everything here is exactly as we've covered it so far, but things start to depart around here. They mention that they don't just apply the naive algorithm as we have done it. Here's an example. Suppose that you have common words like "dog". What will happen is that "dog," of course, occurs very frequently in the text, and it occurs right next to all kinds of punctuation as an example, so "dog." "dog!" "dog?" etc. Naively, you might imagine that the BPE algorithm could merge these to be single tokens, and then you end up with lots of tokens that are just like "dog" with a slightly different punctuation. It feels like you're clustering things that shouldn't be clustered. You're combining kind of semantics with punctuation, and this feels suboptimal. Indeed, they also say that this is suboptimal according to some of the experiments. What they want to do is they want to top down in a manual way, enforce that some types of characters should never be merged together. They want to enforce these merging rules on top of the byte pair encoding algorithm. Let's take a look at their code and see how they actually enforce this, and what kinds of merges they actually do perform. I have a tab open here for GPT-2 under OpenAI on GitHub. When we go to source, there is an `encoder.py`. I don't personally love that they call it `encoder.py`, because this is the tokenizer, and the tokenizer can do both encode and decode, so it feels kind of awkward to me that it's called encoder, but that is the tokenizer. There's a lot going on here, and we're going to step through it in detail. For now, I just want to focus on this part here: the create a regex pattern here that looks very complicated, and we're going to go through it in a bit, but this is the core part that allows them to enforce rules for what parts of the text will never be merged for sure. Notice that `re.compile()` here is a little bit misleading because we're not just doing `import re`, which is the Python re module, we're doing `import regex as re`, and `regex` is a Python package that you can install with `pip install regex`. It's basically an extension of re, so it's a bit more powerful re. Let's take a look at this pattern and what it's doing, and why this is actually doing the separation that they are looking for. I've copy-pasted the pattern here to our Jupyter notebook where we left off, and let's take this pattern for a spin. In the exact same way that their code does, we're going to call an `re.findall()` for this pattern on any arbitrary string that we are interested in. This is the string that we want to encode into tokens to feed into an LLM like GPT-2. What exactly is this doing? `Re.findall()` will take this pattern and try to match it against a string. The way this works is that you are going from left to right in the string, and you're trying to match the pattern. `re.findall()` will get all the occurrences and organize them into a list. When you look at the pattern, first of all, notice that this is a raw string, and then these are three double quotes just to start the string. The string itself, this is the pattern itself, and notice that it's made up of a lot of "or" statements. See these vertical bars? Those are "or" statements in RegEx. You go from left to right in this pattern and try to match it against the string wherever you are. We have "hello," and we're going to try to match it. Well, it's not apostrophe s, it's not apostrophe t, or any of these, but it is an optional space followed by a `\p{L}` one or more times. What is `\p{L}`? It is coming to some documentation that I found. There might be other sources as well. `\p{L}` is a letter, any kind of letter from any language. "hello" is made up of letters: h, e, l, etc. An optional space followed by a bunch of letters, one or more letters, is going to match "hello," but then the match ends because a white space is not a letter. From there on begins a new sort of attempt to match against the string again. Starting in here, we're going to skip over all of these again until we get to the exact same point again, and we see that there's an optional space. This is the optional space, followed by a bunch of letters, one or more of them, and so that matches. When we run this, we get a list of two elements: "hello" and then " world". "How are you?" If we add more letters, we would just get them like this. What is this doing, and why is this important? We are taking our string, and instead of directly encoding it for tokenization, we are first splitting it up. When you actually step through the code, and we'll do that in a bit more detail, what it's really doing on a high level is that it first splits your text into a list of texts, just like this one, and all these elements of this list are processed independently by the tokenizer, and all of the results of that processing are simply concatenated. "hello," "world," "how," "are," "you". We have five elements of a list. All of these will independently go from text to a token sequence, and then that token sequence is going to be concatenated. It's all going to be joined up. Roughly speaking, what that does is you're only ever finding merges between the elements of this list. You can only ever consider merges within every one of these elements individually. After you've done all the possible merging for all of these elements individually, the results of all that will be joined by concatenation. You are basically, what you're doing effectively is you are never going to be merging this "e" with this space, because they are now parts of the separate elements of this list. You are saying we are never going to merge "e space" because we're breaking it up in this way.
