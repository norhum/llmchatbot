Here is the cleaned-up transcript:

We are going to define a layer of neurons. Here we have a schematic for a multilayer perceptron (MLP). We see that each layer has a number of neurons. These neurons are not connected to each other, but all of them are fully connected to the input. What is a layer of neurons? It is just a set of neurons evaluated independently. In the interest of time, I am going to do something fairly straightforward. Literally, a layer is just a list of neurons. We take the number of neurons as an input argument. How many neurons do you want in your layer? This is the number of outputs in this layer. We just initialize completely independent neurons with this given dimensionality. When we call on it, we just independently evaluate them. Now, instead of a neuron, we can make a layer of neurons. They are two-dimensional neurons. Let's have three of them. We now see that we have three independent evaluations of three different neurons. Finally, let's complete this picture and define an entire multi-layer perceptron or MLP. As we can see here, in an MLP, these layers just feed into each other sequentially. I am going to copy the code here in the interest of time. An MLP is very similar. We are taking the number of inputs as before. Now, instead of taking a single n_out, which is the number of neurons in a single layer, we are going to take a list of n_outs. This list defines the sizes of all the layers that we want in our MLP. Here, we just put them all together and iterate over consecutive pairs of these sizes and create layer objects for them. In the call function, we are just calling them sequentially. That is an MLP. Let's actually re-implement this picture. We want three input neurons and then two layers of four and an output unit. We want a three-dimensional input. Say this is an example input. We want three inputs into two layers of four and one output. This is an MLP. There we go. That is a forward pass of an MLP. To make this a little bit nicer, you see how we have just a single element, but it is wrapped in a list. This is because the layer always returns lists. For convenience, we return outs at zero if the length of out is exactly a single element. Otherwise, return the full list. This will allow us to just get a single value out at the last layer that only has a single neuron. Finally, we should be able to draw the dot of n of x. As you might imagine, these expressions are now getting relatively involved. This is an entire MLP that we are defining now all the way until a single output. Obviously, you would never differentiate on pen and paper these expressions, but with micrograd, we will be able to back propagate all the way through this and back propagate into the weights of all these neurons. Let's see how that works. Let's create ourselves a very simple example dataset here. This dataset has four examples. We have four possible inputs into the neural net, and we have four desired targets. We would like the neural net to assign or output 1.0 when it is fed this example, -1 when it is fed these examples, and 1 when it is fed this example. It is a very simple binary classifier neural net that we would like here. Now let's think what the neural net currently thinks about these four examples. We can just get their predictions. Basically, we can just call n of x for x in axes and then we can print. These are the outputs of the neural net on those four examples. The first one is 0.91, but we would like it to be one, so we should push this one higher. This one we want to be higher. This one says 0.88, and we want this to be -1. This is 0.8, and we want it to be -1. This one is 0.8, and we want it to be one. How do we make the neural net and how do we tune the weights to better predict the desired targets? The trick used in deep learning to achieve this is to calculate a single number that somehow measures the total performance of your neural net. We call this single number the loss. The loss is a single number that we are going to define. It basically measures how well the neural net is performing. Right now, we have the intuitive sense that it is not performing very well because we are not very close to this. The loss will be high, and we will want to minimize the loss. In particular, in this case, we are going to implement the mean squared error loss. This is doing is we are going to iterate for y ground truth and y output in the zip of y's and y_pred. We are going to pair up the ground truths with the predictions. This zip iterates over tuples of them. For each y ground truth and y output, we are going to subtract them and square them. Let's first see what these losses are. These are individual loss components. For each one of the four, we are taking the prediction and the ground truth. We are subtracting them and squaring them. Because this one is so close to its target, 0.91 is almost one. Subtracting them gives a very small number. Here, we would get like a -0.1, and then squaring it just makes sure that, regardless of whether we are more negative or more positive, we always get a positive number. Instead of squaring, we could also take, for example, the absolute value. We need to discard the sign. You see that the expression is arranged so that you only get zero exactly when y out is equal to y ground truth. When those two are equal, so your prediction is exactly the target, you are going to get zero. If your prediction is not the target, you are going to get some other number. Here, for example, we are way off, and that is why the loss is quite high. The more off we are, the greater the loss will be. We do not want a high loss. We want a low loss. The final loss here will be just the sum of all of these numbers. This should be zero roughly, plus zero roughly, plus seven, so the loss should be about seven here. We want to minimize the loss. We want the loss to be low. If the loss is low, then every one of the predictions is equal to its target. The lowest it can be is zero, and the greater it is, the worse off the neural net is predicting. Now, if we do loss dot backward, something magical happened when I hit enter. The magical thing that happened is that we can look at n.layers, then the neuron, and then layers at say the first layer, the neurons at zero. Remember that MLP has the layers which is a list, and each layer has neurons which is a list. That gives us an individual neuron. It has some weights. We can, for example, look at the weights at zero. Oops, it is not called weights, it is called w, and that is a value. Now, this value also has a grad because of the backward pass. Because this gradient here on this particular weight of this particular neuron of this particular layer is negative, we see that its influence on the loss is also negative. Slightly increasing this particular weight of this neuron of this layer would make the loss go down. We actually have this information for every single one of our neurons and all their parameters. It is worth looking at also the draw dot loss, by the way. Previously, we looked at the draw dot of a single neural neuron forward pass, and that was already a large expression. What is this expression? We actually forwarded every one of those four examples, and then we have the loss on top of them with the mean squared error. This is a massive graph because this graph that we have built up, it is excessive because it has four forward passes of a neural net for every one of the examples, and then it has the loss on top. It ends with the value of the loss, which was 7.12. This loss will now back propagate through all the four forward passes all the way through every single intermediate value of the neural net all the way back to the parameters of the weights, which are the input. These weight parameters here are inputs to this neural net, and these numbers here, these scalars, are inputs to the neural net. If we went around here, we would probably find some of these examples. This 1.0 potentially, maybe this 1.0, or some of the others. You will see that they all have gradients as well. These gradients on the input data are not that useful to us. The input data seems to be not changeable. It is a given to the problem. It is a fixed input. We are not going to be changing it or messing with it, even though we do have gradients for it. Some of these gradients here will be for the neural network parameters, the w’s and the b’s. We, of course, want to change those. Now, we are going to want some convenience code to gather up all of the parameters of the neural net so that we can operate on all of them simultaneously. Every one of them we will nudge a tiny amount based on the gradient information. Let's collect the parameters of the neural net all in one array. Let's create the parameters of self that just returns self.w, which is a list, concatenated with a list of self.b. This will just return a list. A list plus list just gives you a list. That is parameters of a neuron. I am calling it this way because also PyTorch has a parameters on every single nn.module, and it does exactly what we are doing here. It just returns the parameter tensors for us as the parameter scalars. A layer is also a module, so it will have parameters itself. Basically, what we want to do here is something like this. Like params is here, and then for a neuron in self.neurons, we want to get neuron.parameters, and we want to params.extend, right? These are the parameters of this neuron, and then we want to put them on top of params, so params.extend of p’s, and then we want to return params. This is way too much code, so there is a way to simplify this, which is return p for neuron in self.neurons for p in neuron.parameters. It is a single list comprehension in Python. You can sort of nest them like this. You can then create the desired array. These are identical. We can take this out. Let's do the same here: def parameters self and return a parameter for layer in self.layers for p in layer.parameters, and that should be good. Let me pop out this so that we do not re-initialize our network because we need to re-initialize our... Unfortunately, we will probably have to re-initialize the network because we just added functionality. This class, of course, we want to get all the network parameters, but that is not going to work because this is the old class. We do have to re-initialize the network, which will change some of the numbers. Let me do that so that we pick up the new API. We can now do n.parameters, and these are all the weights and biases inside the entire neural net. In total, this MLP has 41 parameters. We will be able to change them. If we recalculate the loss here, we see that, unfortunately, we have slightly different predictions and slightly different losses, but that is okay. We see that this neuron’s gradient is slightly negative. We can also look at its data right now, which is 0.85. This is the current value of this neuron, and this is its gradient on the loss. What we want to do now is we want to iterate for every p in n.parameters. For all the 41 parameters in this neural net, we actually want to change p.data slightly according to the gradient information. We need to do a tiny update in this gradient descent scheme. In gradient descent, we are thinking of the gradient as a vector pointing in the direction of increased loss. In gradient descent, we are modifying p.data by a small step size in the direction of the gradient. The step size, as an example, could be a very small number like 0.01 is the step size times p.grad. We have to think through some of the signs here. Working with this specific example here, we see that if we just left it like this, then this neuron’s value would be currently increased by a tiny amount of the gradient. The gradient is negative, so this value of this neuron would go slightly down. It would become like 0.84 or something like that. If this neuron’s value goes lower, that would actually increase the loss. That is because the derivative of this neuron is negative. Increasing this makes the loss go down, so increasing it is what we want to do instead of decreasing it. What we are missing here is actually a negative sign. This other interpretation is that we want to minimize the loss. We do not want to maximize the loss. We want to decrease it. The other interpretation, as I mentioned, is that you can think of the gradient vector, the vector of all the gradients, as pointing in the direction of increasing the loss. We want to decrease it, so we want to go in the opposite direction. You can convince yourself that this sort of plug does the right thing here with the negative because we want to minimize the loss. If we nudge all the parameters by a tiny amount, then we will see that this data will have changed a little bit. This neuron is now a tiny amount greater in value. 0.854 went to 0.857, and that is a good thing. Slightly increasing this neuron’s data makes the loss go down, according to the gradient. The correct thing has happened sign wise. Because we have changed all these parameters, we expect that the loss should have gone down a bit. We want to re-evaluate the loss. Let me do that outside here so that we can compare the two loss values. If I recalculate the loss, we expect the new loss now to be slightly lower than this number. Hopefully, what we are getting now is a tiny bit lower than 4.84, which is 4.36. Remember, the way we have arranged this is that low loss means that our predictions are matching the targets. Our predictions now are probably slightly closer to the targets. All we have to do is iterate this process. We have done the forward pass, and this is the loss. Now we can loss.backward. Let me take these out. We can do a step size, and now we should have a slightly lower loss. 4.36 goes to 3.9. We have done the forward pass, here is the backward pass, nudge. Now the loss is 3.66, 3.47, and you get the idea. We just continue doing this. This is gradient descent. We are iteratively doing a forward pass, backward pass, update, forward pass, backward pass, update. The neural net is improving its predictions. If we look at y_pred now, we see that this value should be getting closer to one. This value should be getting more positive. These should be getting more negative, and this one should be also getting more positive. If we iterate this a few more times, we may be able to go a bit faster. Let's try a slightly higher learning rate. There we go. Now we are at 0.31. If you go too fast, by the way, if you try to make it too big of a step, you may actually overstep it. It is overconfidence. Remember, we do not actually know exactly about the loss function. The loss function has all kinds of structure. We only know about the very local dependence of all these parameters on the loss. If we step too far, we may step into a part of the loss that is completely different, and that can destabilize training and make your loss actually blow up. The loss is now 0.04, so the predictions should be really quite close. Let's take a look. This is almost one, almost negative one, almost one. We can continue going. Backward, update. There we go. We went way too fast, and we actually overstepped. We got too eager. Where are we now? 7e-9, so this is very very low loss. The predictions are basically perfect. Somehow, we were doing way too big updates, and we briefly exploded. Somehow, we ended up getting into a really good spot. Usually, this learning rate and the tuning of it is a subtle art. You want to set your learning rate. If it is too low, you are going to take way too long to converge. If it is too high, the whole thing gets unstable, and you might actually even explode the loss, depending on your loss function. Finding the step size to be just right is a pretty subtle art sometimes when you are using vanilla gradient descent, but we happened to get into a good spot. We can look at n.parameters. This is the setting of weights and biases that makes our network predict the desired targets very very close. We have successfully trained the neural net. Let's make this a tiny bit more respectable and implement an actual training loop. This is the data definition that stays. This is the forward pass. For k in range, we are going to take a bunch of steps. First, you do the forward pass, we validate the loss. Let's re-initialize the neural net from scratch. Here is the data, and we first do a forward pass. Then, we do the backward pass, and then we do an update. That is gradient descent. We should be able to iterate this, and we should be able to print the current step, the current loss. Let's just print the number of the loss, and that should be it. The learning rate 0.01 is a little too small. 0.1 we saw is a little bit dangerously too high. Let's go somewhere in between. We will optimize this for not 10 steps, but let's go for say 20 steps. Let me erase all of this junk and run the optimization. You see how we have actually converged slower in a more controlled manner and got to a loss that is very low. I expect y_pred to be quite good. There we go. That is it. This is kind of embarrassing, but we actually have a really terrible bug in here. It is a subtle bug, and it is a very common bug. I cannot believe I have done it for the 20th time in my life, especially on camera. I could have reshot the whole thing, but I think it is pretty funny. You get to appreciate a bit what working with neural nets is like. Sometimes we are guilty of this bug. I have actually tweeted the most common neural net mistakes a long time ago. I am not really going to explain any of these, except that we are guilty of number three. You forgot to zero grad before the backward. What is that? Basically, what is happening, and it is a subtle bug, and I am not sure if you saw it, is that all of these weights here have a dot data and a dot grad. The grad starts at zero. Then, we do a backward, and we fill in the gradients. Then, we do an update on the data, but we do not flush the grad. It stays there. When we do the second forward pass and we do a backward again, remember that all the backward operations do a plus equals on the grad. These gradients just add up, and they never get reset to zero. We did not zero grad. Here is how we zero grad. Before the backward, we need to iterate over all the parameters, and we need to make sure that p.grad is set to zero. We need to reset it to zero just like it is in the constructor. Remember, all the way here, for all these value nodes, the grad is reset to zero. All these backward passes do a plus equals on that grad. We need to make sure that we reset these grads to zero so that when we do backward, all of them start at zero. The actual backward pass accumulates the loss derivatives into the grads. This is zero grad in PyTorch. We will get a slightly different optimization. Let's reset the neural net. The data is the same. This is now, I think, correct. We get a much more slower descent. We still end up with pretty good results. We can continue this a bit more to get down lower and lower and lower. The only reason that the previous thing worked, it is extremely buggy. The only reason that worked is that this is a very very simple problem, and it is very easy for this neural net to fit this data. The grads ended up accumulating, and it effectively gave us a massive step size, and it made us converge extremely fast. We have to do more steps to get to very low values of loss and get y_pred to be really good. We can try to step a bit greater. We are going to get closer and closer to one, minus one, and one. Working with neural nets is sometimes tricky because you may have lots of bugs in the code. Your network might actually work just like ours worked, but the chances are that if we had a more complex problem, then this bug would have made us not optimize the loss very well. We were only able to get away with it because the problem is very simple. Let's now bring everything together and summarize what we learned. What are neural nets? Neural nets are these mathematical expressions. Fairly simple mathematical expressions in the case of a multi-layer perceptron that take the data as input and they take the weights and the parameters of the neural net as input. A mathematical expression for the forward pass followed by a loss function. The loss function tries to measure the accuracy of the predictions. Usually, the loss will be low when your predictions are matching your targets, or where the network is behaving well. We manipulate the loss function so that when the loss is low, the network is doing what you want it to do on your problem. We backward the loss. We use backpropagation to get the gradient. We know how to tune all the parameters to decrease the loss locally, but then we have to iterate that process many times in what is called gradient descent. We simply follow the gradient information. That minimizes the loss. The loss is arranged so that when the loss is minimized, the network is doing what you want it to do. We just have a blob of neural stuff, and we can make it do arbitrary things. That gives neural nets their power. This is a very tiny network with 41 parameters, but you can build significantly more complicated neural nets with billions, at this point almost trillions of parameters. It is a massive blob of neural tissue, simulated neural tissue roughly speaking. You can make it do extremely complex problems. These neurons then have all kinds of very fascinating emergent properties when you try to make them do significantly hard problems, as in the case of GPT, for example. We have massive amounts of text from the internet. We are trying to get a neural net to predict, to take a few words and try to predict the next word in a sequence. That is the learning problem. It turns out that when you train this on all of the internet, the neural net actually has really remarkable emergent properties. That neural net would have hundreds of billions of parameters, but it works on fundamentally the exact same principles. The neural net of course will be a bit more complex, but otherwise the value in the gradient would be identical. The gradient descent would be there and would be basically identical. People usually use slightly different updates. This is a very simple stochastic gradient descent update. The loss function would not be mean squared error. They would be using something called the cross-entropy loss for predicting the next token. There are a few more details, but fundamentally the neural network setup and neural network training is identical and pervasive. You now understand intuitively how that works under the hood. In the beginning of this video, I told you that by the end of it you would understand everything in micrograd, and then we would slowly build it up. Let me briefly prove that to you. I am going to step through all the code that is in micrograd as of today. Some of the code will change by the time you watch this video because I intend to continue developing micrograd. Let's look at what we have. Init.py is empty. When you go to engine.py, that has the value. Everything here you should mostly recognize. We have the data, grad attributes, we have the backward function, we have the previous set of children and the operation that produced this value. We have addition, multiplication, and raising to a scalar power. We have the ReLU non-linearity, which is a slightly different type of non-linearity than tanh that we used in this video. Both of them are non-linearities. Tanh is not actually present in micrograd as of right now, but I intend to add it later with the backward, which is identical. All these other operations are built up on top of the operations here. Values should be very recognizable except for the non-linearity used in this video. There is no massive difference between ReLU, tanh, sigmoid, and these other non-linearities. They are all roughly equivalent and can be used in MLPs. I used tanh because it is a bit smoother, and because it is a little bit more complicated than ReLU. It stressed a little bit more the local gradients and working with those derivatives, which I thought would be useful. nn.py is the neural networks library as I mentioned. You should recognize identical implementation of neuron, layer, and MLP. Notably, we have a class module here. There is a parent class of all these modules. I did that because there is an nn.module class in PyTorch, and so this exactly matches that API. nn.module in PyTorch also has a zero grad, which I have refactored out here. That is the end of micrograd. There is a test, which you will see basically creates two chunks of code, one in micrograd and one in PyTorch. We will make sure that the forward and the backward pass agree identically for a slightly less complicated expression. A slightly more complicated expression, everything agrees. We agree with PyTorch on all of these operations. Finally, there is a demo.ipynb here. It is a bit more complicated binary classification demo than the one I covered in this lecture. We only had a tiny data set of four examples. Here, we have a bit more complicated example with lots of blue points and lots of red points. We are trying to build a binary classifier to distinguish two-dimensional points as red or blue. It is a bit more complicated MLP. It is a bigger MLP. The loss is a bit more complicated because it supports batches. Because our dataset was so tiny, we always did a forward pass on the entire dataset of four examples. When your dataset is like a million examples, we pick out some random subset. We call that a batch, and then we only process the batch forward, backward, and update. We do not have to forward the entire training set. This supports batching because there are a lot more examples here. We do a forward pass. The loss is slightly more different. This is a max margin loss that I implement here. The one that we used was the mean squared error loss because it is the simplest one. There is also the binary cross-entropy loss. All of them can be used for binary classification and do not make too much of a difference in the simple examples that we looked at so far. There is something called L2 regularization used here. This has to do with generalization of the neural net and controls the overfitting in machine learning settings, but I did not cover these concepts in this video. Potentially later. The training loop, you should recognize. Forward, backward with zero grad, and update. In the update, the learning rate is scaled as a function of the number of iterations, and it shrinks. This is something called learning rate decay. In the beginning, you have a high learning rate. As the network stabilizes near the end, you bring down the learning rate to get some of the fine details in the end. We see the decision surface of the neural net. It learns to separate out the red and the blue area based on the data points. That is the slightly more complicated example and the demo that you are free to go over. As of today, that is micrograd. I also wanted to show you a little bit of real stuff so that you get to see how this is actually implemented in a production-grade library like PyTorch. I wanted to find and show you the backward pass for tanh in PyTorch. In micrograd, we see that the backward pass for tanh is one minus t squared, where t is the output of tanh of x times of that grad, which is the chain rule. We are looking for something that looks like this. I went to PyTorch, which has an open-source GitHub codebase. I looked through a lot of its code. I spent about 15 minutes and could not find tanh. These libraries unfortunately grow in size and entropy. If you just search for tanh, you get apparently 2,800 results in 406 files. I do not know what these files are doing and why there are so many mentions of tanh. These libraries are quite complex, and they are meant to be used, not really inspected. Eventually, I did stumble on someone who tries to change the tanh backward...
