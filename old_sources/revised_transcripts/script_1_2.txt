We want to chain `outgrad` into `self.grad` and `others.grad`. This will be a small piece of the chain rule for multiplication. So, what should this be? Think through it. What is the local derivative here? The local derivative was `others.data`, and then times the grad, that's chained. Here, we have `self.data` times the grad. That's what we've been doing. Finally, for the tanh backward, we want to set out backward to be just backward. Here, we need to back propagate. We have `out.grad` and want to chain it into `self.grad`. `Self.grad` will be the local derivative of this operation, which is tanh. We saw that the local gradient is 1 minus the tanh of x squared, which here is `t`. That's the local derivative because `t` is the output of this tanh. So, 1 minus `t` squared is the local derivative, and then the gradient has to be multiplied because of the chain rule. Thus, `outgrad` is chained through the local gradient into `self.grad`. That should be basically it. We are going to redefine our value node. We are going to swing all the way down here and redefine our expression, making sure that all the grads are zero. Now, we don't have to do this manually anymore. We are going to be calling the `backward` method in the right order. First, we want to call `o.backward`. `O` was the outcome of tanh, right? Calling `o.backward` will execute this function. This is what it will do. Now we have to be careful because there's a times `out.grad`. Remember `out.grad` is initialized to zero. Here we see `grad` zero. As a base case, we need to set `o.grad` to 1.0 to initialize it. Once this is 1, we can call `o.backward`. This should propagate the grad through tanh, so the local derivative times the global derivative which is initialized at one. This should work. I thought about redoing it, but I figured I should just leave the error in here because it's pretty funny. Why is an object not callable? It's because I screwed up. We're trying to save these functions. This is correct. Here, we don't want to call the function because that returns `None`. These functions return `None`. We just want to store the function. Let me redefine the value object. Then we're going to come back, redefine the expression, and redraw. Everything is great. `o.grad` is one. `o.grad` is one. Now this should work. Okay, so `o.backward` should have this gradient as 0.5. If we redraw, and if everything went correctly, 0.5, yay! So, now we need to call `n.backward`. That seems to have worked. So, `n.backward` routed the gradient to both of these. This is looking great. Now, we could, of course, call `b.backward`. What's going to happen? Well, `b` doesn't have a backward. `B's` backward is by initialization the empty function, so nothing would happen. We can call it on it, but when we call this one's backward, then we expect this 0.5 to get further routed, right? So, there we go, 0.5, 0.5. Finally, we want to call it here on `x2`, `w2`, and on `x1`, `w1`. Do both of those, and there we go. We get 0, 0.5, -1.5, and 1, exactly as we did before, but now we've done it through calling that backward sort of manually. We have one last piece to get rid of, which is us calling `_backward` manually. Let's think through what we are actually doing. We've laid out a mathematical expression, and now we're trying to go backwards through that expression. Going backwards through the expression just means that we never want to call a `backward` for any node before we've done everything after it. So, we have to do everything after it before we're ever going to call `backward` on any node. We have to get all of its full dependencies. Everything that it depends on has to propagate to it before we can continue back propagation. This ordering of graphs can be achieved using something called topological sort. Topological sort is basically laying out a graph such that all the edges go only from left to right. Here we have a graph. It's a directed acyclic graph (DAG). This shows two different topological orders of it, where basically you'll see that it's laying out the nodes such that all the edges go only one way from left to right. Implementing topological sort, you can look in Wikipedia. I'm not going to go through it in detail. Basically, this is what builds a topological graph. We maintain a set of visited nodes, and then we are going through, starting at some root node, which for us is `o`. That's where we want to start the topological sort. Starting at `o`, we go through all of its children, and we need to lay them out from left to right. Basically, this starts at `o`. If it's not visited, then it marks it as visited. Then, it iterates through all of its children and calls `build_topological` on them. After it's gone through all the children, it adds itself. This node that we're going to call it on, like say `o`, is only going to add itself to the `topo_list` after all of the children have been processed. That's how this function is guaranteeing that you're only going to be in the list once all your children are in the list. That's the invariant that is being maintained. If we build upon `o` and then inspect this list, we're going to see that it ordered our value objects. The last one is the value of 0.707, which is the output, so this is `o`. Then, this is `n`, and then all the other nodes get laid out before it. That builds the topological graph. Really, what we're doing now is we're just calling `_backward` on all of the nodes in a topological order. If we just reset the gradients, they're all zero. What did we do? We started by setting `o.grad` to be 1. That's the base case. Then, we built the topological order, and then we went for `node` in the reversed `topo`. Now, in the reverse order because this list goes from you know, we need to go through it in reverse order, so starting at `o`, `node.backward`, and this should be it. There we go. Those are the correct derivatives. Finally, we are going to hide this functionality. I'm going to copy this, and we're going to hide it inside the `Value` class because we don't want to have all that code lying around. Instead of an `_backward`, we're now going to define an actual `backward`, so that's `backward` without the underscore. That's going to do all the stuff that we just arrived at. Let me just clean this up a little bit. So, we're first going to build a topological graph starting at `self`. `Build_topo(self)` will populate the topological order into the `topo_list`, which is a local variable. Then we set `self.grad` to be 1. Then, for each node in the reversed list, so starting at us and going to all the children, we call `_backward`. That should be it. Save. Come down here. Redefine. Okay, all the grads are zero. Now, what we can do is `o.backward`, without the underscore, and there we go. That's back propagation in place for one neuron. Now, we shouldn't be too happy with ourselves actually, because we have a bad bug, and we have not surfaced the bug because of some specific conditions that we are in. We have to think about it right now. Here's the simplest case that shows the bug. Say I create a single node `a`, and then I create a `b` that is `a` plus `a`, and then I call backward. What's going to happen is `a` is 3, and then `b` is `a` plus `a`, so there are two arrows on top of each other here. We can see that `b` is, of course, the forward pass works. `b` is just `a` plus `a`, which is 6. But, the gradient here is not actually correct that we calculate it automatically, and that's because just doing calculus in your head, the derivative of `b` with respect to `a` should be two, 1 plus 1. It's not one. Intuitively, what's happening here? `B` is the result of `a` plus `a`, and then we call `backward` on it. Let's go up and see what that does. `B` is a result of addition. So, out is `b`, and then when we called `backward`, what happened is `self.grad` was set to 1, and then `other.grad` was set to 1. But, because we're doing `a` plus `a`, self and other are actually the exact same object. We are overriding the gradient. We are setting it to 1, and then we are setting it again to 1. That's why it stays at 1. That's a problem. There's another way to see this in a little bit more complicated expression. Here we have `a` and `b`, and then `d` will be the multiplication of the two, and `e` will be the addition of the two, and then we multiply `e` times `d` to get `f`. Then, we called `f.backward`, and these gradients, if you check, will be incorrect. Fundamentally what's happening here again is basically we're going to see an issue anytime we use a variable more than once. Until now, in these expressions above, every variable is used exactly once, so we didn't see the issue. But, here, if a variable is used more than once, what's going to happen during the backward pass? We're backpropagating from `f` to `e` to `d`, so far so good. But now, `e.backward` and it deposits its gradients to `a` and `b`. Then, we come back to `d` and call `backward`, and it overwrites those gradients at `a` and `b`. That's obviously a problem. The solution here, if you look at the multivariate case of the chain rule and its generalization, the solution is basically that we have to accumulate these gradients. These gradients add, and so instead of setting those gradients, we can simply do `+=`. We need to accumulate those gradients: `+=`, `+=`, `+=`, `+=`. This will be okay, remember, because we are initializing them at zero, so they start at zero, and then any contribution that flows backwards will simply add. Now, if we redefine this one because of the `+=`, this now works because `a.grad` started at zero, and we called `b.backward`. We deposit one, and then we deposit one again, and now this is two, which is correct. Here, this will also work, and we'll get correct gradients. Because when we call `e.backward`, we will deposit the gradients from this branch, and then we get back into `d.backward`. It will deposit its own gradients, and those gradients simply add on top of each other, so we just accumulate those gradients and that fixes the issue. Okay, now before we move on, let me actually do a bit of cleanup here and delete some of this intermediate work, so we're not going to need any of this now that we've derived all of it. We are going to keep this because I want to come back to it. Delete the tanh, delete our morning example, delete the step, delete this, keep the code that draws, and then delete this example. Leave behind only the definition of value. Now, let's come back to this non-linearity here that we implemented, the tanh. Now, I told you that we could have broken down tanh into its explicit atoms in terms of other expressions if we had the `exp` function. If you remember, tanh is defined like this. We chose to develop tanh as a single function, and we can do that because we know its derivative and can back propagate through it. But we can also break down tanh and express it as a function of `exp`, and I would like to do that now because I want to prove to you that you get all the same results and all those gradients, but also because it forces us to implement a few more expressions. It forces us to do exponentiation, addition, subtraction, division, and things like that, and I think it's a good exercise to go through a few more of these. Okay, let's scroll up to the definition of value, and here one thing that we currently can't do is we can do like a value of say 2.0, but we can't do, you know, here for example, we want to add constant one, and we can't do something like this. We can't do it because it says object has no attribute data. That's because `a + 1` comes right here to `add`, and then `other` is the integer one. Then, here python is trying to access `1.data`, and that's not a thing. That's because basically one is not a value object, and we only have addition for value objects. As a matter of convenience, so that we can create expressions like this and make them make sense, we can simply do something like this. Basically, we let `other` alone if `other` is an instance of `Value`. If it's not an instance of `Value`, we're going to assume that it's a number like an integer or float. We're going to simply wrap it in `Value`, and then `other` will just become `Value(other)`. Then, `other` will have a `data` attribute, and this should work. If I just say this, redefine value, then this should work. There we go. Okay, now let's do the exact same thing for multiply because we can't do something like this again for the exact same reason, so we just have to go to `mul`. If `other` is not a value, then let's wrap it in `Value`. Let's redefine value, and now this works. Now, here's a kind of unfortunate and not obvious part. `a * 2` works. We saw that, but `2 * a`, is that going to work? You'd expect it to, right? But actually, it will not. The reason it won't is because python doesn't know. When you do `a * 2`, python will go and it will basically do something like `a.mul(2)`. That's basically what it will call. But, `2 * a` is the same as `2.mul(a)`, and 2 can't multiply a value, and so it's really confused about that. Instead, what happens is in python, the way this works is you are free to define something called the `rmul`. The `rmul` is kind of like a fallback. If python can't do `2 * a`, it will check if, by any chance, `a` knows how to multiply `2`, and that will be called into `rmul`. Because python can't do `2 * a`, it will check, is there an `rmul` in `Value`, and because there is, it will now call that. What we'll do here is we will swap the order of the operands. Basically, `2 * a` will redirect to `rmul`, and `rmul` will basically call `a * 2`, and that's how that will work. Redefining now with `rmul`, `2 * a` becomes 4. Okay, now looking at the other elements that we still need, we need to know how to exponentiate and how to divide. Let's first look at the exponentiation part. We're going to introduce a single function `exp` here, and `exp` is going to mirror tanh in the sense that it's a simple single function that transforms a single scalar value and outputs a single scalar value. We pop out the python number, we use `math.exp` to exponentiate it, create a new value object, everything that we've seen before. The tricky part, of course, is how do you propagate through `e` to the `x`, and so here you can potentially pause the video and think about what should go here. Basically, we need to know what is the local derivative of `e` to the `x`. The derivative with respect to x of `e` to the `x` is famously just `e` to the `x`. We've already just calculated `e` to the `x`, and it's inside `out.data`. We can do `out.data` times `out.grad`. That's the chain rule, so we're just chaining onto the current running grad, and this is what the expression looks like. It looks a little confusing, but this is what it is. That's the exponentiation. Redefining, we should now be able to call `a.exp`, and hopefully the backward pass works as well. Okay, and the last thing we'd like to do of course is we'd like to be able to divide. Now, I actually will implement something slightly more powerful than division because division is just a special case of something a bit more powerful. In particular, just by rearranging, if we have some kind of a `b = Value(4.0)` here, we'd like to basically be able to do `a / b`, and we'd like this to be able to give us 0.5. Division actually can be reshuffled as follows. If we have `a / b`, that's actually the same as `a * (1 / b)`. That's the same as `a` multiplying `b` to the power of negative one, and so what I'd like to do instead is I basically like to implement the operation of `x` to the `k` for some constant `k`. It's an integer or a float, and we would like to be able to differentiate this. Then, as a special case, negative one will be division. I'm doing that just because it's more general, and you might as well do it that way. Basically, what I'm saying is we can redefine division. We will put it here somewhere. Yeah, we can put it here somewhere. What I'm saying is that we can redefine division, so `self / other` can actually be rewritten as `self * (other ** -1)`. Now, a value raised to the power of negative one, we have now defined that. So, here's, so we need to implement the `pow` function. Where am I going to put the power function? Maybe here somewhere. This is the skeleton for it. This function will be called when we try to raise a value to some power, and `other` will be that power. Now, I'd like to make sure that other is only an `int` or a `float`. Usually, `other` is some kind of a different value object, but here, `other` will be forced to be an `int` or a `float`. Otherwise, the math won't work for or try to achieve in the specific case that would be a different derivative expression if we wanted other to be a value. So here, we create the output value which is just, you know, this `data` raised to the power of `other`. `Other` here could be, for example, negative one, that's what we are hoping to achieve. Then, this is the backwards stub, and this is the fun part, which is what is the chain rule expression here for back propagating through the power function where the power is to the power of some kind of a constant. This is the exercise. Maybe pause the video here and see if you can figure it out yourself as to what we should put here. You can actually go here and look at derivative rules as an example, and we see lots of derivatives that you can hopefully know from calculus. In particular, what we're looking for is the power rule because that's telling us that if we're trying to take the derivative of `x` to the `n`, which is what we're doing here, then that is just `n` times `x` to the `n` minus 1, right? Okay, so that's telling us about the local derivative of this power operation. All we want here, basically, `n` is now other, and `self.data` is `x`. This now becomes `other`, which is `n`, times `self.data`, which is now a python `int` or a float. It's not a value object. We're accessing the `data` attribute, raised to the power of `other` minus one, or `n` minus one. I can put brackets around this, but this doesn't matter because power takes precedence over multiply in python. That would have been okay. That's the local derivative only. Now, we have to chain it, and we chain just simply by multiplying by `out.grad`, that's the chain rule, and this should technically work, and we're going to find out soon. But now, if we do this, this should now work, and we get 0.5, so the forward pass works, but does the backward pass work? I realized that we actually also have to know how to subtract. Right now, `a - b` will not work. To make it work, we need one more piece of code here. Basically, this is the subtraction. The way we're going to implement subtraction is we're going to implement it by addition of a negation. To implement negation, we're going to multiply by negative one. Again, just using the stuff we've already built, expressing it in terms of what we have. `a - b` is now working. Okay, so now let's scroll again to this expression here for this neuron. Let's just compute the backward pass here once we've defined `o`, and let's draw it. Here are the gradients for all these leaf nodes for this two-dimensional neuron that has a tanh that we've seen before. Now, what I'd like to do is I'd like to break up this tanh into this expression here, so let me copy paste this here. Now, instead of, we'll preserve the label, and we will change how we define `o`. In particular, we're going to implement this formula here, so we need `e` to the `2x` minus 1 over `e` to the `x` plus 1. So, `e` to the `2x`, we need to take 2 times `n`, and we need to exponentiate it. That's `e` to the `2x`. Then, because we're using it twice, let's create an intermediate variable, `e`. Define `o` as `(e - 1) / (e + 1)`. That should be it. Then, we should be able to draw that of `o`. Before I run this, what do we expect to see? Number one, we're expecting to see a much longer graph here because we've broken up tanh into a bunch of other operations. Those operations are mathematically equivalent, and so what we're expecting to see is, number one, the same result here, so the forward pass works. Number two, because of that mathematical equivalence, we expect to see the same backward pass, the same gradients on these leaf nodes. These gradients should be identical. Let's run this. Number one, let's verify that instead of a single tanh node, we have now exp, we have plus, we have times negative one. This is the division. We end up with the same forward pass here. Then, the gradients, we have to be careful because they're in slightly different order. Potentially, the gradients for `w2x2` should be 0 and 0.5. `w2` and `x2` are 0 and 0.5. `w1` and `x1` are 1 and -1.5. 1 and -1.5. That means that both our forward passes and backward passes were correct because this turned out to be equivalent to tanh before. The reason I wanted to go through this exercise is number one, we got to practice a few more operations and writing more backward passes. Number two, I wanted to illustrate the point that the level at which you implement your operations is totally up to you. You can implement backward passes for tiny expressions like a single individual plus, or a single times, or you can implement them for say tanh, which is kind of a potentially, you can see it as a composite operation because it's made up of all these more atomic operations. Really, all of this is kind of like a fake concept. All that matters is we have some kind of inputs, some kind of output, and this output is a function of the inputs in some way. As long as you can do forward pass and the backward pass of that little operation, it doesn't matter what that operation is and how composite it is. If you can write the local gradients, you can chain the gradient, and you can continue back propagation. The design of what those functions are is completely up to you. Now, I would like to show you how you can do the exact same thing by using a modern deep neural network library like, for example, PyTorch, which I've roughly modeled micrograd by. PyTorch is something you would use in production. I'll show you how you can do the exact same thing but in PyTorch API. I'm just going to copy paste it in and walk you through it a little bit. This is what it looks like. We're going to import PyTorch. Then, we need to define these value objects like we have here. Micrograd is a scalar valued engine, so we only have scalar values like 2.0. But, in PyTorch, everything is based around tensors. Like I mentioned, tensors are just n-dimensional arrays of scalars. That's why things get a little bit more complicated here. I just need a scalar value to a tensor, a tensor with just a single element. By default, when you work with PyTorch, you would use more complicated tensors like this. If I import PyTorch, I can create tensors like this. This tensor, for example, is a two by three array of scalar scalars in a single compact representation. We can check its shape. We see that it's a two by three array, and so on. This is usually what you would work with in the actual libraries. Here, I'm creating a tensor that has only a single element, 2.0. Then, I'm casting it to be double because python is by default using double precision for its floating-point numbers. I'd like everything to be identical. By default, the data type of these tensors will be float32, so it's only using a single precision float. I'm casting it to double so that we have float64, just like in python. I'm casting to double, and then we get something similar to `Value(2)`. The next thing I have to do is because these are leaf nodes, by default PyTorch assumes that they do not require gradients. I need to explicitly say that all of these nodes require gradients. This is going to construct scalar valued one element tensors, making sure that PyTorch knows that they require gradients. By default, these are set to false, by the way, because of efficiency reasons. Usually, you would not want gradients for leaf nodes like the inputs to the network. This is just trying to be efficient in the most common cases. Once we've defined all of our values in python, we can perform arithmetic just like we can here in microgradland, so this will just work. There's a `torch.tanh` also. When we get back is a tensor again. We can just like in micrograd, it's got a data attribute and it's got a grad attribute. These tensor objects just like in micrograd have a data and a grad. The only difference here is that we need to call that item because otherwise, PyTorch that item basically takes a single tensor of one element, and it just returns that element, stripping out the tensor. Let me just run this, and hopefully we are going to get, this is going to print the forward pass, which is 0.707, and this will be the gradients, which hopefully are 0, 0.5, -1.5 and 1. If we just run this, there we go, 0.7. So, the forward pass agrees. Then, 0.5, 0, -1.5 and 1, so PyTorch agrees with us. Just to show you here, basically `o` here's a tensor with a single element, and it's a double. We can call that `item` on it to just get the single number out. That's what item does. `O` is a tensor object like I mentioned, and it's got a `backward` function just like we've implemented, and then all of these also have a `grad`. Like, `x2` for example, in the grad, it's a tensor, and we can pop out the individual number with that item. Basically, Torch can do what we did in micrograd. It's a special case when your tensors are all single element tensors. The big deal with PyTorch is that everything is significantly more efficient because we are working with these tensor objects, and we can do lots of operations in parallel on all of these tensors. Otherwise, what we've built very much agrees with the API of PyTorch. Now that we have some machinery to build out pretty complicated mathematical expressions, we can also start building out neural nets, and as I mentioned, neural nets are just a specific class of mathematical expressions. We're going to start building out a neural net piece by piece, and eventually, we'll build out a two-layer multi-layer layer perceptron, as it's called. I'll show you exactly what that means. Let's start with a single individual neuron. We've implemented one here, but here I'm going to implement one that also subscribes to the PyTorch API in how it designs its neural network modules. Just like we saw that we can match the API of PyTorch on the auto grad side, we're going to try to do that on the neural network modules. Here's the class `Neuron`. Just for the sake of efficiency, I'm going to copy paste some sections that are relatively straightforward. The constructor will take the number of inputs to this neuron, which is how many inputs come to a neuron. This one, for example, has three inputs. Then, it's going to create a weight, there is some random number between -1 and 1, for every one of those inputs, and a bias that controls the overall trigger happiness of this neuron. Then we're going to implement a `def __call__` of `self` and `x`, some input `x`. Really what we don't do here is `w * x + b`, where `w * x` here is a dot product specifically. If you haven't seen `call`, let me just return `0.0` here for now. The way this works now is we can have an `x` which is say like `2.0, 3.0`. Then, we can initialize a neuron that is two-dimensional because these are two numbers. Then, we can feed those two numbers into that neuron to get an output, and so when you use this notation `n(x)`, python will use `call`. Currently, `call` just returns `0.0`. Now, we'd like to actually do the forward pass of this neuron instead. We're going to do here first is we need to basically multiply all of the elements of `w` with all of the elements of `x` pairwise. We need to multiply them. The first thing we're going to do is we're going to zip up `self.w` and `x`. In python, zip takes two iterators, and it creates a new iterator that iterates over the tuples of the corresponding entries. For example, just to show you, we can print this list and still return 0.0 here. So, we see that these `w`s are paired up with the `x`s. `w` with `x`. Now what we want to do is for `wi`, `xi` in, we want to multiply `wi` times `xi`. Then, we want to sum all of that together to come up with an activation, and add also `self.b` on top. That's the raw activation. Then, of course, we need to pass that through a non-linearity, so what we're going to be returning is `act.tanh`. Here's out. Now, we see that we are getting some outputs, and we get a different output from a neuron each time because we are initializing different weights and biases. To be a bit more efficient here, actually, `sum` by the way takes a second optional parameter which is the `start`. By default, the start is zero, so these elements of this sum will be added on top of zero to begin with. We can just start with `self.b`. Then we just have an expression like this. The generator expression here must be parenthesized in python. There we go. Yep. So, now we can forward a single neuron. Next up, we're going to.
