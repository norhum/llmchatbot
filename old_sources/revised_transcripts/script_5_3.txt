In a backward pass, we have the derivative of the loss with respect to all elements of Y, which is a vector with multiple numbers. We also have gamma and beta, and this is like the compute graph.  We have x-hat, mu, sigma squared, and x. We are given DL by DYI and we want DL by dxi for all the Is in these vectors.  It’s important to note that while x, x-hat, and Y contain multiple nodes, mu and sigma squared are individual scalars, and you have to consider this or you'll get your math wrong. I suggest going in the following order for backpropagation: x-hat, then sigma squared, then mu, and then x.  This is similar to a topological sort; in micrograd we would go from right to left, and we’re doing the same with symbols on paper. To get DL by dxi-hat, we take DL by DYI and multiply it by gamma, because any individual Yi is gamma times x i-hat plus beta. This gives us the derivatives for all the x-hats. Now, let's derive DL by D sigma squared. We must remember that there are many x-hats, and sigma squared is a single number. Sigma squared has a large fan out, with many arrows going to the x-hats. There's a back propagating signal from each x-hat into sigma squared, so we sum over all Is from 1 to m of DL by dxi-hat times dxi-hat by D sigma squared, which is the local gradient.  Mathematically, this simplifies to a specific expression for DL by D sigma squared. We will use this when backpropagating into mu, and then x.  Now let's backpropagate into mu.  Mu influences all the x-hats, so if our mini-batch size is 32, there are 32 numbers and 32 arrows going back to mu.  Mu going to sigma squared is a single arrow since sigma squared is a scalar.  So, in total, there are 33 arrows emanating from mu, and they all have gradients going into mu that must be summed up.  Thus, when looking at the expression for DL by D mu, I am summing over all the gradients of DL by dxi-hat times dxi-hat by D mu, plus the one arrow, which is DL by D sigma squared times D sigma squared by D mu. After simplifying, the first term is straightforward, and for the second term, if we assume mu is the average of the xI's, as it is in this case, then the gradient D sigma squared by D mu vanishes and becomes zero, canceling out the entire second term, leaving us with a straightforward expression for DL by D mu. Now for deriving DL by dxi, first count how many numbers are inside X; there are 32 numbers, each xI. From each xI, there's an arrow to mu, to sigma squared, and to x-hat. Each x i-hat is a function of xI, so there are 32 parallel arrows going between x and x-hat. Each xI has three arrows emanating from it: to mu, to sigma squared, and to the associated x-hat.  We apply the chain rule and add the contributions from these three paths. This means chaining through mu, sigma squared, and x-hat. We have DL by dxi-hat, DL by D mu, and DL by D sigma squared, but we need dxi-hat by dxi, D mu by dxi, and D sigma squared by dxi. You can derive these by differentiating the expressions. After doing this, we plug everything together, multiplying the terms and adding them up. When plugging in the DL by D sigma squared term, we use a different iterator, J, to avoid mixing up the local iterator within the expression with the iterator over the batch elements, I. After simplifying, some terms can be factored, and we end up with a fairly simple mathematical expression that uses the derivatives from earlier steps. This expression gives us DL by dxi for all the Is. This is the end of the backward pass analytically. I implemented the expression into a single line of code, and you can see the max difference is tiny, so this is the correct implementation.  Getting this formula was not trivial and involves considering that this formula is for a single neuron and a batch of 32 examples, whereas we have 64 neurons and this expression has to evaluate the backward pass for all of them in parallel. Additionally, we need to make sure the sums broadcast correctly onto everything else. After verifying that the shapes are correct, and that this expression works, you can also check against PyTorch and get the same answer, giving confidence in its correctness.  Exercise four involves putting everything together.  We re-initialize the neural net and, instead of calling loss.backward, we use our manual backpropagation. By copying and pasting the code we derived, we can drive our own gradients and optimize the neural net. I achieved a good loss, and that is expected because we replaced loss.backward with our own derived code, and the gradients are identical.  This gives full visibility on what's under the hood. The full backward pass includes backpropagating through cross entropy, the second layer, the tanh nonlinearity, batch normalization, the first layer, and the embedding. This entire process is only about 20 lines of code and gives us gradients and allows us to remove loss.backward. After filling in the manual backpropagation, this code should run for 100 iterations and then break, so you can check that the manual gradients match the PyTorch gradients, which will be very close, with differences around 1e-9, which I don't know where it’s coming from exactly. Once confident, you can remove the break and also remove loss.backward. Then you don’t need pytorch’s grad, and you will update with the manually derived gradients. You can also add “with no_grad” to the training loop to make it more efficient, then you can run the optimization, and you'll see that the loss.backward isn't running. After finishing, I calibrated the batchnorm parameters, ran the loss and sampled from the model. The model worked and produced similar output as before. We estimated the gradients manually without using PyTorch's autograd. Hopefully, you see that the backward pass of this neural net is not too complicated, where each layer is a few lines of code, with batch normalization being an exception. This provides a good sense of how these backward passes work and how to derive them yourself. In the next lecture, we'll go into recurrent neural networks, LSTMs, and other variants of RNNs, to achieve better log likelihoods.
