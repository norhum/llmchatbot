Okay, here is the cleaned-up lecture transcript:

Today, we are continuing our implementation of makemore. In the last lecture, we implemented the bigram language model using both counts and a simple neural network with a single linear layer. We saw that we predicted the distribution for the next character in a sequence by looking at only the single previous character. We did this by taking counts and normalizing them into probabilities, so that each row sums to one. This works if you only have one character of previous context. However, the predictions from this model are not very good because it only considers one character of context, and thus the model did not produce name-like sounding results. If we take more context into account when predicting the next character, the size of the table grows exponentially with the length of the context. For example, if we take a single character, there are 27 possibilities.  If we take two characters, there are 729 possibilities. If we take three characters, there are 20,000 possibilities. There are way too many rows for the matrix, too few counts for each possibility, and the model does not work well. Therefore, today we will implement a multi-layer perceptron model to predict the next character in a sequence. This modeling approach follows the Bengio et al. 2003 paper. This is not the very first paper that proposed using multi-layer perceptrons for this task, but it is very influential. It is often cited to represent this idea and is a very nice write-up. I invite you to read it.  In the introduction, they describe the same problem that I just described. To address it, they propose the following model. Keep in mind that we are building a character-level language model, while in this paper, they have a vocabulary of 17,000 words and build a word-level language model. We will still stick with characters, but we will take the same modeling approach. They associate a 30-dimensional feature vector to each of the 17,000 words, embedding each word into a 30-dimensional space. In the beginning, these words are initialized completely randomly. We will tune these word embeddings using backpropagation. During training, these vectors will move around in the space. Words with similar meanings or synonyms might end up in a similar part of the space, and words with very different meanings would go elsewhere in the space. Their modeling approach is otherwise identical to ours. They use a multi-layer neural network to predict the next word given the previous words, and they maximize the log likelihood of the training data to train the network.
Here, they provide an example to illustrate why this approach works. Suppose you are trying to predict a word in the phrase "a dog was running in a blank." If this exact phrase has never occurred in the training data, the model is out of distribution. However, this approach allows you to get around that. You may not have seen the exact phrase, but maybe you have seen similar phrases. Perhaps your network has learned that 'a' and 'the' are often interchangeable and so their embeddings are nearby. Knowledge can transfer through this embedding, and generalization can occur. Similarly, the network could know that cats and dogs are animals and co-occur in similar contexts, even if you haven't seen them together in this exact phrase.

Let’s look at the diagram of the neural network. In this example, we are taking three previous words and trying to predict the fourth word. Every word is represented by its index in the vocabulary. Since there are 17,000 words, these are integers between 0 and 16,999. There is a lookup table called C, which is a matrix of 17,000 by 30. Every index is plucking out a row of this embedding matrix, converting each word to its corresponding 30-dimensional vector.  The input layer has 30 neurons for each of the three words, totaling 90 neurons. The matrix C is shared across all words. Next is the hidden layer, the size of which is a hyperparameter and can be as large or as small as you would like. For example, the size could be 100. We will evaluate multiple choices for the size of this hidden layer. All 100 neurons are fully connected to the 90 inputs. This is followed by a tanh non-linearity and then the output layer, which has 17,000 neurons because there are 17,000 possible words. All of these neurons are fully connected to the hidden layer. This layer has the most parameters and computation.  The 17,000 logits are passed to a softmax layer, where each logit is exponentiated, and then everything is normalized to sum to one, giving a probability distribution for the next word. During training, we have the label, which is the index of the next word.  This index is used to pluck out the probability of that word. We maximize the probability of that word with respect to the parameters of this neural network, including the weights and biases of the output layer, the hidden layer, and the embedding lookup table C. All of this is optimized using back propagation.

I have started a new notebook.  We are importing PyTorch and matplotlib.  I am reading all the names into a list of words and showing the first eight. There are 32,000 names in total. I am building out the vocabulary of characters and mappings from characters as strings to integers and vice versa.  The first thing we need to do is to compile the data set for the neural network. I have rewritten the code for that. I will explain it briefly after I run it. First, we define the `block_size`, which is the context length, or how many characters to use to predict the next one. In this example, we are taking three characters to predict the fourth one. Then we build out the `x` and `y` arrays.  The `x` array is the input to the neural net, and the `y` array contains the labels.  I am looping over the first five words for efficiency, but later we will use the entire training set. Here, I am printing the word ‘emma’ and showing the five examples generated from it. Given a context of “..”, the first character is 'e', and the label is 'm'. When the context is “..e”, the label is 'm', and so forth. I start with a padded context of zero tokens.  I iterate over all the characters, build the `y` array with the current character and the `x` array with the current context, print everything, and crop the context to include the new character. This creates a rolling window of context.  We can change the `block_size`, for example to four, five, or ten.  Currently, it is three, which matches the paper. Finally, the data set contains 32 examples from the five words. Each input to the neural net is three integers, and the label is also an integer.

Now, let's write a neural network that takes these inputs and predicts the outputs. First, let’s build the embedding lookup table `c`.  We have 27 characters and we will embed them into a lower dimensional space.  In the paper, they embed 17,000 words into a 30-dimensional space.  In our case, we have only 27 characters, so we will start with a two-dimensional space. The lookup table will have 27 rows and two columns of random numbers. Each of the 27 characters will have a two-dimensional embedding. Before we embed all the integers in `x`, let's embed a single integer, say 5, so we get a sense of how this works.  One way to do this is to index into row 5 of `c`. Another way is to use one-hot encoding.  We must make some type errors intentionally so you get a sense of how to fix them. So, we will make a one-hot vector for integer 5 with 27 classes. The input must be a torch tensor.  The shape of this tensor is 27, and the fifth dimension is one.  If we multiply this one-hot vector by `c`, we get an error because the types are mismatched.  We have to cast the one-hot vector to float.  The result is the same because all the zeros in the one-hot vector mask out everything in `c` except for the fifth row, which is plucked out. This tells you that we can think of the embedding as indexing into a lookup table or as the first layer of a neural net where we encode integers into one-hot vectors and feed those into a linear layer. We will use indexing because it is much faster. Now, embedding a single integer like 5 is easy, but how do we embed all of the 32 by 3 integers stored in `x`? PyTorch indexing is very flexible.  We can index using lists. We can index with a tensor of integers.  We can repeat rows.  We can also index with multi-dimensional tensors.  We can simply use `c` at `x`. The shape of this is 32 by 3 by 2. We are retrieving the embedding for every integer in `x`. PyTorch indexing is awesome. To embed all the integers in `x`, we simply use `c` of `x`. That's our embedding.
Now let’s construct the hidden layer. We have weights, `w1`, initialized randomly. The number of inputs is 3 times 2, or 6. The number of neurons is a variable, let’s say 100. The biases, `b1`, are also initialized randomly with 100 elements. We cannot directly multiply the embeddings with `w1` and add the bias because the embeddings are stacked up in the dimensions of the input tensor. We need to concatenate these inputs, so we can perform the multiplication. How do we transform the 32 by 3 by 2 tensor into a 32 by 6 tensor? There are usually many ways to implement the same thing in PyTorch. The documentation is extensive, containing many functions. We can use the `torch.cat` function, which concatenates a sequence of tensors in a given dimension. We can concatenate the three embeddings for each input. We want to grab all the examples, the zeroth index, and then all of this. This plucks out the 32 by 2 embeddings of just the first word. We treat this as a sequence and call `torch.cat`. We have to tell it along which dimension to concatenate, which is dimension 1. The result is a tensor of shape 32 by 6. We took 32 by 3 by 2 and flattened it to 32 by 6. This is not a great approach because it is hard-coded to a `block_size` of 3. If we changed it to 5, then we would need to rewrite our code. PyTorch comes to the rescue with `unbind`, which removes a tensor dimension and returns a tuple of all the slices. When we call `torch.unbind` on `m` along dimension 1, it gives us a list of tensors. We call `torch.cat` on this list along the first dimension. The result is the same, but this version works even if we change `block_size`. There is a more efficient way using `view` that hints at how tensors work internally. Let's create an array of elements from 0 to 17, with a shape of 18. We can represent this as tensors of different sizes by calling `view`. As long as the total number of elements remains the same, we can reshape a tensor in a variety of ways. This is an extremely efficient operation in PyTorch because the underlying storage, which is a one-dimensional vector, is not being changed. The only thing that is changed are attributes that dictate how the one-dimensional vector is interpreted. So, when we call `view`, no memory is being changed or created. There is a blog post called PyTorch Internals that explains this in more detail.  We can view our embeddings of the shape 32 by 3 by 2 instead as 32 by 6. PyTorch concatenates these two dimensions. This gives us the same result. So, we can view the embedding as a 32 by 6 tensor, which will work in our multiplication.  The shape of `h` is 100-dimensional activations for each of the 32 examples.  We need to apply the `tanh` non-linearity to get the final `h`.
The final h are now numbers between -1 and 1 with a shape of 32 by 100. This represents the hidden layer of activations. We have to be careful with the bias addition. The shape of `h` is 32 by 100, and the shape of `b1` is 100.  The broadcasting aligns on the right, and so it adds the same bias vector to all rows of the matrix. Now, let’s create the final layer, with `w2` and `b2`. The input is now 100. The output number of neurons will be 27 because we have 27 characters. The biases will also be 27.  The logits, the output of the neural net, are computed by multiplying `h` by `w2` and adding `b2`. The shape of the logits is 32 by 27. We want to exponentiate the logits to get the fake counts and then normalize them into a probability. We divide the counts by the sum along the first dimension, keeping the dimension. The shape of `prob` is now 32 by 27, and every row sums to one. Now we have the actual letter that comes next, in the array `y`, which was created during dataset creation.  We want to index into the rows of `prob` and pluck out the probability assigned to the correct character given by `y`. We create a range of 32, which is like an iterator over the rows of `prob`. For each row, we grab the column as given by `y`.  The probabilities for the correct characters are obtained. For some of the characters the probability is 0.2 but for others it is as low as 0.01. This means the network thinks some characters are very unlikely. However, we haven't trained the neural network yet. We take these probabilities, compute their log probability, average it, and then take the negative of it to create the negative log likelihood loss.  The loss is 17, which is the loss we would like to minimize.

I have rewritten everything to make it more respectable. Here’s our dataset. Here are all the parameters. I am now using a generator to make it reproducible. I have clustered all the parameters into a single list, so it is easy to count them. In total, there are currently about 3400 parameters. This is the forward pass, and we arrive at a single number, the loss. I would like to make this even more respectable. These lines where we take the logits and calculate the loss are reinventing the wheel. This is just classification, and many people use classification, and thus, there is a function `functional.cross_entropy` in PyTorch to calculate this more efficiently.
