Here is a cleaned-up version of the transcript:

"You can get evasion, where the model responds, 'I'm sorry, I can't hear you.' You can also get hallucinations. You can even get insults. For example, if you ask about Streamer Bot, the model might call you names or use strange humor. The model is effectively breaking when asked about simple strings like "at Roth" or "sold gold Magikarp." This is happening because there are a variety of documented behaviors related to trigger words. If you ask the model about these trigger words, or include them in your prompt, the model goes haywire and exhibits strange behaviors, including violating typical safety guidelines and the alignment of the model, such as swearing. 

This comes down to tokenization.  The string "sold gold Magikarp" is actually a Reddit user.  It is thought that the tokenization dataset was very different from the training dataset for the language model.  In the tokenization dataset, there was a lot of Reddit data where the user "sold gold Magikarp" was mentioned frequently.  Because this user was common, the string "sold gold Magikarp" would occur many times in the tokenization data set.  These tokens ended up being merged into a single individual token dedicated to that Reddit user.  There was likely a dedicated token for "sold gold Magikarp" in the 50,000 token vocabulary of GPT-2. 

However, when the language model was later trained, this Reddit data was not present.  The token "sold gold Magikarp" never appeared in the training set for the actual language model.  This token was initialized randomly and never updated during optimization. The corresponding row in the embedding table never gets sampled and never gets trained. It's like unallocated memory. At test time, if you evoke this token, youâ€™re pulling out a completely untrained row of the embedding table. This feeds into a transformer and creates undefined behavior because the model is out of sample and out of distribution.  Any of these weird tokens will evoke this type of behavior.

Different formats and representations, like different languages, are more or less efficient with GPT tokenizers.  For example, JSON is very dense in tokens, while YAML is more efficient. The same information encoded in JSON might use 116 tokens, while using only 99 tokens in YAML. In a token economy, where we are paying per token, you should prefer YAML over JSON. Tokenization density is important. You need to spend time in the tokenizer and measure the token efficiencies of different formats. 

Tokenization is critical to understand. It has many sharp edges, security issues, and AI safety issues, as we saw with the unallocated memory. Eternal glory goes to anyone who can get rid of it. I have shown one paper that tried to address this. I hope more will follow. 

If you can reuse GPT-4 tokens and vocabulary in your application, you should do so. Use tiktoken, because it is a very efficient and nice library for inference for BPE. I also like the byte level BPE that tiktoken and OpenAI use. If you want to train your own vocabulary from scratch, use BPE with SentencePiece. I am not a fan of SentencePiece because of its byte fallback and that it is doing BPE on Unicode code points.  SentencePiece has many settings and it's easy to miscalibrate them. You might end up cropping your sentences or something because of some parameter you do not fully understand. Be careful with the settings, and copy what Meta did, or spend a lot of time looking at the hyperparameters.  Even with the settings correct, the algorithm might be inferior to what is happening here.  If you need to train a vocabulary, maybe wait for the fast and efficient MBP. What we really want is tiktoken but with training code, which does not currently exist.

MBP is an implementation of it, but it is currently in Python.  That's all I have to say for tokenization. There might be an advanced and more detailed video in the future, but for now, that's it.

GPT-1 increased the context size from 512 to 1024 tokens. GPT-4 increased it further. Next, I would like to walk through the code from OpenAI on the GPT-2 encoded ATP. I am going to sneeze. Then what is happening here is this is a sparse layer that I will explain in a bit. What's happening here is..."
