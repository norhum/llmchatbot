Hi everyone, I hope you are well. Next, I'd like to build out "make more," a repository on my GitHub page. Similar to "micrograd," I'll build it step-by-step and spell everything out so we can build it slowly together. "Make more," as the name suggests, makes more of the things you give it. For example, "names.txt" is a dataset to make more. In "names.txt," you will find a very large dataset of names, about 32,000, which I found randomly on a government website. When you train "make more" on this dataset, it will learn to make more things like these, specifically things that sound name-like but are unique names. If you are looking for a cool, unique name for a baby, "make more" might help. Here are some example generations from the neural network after training: "dontel," "irot," and "zhendi," which sound name-like but are not actual names. Under the hood, "make more" is a character-level language model. This means that it treats every line as an example and each example as a sequence of individual characters. For example, "r e e s e" is a sequence of characters. The model learns to predict the next character in the sequence. We will implement various character-level language models, from simple bi-gram and bag-of-words models to multi-layer perceptrons, recurrent neural networks, and even modern transformers. The transformer we build will be similar to GPT-2, a modern network. By the end of this series, you will understand how that works on the character level. After characters, we will likely work on the word level to generate documents of words, then explore image and image-text networks, like Dolly and Stable Diffusion. But, for now, we must start with character-level language modeling. Let's begin.

We will start with a blank Jupyter Notebook page. First, I want to load the "names.txt" dataset. We'll open "names.txt" for reading, read everything into a massive string, and then split it by lines to get all the words as a Python list of strings. For example, the first ten words are "emma," "olivia," "eva," and so on. This list appears to be sorted by frequency. We want to learn more about this dataset. The total number of words is roughly 32,000. The shortest word is two characters long, and the longest word is 15 characters long.

Let's think through our first language model. A character-level language model predicts the next character in a sequence, given a sequence of characters. Every word, like "isabella," contains multiple examples. "Isabella" tells us that 'i' is likely to come first, 's' is likely to come after 'i', 'a' is likely to come after 'is', 'b' is likely to come after 'isa', and so on. Also, after "isabella," the word is likely to end. There's a lot of statistical structure packed into a single word, and we have 32,000 of them.

We will begin by building a bi-gram language model. It only considers two characters at a time. We're always looking at the previous character to predict the next one. It's a simple model but a great place to start. Let's look at the bi-grams in our dataset, which are two characters in a row. For every word, we will iterate through its consecutive characters. A good way to do this in Python is using `zip`. For example, with "emma," the `zip` function will print "em," "mm," and "ma". The string "emma" is "w," and the string "mma" is "w[1:]". `Zip` pairs them up and creates an iterator over tuples of consecutive entries. If one list is shorter, it halts. Therefore, we return "em", "mm", and "ma". We also know that 'e' is likely to come first, and 'a' is likely to come last. We will create a special array called "all characters" and add special start and end tokens. We will use the start token plus the characters in a word plus the special end character. Then iterating over the characters will give us diagrams, like "start e," "e m", "m m," "m a", and "a end."

To learn the statistics of which characters follow others, we'll count how often these combinations occur. We'll use a dictionary called "b," which will map a bi-gram to its counts. We will iterate over the words, and if the bi-gram is already in the dictionary, we'll increment its count. Otherwise, we'll add it with a count of one. Let's look at the dictionary "b". Many bi-grams occur once. "a" followed by the end character occurred three times because all of "emma", "olivia", and "eva" all ended in "a". Now, we can apply this to all the words. Now "b" contains the statistics of the entire dataset. We can look at the most and least common bi-grams. Sorting the dictionary items by value shows the least and most likely bi-grams.

It is more convenient to store this information in a two-dimensional array instead of a Python dictionary. The rows will represent the first character of the bi-gram, and the columns will represent the second. The array entry will contain how often the first character follows the second. We will use PyTorch's `torch.tensor` to create multi-dimensional arrays. Let's import `torch`. We can create an array of zeros with a size of 3 by 5. The default data type is float32. We will use int32 as the data type because we are dealing with counts. We can manipulate individual entries efficiently. For example, we can change the entry at index (1, 3) to one, or (1, 3) to three. We can also change entry (0,0) to five. We need a 28 by 28 array because we have 26 letters and two special characters, start and end. We'll call it "N."

Now we need to map the characters to integers to index the array. We'll construct a character array by concatenating all words into one massive string. Then, we'll create a set, which removes duplicates, to get all lowercase characters. We'll then convert the set into a list sorted from 'a' to 'z'. We need a lookup table, which we'll call "s2i," which maps characters to integers. The special start token maps to 26 and the special end token maps to 27 because "z" is at position 25. We can now map the first and second characters to integers, using `s2i`. Then, we can update the counts in array "N" using the indexes. Printing "N" will display the array, but it will look messy. We'll use a library called matplotlib to visualize the array more nicely.

`matplotlib` allows us to create figures. We can use `plt.imshow()` to show the array. However, it will still look ugly. We'll try a nicer visualization. We need to invert the `s2i` dictionary. We will call this `i2s`. `i2s` maps integers to characters. I've written code to display the array nicely. It iterates through all cells, creating the character string based on `i2s`, then plotting the bi-gram text and the corresponding number. The `.item()` is needed because when indexing the torch tensors, it returns another tensor, not an integer. So `.item()` will extract the integer. The array shows the counts, and some appear more often than others. You can see an entire row of zeros, because the end character can never be the first character of a bi-gram. We have entire columns of zeros because the start character can never be the second element. We're wasting space and the start and end tokens are crowded.

We will not use two special tokens. Instead, we'll use one, called "dot", and use a 27 by 27 array. I want to place this token at position zero and offset all the other letters by one. So, 'a' will start at one. Now, `s2i` will map 'a' to 1, and '.' to 0. `i2s` is not changed. This will be reversed here. We begin with an array of zeros, with size 27 by 27. We do the same counting as before. The special start and end character is represented by the dot character now. We see that the dot never happens as a non-first character. The first row has the count for all the first letters, and the rightmost column shows all the ending characters. In between, we have the structure of what characters follow each other. The array has all the information needed to sample from this bigram character-level language model.

We can start sampling by following these probabilities and counts. We start with the dot, the start token. To sample the first character of a name, we look at the first row of array N. These counts indicate how often each character starts a word. We can grab the first row using array indexing. Then, we need to convert these counts to probabilities. First, we will convert the integers to floats. To create the probability distribution, we'll divide it by its sum. This will give a proper probability distribution that sums to one. Now, we can sample from this distribution. To sample, we'll use `torch.multinomial`, which returns samples from a multinomial probability distribution. It takes probabilities and returns sampled integers according to the probabilities. To make everything deterministic, we will use a generator object in PyTorch. It makes the results reproducible. We can seed a generator object with a number. Using the same seed allows for consistent random results. We can normalize the results and use `torch.multinomial` to draw samples from it. This draws samples based on the probability distribution. For example, with a 60% probability of the first element, about 60% of the sample will be 0, 30% will be 1, and only 10% will be 2. In our case, we have created this `p`. We can use it to sample from the distribution. When we sample once, it is the index 13. We have to use `.item()` to get the integer. Then we map the index using `i2s` to the character, which is 'm' in this case.

We can continue sampling. Now, knowing 'm' is the first character, we can use the row corresponding to "m". These counts in this row are the counts of the next character. We are ready to write the sampling loop. We begin with index 0, the start token. We grab the row from array "N" based on our current index. Then convert it to floats. Then we normalize the probabilities. We will grab the PyTorch generator object, and draw a single sample. If the sample is zero, then the word ends, and we break. Otherwise, we will print the next character. That is it. We sampled "m," then "o," then "r," then "dot".

Now let's sample a few times. We will create an output list instead of printing directly. We append to the list. Then we print the joined result. We always get the same result because of the generator, so let's loop a few times and sample multiple names. These names do not look very good. The bi-gram language model is terrible. It generates nonsense names because it doesn't know that the first 'h' was the first character. It only knows that 'h' might be the last character. So, it generates all these nonsense names.

Another way to see that it is doing something reasonable is to create a uniform distribution. We will set all probabilities to be the same. This is the output from a completely untrained model, where everything is equally likely, which is obviously garbage. The trained model has more name-like results. It is working, but bi-grams are too terrible, so we have to do better. I want to address an inefficiency that we have. We are constantly fetching rows from array "N."
