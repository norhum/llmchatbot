Okay, so I introduced another file that will download the Fine Web Edu dataset from Hugging Face. It will pre-process and pre-tokenize all of the data, and save data shards to a folder on the local disk. While this is running, I wanted to briefly mention that you can look through the dataset viewer to get a sense of what's in it. It seems to be working fairly well, discussing topics like nuclear energy in France and Mexican America. The filters were applied automatically using LLaMA 370B, where the LLMs are judging which content is educational and allowing that content through.

In terms of the script, I'm not going to go through the full script since it's not as interesting or LLM-centric. When you run this, it first loads the dataset, which is all Hugging Face code. You'll need to pip install datasets. Then it tokenizes all the documents within the dataset. When tokenizing a single document, it starts the tokens with the end-of-text token. This is a special token in the GPT2 tokenizer, with the ID 50256, and it begins the document. It extends this with all of the document's tokens. Then it creates a NumPy array from that, ensuring that all tokens are between 0 and 65,535, since the GPT2 max token ID is well below that. We're using uint16 to save space. There's a bunch of multiprocessing code, which isn't that exciting. We're loading the dataset, tokenizing it, and saving everything to shards, which are NumPy files very similar to torch tensors. The first shard, 0000, is a validation shard, and all other shards are training shards, each containing 100 million tokens. This makes it easier to work with the files, as a single massive file can be hard to manage on disk.

We'll let this run, which will take about 30 minutes or so. Then we will come back to actually train on this data. This is a good dataset, we're doing a lot of tokens per second, we have 8 GPUs, and the code is ready for a serious training run.

Okay, so we're back. If we list the 'edu_fine_web' directory, we see 100 shards. Each shard is 100 million tokens, so 100 shards are 10 billion tokens in total.  Switching over to the main file, I made some adjustments to our data loader because we're not using Shakespeare anymore. Instead, we want to use the Fine Web shards. There's code here that loads these shards, loading the uint16 NumPy file and converting it to a torch.long tensor. Then we're enumerating all the shards. I also added a split to the data loader so we can load the train split and also the validation split. We also have not just the current position but also the current shard. When we run out of tokens in a single shard, we first advance the shard and loop if we need to, then we get the tokens and readjust the position. This data loader will now iterate all the shards.

Our train loader now has a train split. We're doing 2 to the 19th tokens per step and want to do roughly 10 billion tokens, which is how many unique tokens we have. If we divide 10 billion tokens by 2 to the 19th, we get 19,731 steps. The GPT-3 paper says to warm up the learning rate over 375 million tokens. Dividing 375 million by 2 to the 19th, we get 715 steps. So, warm-up steps is set to 715, which matches the warm-up schedule that GPT-3 used. 715 is very mild and could be made more aggressive, maybe even 100. However, let's leave it for now to have the exact hyperparameters of GPT-3.

I also adjusted the batch size. I believe I can fit 64 on my GPU as a micro batch size. That means 64 * 124 per GPU, and we have 8 GPUs. This would result in the full batch size without gradient accumulation. If this works, then this is basically a serious pre-training run. We're not logging or evaluating the validation split yet, but if we let this run for a while, we're going to get a pretty good model, perhaps even on par with or better than GPT2 124M.

Okay, it looks like everything is going great. We're processing 1.5 million tokens per second, doing 330 milliseconds per iteration, and have a total of 19,731 iterations to do, which will take about 1.7 hours. We don't even have to use gradient accumulation. If you don't have that luxury with your GPU, just decrease the batch size until things fit, but keep it to nice numbers.  We're currently warming up the learning rate.

Now I'd like to add evaluations on the validation split, logging, and visualizations of our losses.

I've adjusted the code to evaluate on the validation split by creating a Val loader by passing in `split = "Val"`. In the data loader, I introduced a new function `reset`, which is called at init and resets the data loader. In the main training loop, every 100th iteration, including the zeroth iteration, we put the model into evaluation mode, reset the Val loader, and accumulate the gradients over 20 steps, and then print the validation loss. This is the same logic as the training loop, but with no loss backward. It is just inference.

This will print the validation loss every 100th iteration. It will also allow us to take the GPT2 124M as OpenAI released it and see what kind of loss it achieves on this validation loss. This is not a fair comparison to GPT2, as it was trained on a different distribution. However, a validation split is important in a training run to make sure that we are not overfitting. This is especially a concern if we were to train for more epochs.

I deleted orphaned code for sampling and moved it up to the main loop. Once in a while, we validate, and once in a while, we sample, doing this every 100 steps while training on every step. The only thing I did is create a generator object in PyTorch for sampling to have direct control over the random numbers and not impact the training RNG. I'm using a special sampling RNG and making sure to seed it.

We're running a bit slower because I had to disable `torch.compile` to get sampling to work. It gives me a very scary error, which I hope to resolve later. I will be releasing all this code, with git commits to document each change.

I have the optimization running and we're on step 6,000. While this is training, I would like to introduce the HSwag eval. HSwag is a sentence completion dataset with multiple-choice questions. For each question, there's a shared context, and multiple options. The idea is that the multiple choices are constructed so one is a natural continuation and the others are not. Models that are trained well will be able to tell these apart. The sentences are sourced from ActivityNet and WikiHow. HSwag is interesting because the incorrect options are sourced adversarially from language models, which makes it harder for other language models. Humans have a 95% accuracy on it, but language models at the time had around 48%. It is an evaluation that has early signals.

The way we will evaluate is that we have a shared context. Instead of giving the model a multiple-choice question and asking for A, B, C, or D, we will give it a completion task, which is native to how these models work. So, we construct a batch of four rows, the shared context tokens, and then the four options. Only one is correct.  These options may be of different lengths, so we take the longest length and pad the others. The mask then identifies the active tokens, with zeros for the padded areas.  To get the language model to predict A, B, C, or D, we will look at the tokens and pick the option that gets the highest average probability.

Now let's implement this and incorporate it into the script.

I introduced a file called `hell_swag.py`. I'm not going to go through all of it because it's tedious. I'm downloading HSwag from GitHub and rendering its 10,000 examples into this 4xT format. The `render_example` function returns the tokens, mask, and correct label. The evaluate function loads GPT2 from Hugging Face and runs the eval. It calculates the cross-entropy loss of predicting the next token in a sequence and then picks the row with the lowest average loss. This gives the predicted option. If you run this, you'll see GPT2 124M gets 29.5% accuracy. GPT2 XL gets about 49%, so these are fairly low values for current language models. There are other numbers from Uther Harness that are slightly different.

I'm going to incorporate this eval into our main training script to track how HSwag evolves over time.

In `train_gpt2.py`, I made `torch.compile` optional, disabling it by default because it breaks evaluation and sampling. I created a log directory for a log.txt file, recording train loss, validation loss, and HSwag accuracies. A variable indicates the last step. Periodically, every 250th iteration or the last step, we evaluate the validation loss. Then, also every 250th iteration, we evaluate HSwag if not using compile, and we sample from the model. We then perform the training step. The only addition is the section for HSwag. I'm getting all GPUs to collaborate on this. We're iterating over the examples, and each process picks the examples assigned to it. We render an example, put it on the GPU, and get the logits. Then we use a helper function to predict the option with the lowest loss. We keep count of the correct predictions. Multiple processes synchronize their stats, packaging them into tensors, calling `all_reduce` and summing. The master process will then print the HSwag accuracy.

I'm currently running the optimization, and we just had a generation. We are halfway done at step 10,000 out of 20,000. The predictions are less random, and the model seems more self-aware.

Let's wait for the optimization to finish and see what kind of samples we get, as well as look at train loss, validation loss, and HSwag accuracy.

Focusing on the Jupyter Notebook, I have created a new cell that visualizes the train loss, validation loss, and HSwag score. We ran for 19,731 billion tokens, which is one epoch of the sample 10B of Fine Web Edu. In blue, we have training loss; in orange, validation loss. The red horizontal line shows the OpenAI GPT2 124M model evaluated on the validation set of Fine Web Edu. We are surpassing the validation loss of this model. The data set distributions are different, so it's not an entirely fair comparison, but it's a good cross-check. The ideal is to use a standard, withheld evaluation like HSwag. We see that our HSwag performance improved from 25%. We surpassed the GPT2 124M model, with green marking the GPT3 model at 124M. We were able to surpass the GPT2 model using only 10 billion tokens compared to GPT2's 100 billion.

There are many reasons why we could match or surpass this accuracy with fewer training tokens. First, OpenAI GPT2 was trained on a much wider data distribution, and the Fine Web Edu dataset is all English. Perhaps math, code, and multilingual data were stealing capacity from the original GPT2 model. HSwag is also fairly old, so some aspects may have made it into the training set.

Looking at the loss curve, it looks wrong, which I suspect is because the 10 billion sample of Fine Web Edu was not properly shuffled, and there is some periodicity to it. I think we are inheriting the ordering in the data set. This is not ideal, but I hope to fix it by the time this is released.
